# GAD + GSPO ä»£ç ä¿®æ”¹æ€»ç»“

## ä¸€ã€ä¿®æ”¹æ¦‚è§ˆ

å·²å®Œæˆå¯¹æ–° verl æ¡†æ¶çš„ GADï¼ˆGenerative Adversarial Distillationï¼‰é€‚é…ï¼ŒåŒæ—¶ä¿æŒäº† GSPO æ”¯æŒã€‚æ‰€æœ‰ä¿®æ”¹éƒ½æ˜¯**å‘åå…¼å®¹**çš„ï¼Œä¸ä¼šå½±å“ç°æœ‰çš„æ ‡å‡† PPO è®­ç»ƒã€‚

### æ ¸å¿ƒç‰¹æ€§

âœ… **GAD åˆ¤åˆ«å™¨è®­ç»ƒ**ï¼šCritic å¯ä½œä¸ºåˆ¤åˆ«å™¨ï¼ŒåŒºåˆ†æ•™å¸ˆå’Œå­¦ç”Ÿå›å¤  
âœ… **GSPO ç­–ç•¥ä¼˜åŒ–**ï¼šActor ä½¿ç”¨åºåˆ—çº§é‡è¦æ€§é‡‡æ ·  
âœ… **GRPO ä¼˜åŠ¿ä¼°è®¡**ï¼šç»„å†…æ ‡å‡†åŒ–ä¼˜åŠ¿è®¡ç®—  
âœ… **å‘åå…¼å®¹**ï¼šä¸å½±å“æ ‡å‡† PPO è®­ç»ƒæµç¨‹  
âœ… **è‡ªåŠ¨æ£€æµ‹æ¨¡å¼**ï¼šæ ¹æ®æ•°æ®æ˜¯å¦åŒ…å« `teacher_response` è‡ªåŠ¨åˆ‡æ¢è®­ç»ƒæ¨¡å¼

---

## äºŒã€ä¿®æ”¹çš„æ–‡ä»¶æ¸…å•

### 1. `verl/trainer/ppo/core_algos.py`

**ä¿®æ”¹å†…å®¹**ï¼šæ·»åŠ åˆ¤åˆ«å™¨æŸå¤±å‡½æ•°

```python
def compute_discriminator_loss(
    student_vpreds: torch.Tensor,
    teacher_vpreds: torch.Tensor,
    response_mask: torch.Tensor,
    teacher_response_mask: torch.Tensor,
) -> torch.Tensor:
    """
    è®¡ç®—åˆ¤åˆ«å™¨æŸå¤±ï¼šè®©æ•™å¸ˆå¾—åˆ†é«˜äºå­¦ç”Ÿå¾—åˆ†
    Loss = -log(sigmoid(teacher_reward - student_reward))
    """
    teacher_reward = torch.sum(teacher_vpreds * teacher_response_mask, dim=-1)
    student_reward = torch.sum(student_vpreds * response_mask, dim=-1)
    d_loss = -torch.nn.functional.logsigmoid(teacher_reward - student_reward).mean()
    return d_loss
```

**ä½ç½®**ï¼šç¬¬ 1441-1475 è¡Œï¼ˆåœ¨ `compute_value_loss` ä¹‹åï¼‰

---

### 2. `verl/workers/critic/dp_critic.py`

#### 2.1 ä¿®æ”¹ `_forward_micro_batch` æ–¹æ³•

**ä¿®æ”¹å†…å®¹**ï¼š
- æ·»åŠ  `compute_teacher` å‚æ•°ï¼Œæ”¯æŒåŒè·¯å‰å‘æ¨ç†
- å®ç°åºåˆ—çº§å¥–åŠ±æ¨¡å‹ï¼šåªä¿ç•™æœ€åä¸€ä¸ªæœ‰æ•ˆ token çš„å€¼

```python
def _forward_micro_batch(self, micro_batch, compute_teacher=False):
    # æ ¹æ® compute_teacher é€‰æ‹©è¾“å…¥æ•°æ®
    if compute_teacher:
        response_length = micro_batch["teacher_response"].size(-1)
        input_ids = micro_batch["teacher_input_ids"]
        attention_mask = micro_batch["teacher_attention_mask"]
        position_ids = micro_batch["teacher_position_ids"]
    else:
        response_length = micro_batch["responses"].size(-1)
        input_ids = micro_batch["input_ids"]
        attention_mask = micro_batch["attention_mask"]
        position_ids = micro_batch["position_ids"]
    
    # ... å‰å‘æ¨ç† ...
    
    # å…³é”®ï¼šåªä¿ç•™æœ€åä¸€ä¸ªæœ‰æ•ˆ token çš„å€¼ï¼ˆåºåˆ—çº§å¥–åŠ±ï¼‰
    response_mask = attention_mask[:, -response_length:]
    response_lengths = response_mask.sum(dim=1).long()
    last_token_indices = response_lengths - 1
    last_token_mask = torch.zeros_like(response_mask, dtype=torch.bool)
    batch_indices = torch.arange(response_mask.size(0), device=response_mask.device)
    last_token_mask[batch_indices, last_token_indices] = True
    values = values * last_token_mask.type_as(values)
    
    return values
```

#### 2.2 æ·»åŠ  `_forward_batch_teacher_forcing_grpo` æ–¹æ³•

**ä¿®æ”¹å†…å®¹**ï¼šä¸º GRPO æä¾›æ•™å¸ˆå¼ºåˆ¶å€¼

```python
def _forward_batch_teacher_forcing_grpo(self, batch, teacher_repeat):
    """
    ä¸ºåŒä¸€ prompt çš„å¤šä¸ªæ•™å¸ˆå›å¤åˆ†é…é€’å¢çš„å€¼
    ç”¨äº GRPO çš„ç»„å†…ç›¸å¯¹æ¯”è¾ƒ
    """
    response_length = batch["teacher_response"].size(-1)
    input_ids = batch["teacher_input_ids"]
    bsz, seqlen = input_ids.shape
    attention_mask = batch["teacher_attention_mask"]
    
    values = torch.zeros((bsz, response_length), device=input_ids.device)
    response_mask = attention_mask[:, -response_length:]
    response_lengths = response_mask.sum(dim=1).long()
    last_token_indices = response_lengths - 1
    
    # ä¸ºåŒä¸€ç»„çš„æ•™å¸ˆå›å¤åˆ†é…é€’å¢å€¼
    for i in range(0, bsz, teacher_repeat):
        for j in range(teacher_repeat):
            values[i + j, last_token_indices[i + j]] = float(j)
    
    return values
```

#### 2.3 ä¿®æ”¹ `compute_values` æ–¹æ³•

**ä¿®æ”¹å†…å®¹**ï¼š
- æ”¯æŒ `compute_teacher` å…ƒä¿¡æ¯
- æ”¯æŒæ•™å¸ˆå¼ºåˆ¶ï¼ˆteacher forcingï¼‰

```python
def compute_values(self, data: DataProto) -> torch.Tensor:
    # æ£€æŸ¥æ˜¯å¦è®¡ç®—æ•™å¸ˆå€¼
    compute_teacher = data.meta_info.get("compute_teacher", False)
    
    # æ ¹æ® compute_teacher é€‰æ‹©æ•°æ®å­—æ®µ
    if compute_teacher:
        select_keys = ["teacher_response", "teacher_input_ids", 
                      "teacher_attention_mask", "teacher_position_ids"]
        
        # æ•™å¸ˆå¼ºåˆ¶ï¼ˆç”¨äº GRPOï¼‰
        if "teacher_repeat" in data.meta_info:
            teacher_repeat = data.meta_info["teacher_repeat"]
            batch = data.select(batch_keys=select_keys).batch
            return self._forward_batch_teacher_forcing_grpo(batch, teacher_repeat)
    else:
        select_keys = ["responses", "input_ids", "attention_mask", "position_ids"]
    
    # ... å‰å‘æ¨ç† ...
    values = self._forward_micro_batch(model_inputs, compute_teacher=compute_teacher)
    
    return values
```

#### 2.4 ä¿®æ”¹ `update_critic` æ–¹æ³•

**ä¿®æ”¹å†…å®¹**ï¼š
- è‡ªåŠ¨æ£€æµ‹ GAD æ¨¡å¼ï¼ˆæ˜¯å¦åŒ…å« `teacher_response`ï¼‰
- GAD æ¨¡å¼ï¼šä½¿ç”¨åˆ¤åˆ«å™¨æŸå¤±
- æ ‡å‡†æ¨¡å¼ï¼šä½¿ç”¨ value loss

```python
def update_critic(self, data: DataProto):
    self.critic_module.train()
    metrics = {}
    
    # è‡ªåŠ¨æ£€æµ‹è®­ç»ƒæ¨¡å¼
    use_discriminator = "teacher_response" in data.batch
    
    if use_discriminator:
        # GAD æ¨¡å¼ï¼šéœ€è¦å­¦ç”Ÿå’Œæ•™å¸ˆæ•°æ®
        select_keys = [
            "input_ids", "responses", "attention_mask", "position_ids",
            "teacher_input_ids", "teacher_response", 
            "teacher_attention_mask", "teacher_position_ids"
        ]
    else:
        # æ ‡å‡† PPO æ¨¡å¼
        select_keys = ["input_ids", "responses", "response_mask", 
                      "attention_mask", "position_ids", "values", "returns"]
    
    # ... æ•°æ®åŠ è½½ ...
    
    for micro_batch in micro_batches:
        if use_discriminator:
            # GAD åˆ¤åˆ«å™¨è®­ç»ƒ
            student_vpreds = self._forward_micro_batch(micro_batch, compute_teacher=False)
            teacher_vpreds = self._forward_micro_batch(micro_batch, compute_teacher=True)
            
            # è®¡ç®—åˆ¤åˆ«å‡†ç¡®ç‡
            d_acc = (teacher_vpreds.sum(dim=-1) > student_vpreds.sum(dim=-1)).float().mean()
            
            # è®¡ç®—åˆ¤åˆ«å™¨æŸå¤±
            d_loss = core_algos.compute_discriminator_loss(
                student_vpreds=student_vpreds,
                teacher_vpreds=teacher_vpreds,
                response_mask=response_mask,
                teacher_response_mask=teacher_response_mask,
            )
            
            loss.backward()
            
            metrics.update({
                "critic/d_loss": d_loss.item(),
                "critic/d_acc": d_acc.item(),
                "critic/student_value_mean": ...,
                "critic/teacher_value_mean": ...,
            })
        else:
            # æ ‡å‡† PPO å€¼å‡½æ•°è®­ç»ƒ
            vpreds = self._forward_micro_batch(micro_batch, compute_teacher=False)
            vf_loss, vf_clipfrac = core_algos.compute_value_loss(...)
            
            loss.backward()
            
            metrics.update({
                "critic/vf_loss": vf_loss.item(),
                "critic/vf_clipfrac": vf_clipfrac.item(),
                "critic/vpred_mean": ...,
            })
    
    return metrics
```

---

### 3. `verl/utils/dataset/rl_dataset.py`

**ä¿®æ”¹å†…å®¹**ï¼šæ”¯æŒ `teacher_response` çš„åŠ è½½å’Œå¤„ç†

```python
def __getitem__(self, item):
    row_dict: dict = self.dataframe[item]
    messages = self._build_messages(row_dict)
    
    # æå– teacher_responseï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    teacher_response = row_dict.pop("teacher_response", None)
    
    # ... å¤„ç† prompt ...
    
    # å¤„ç† teacher_response
    if teacher_response is not None:
        # 1. Tokenize æ•™å¸ˆå›å¤
        teacher_response_tokens = self.tokenizer(
            teacher_response, return_tensors="pt", add_special_tokens=False
        )
        teacher_response_ids = teacher_response_tokens["input_ids"]
        
        # 2. Postprocessï¼ˆæˆªæ–­/å¡«å……ï¼‰
        teacher_response_ids, _ = verl_F.postprocess_data(
            input_ids=teacher_response_ids,
            max_length=self.max_response_length,
            pad_token_id=self.tokenizer.pad_token_id,
            left_pad=False,  # å³ä¾§å¡«å……
            truncation=self.truncation,
        )
        
        # 3. æ„å»º teacher_input_ids = prompt + teacher_response
        prompt_ids = row_dict["input_ids"]
        teacher_input_ids = torch.cat([prompt_ids.unsqueeze(0), teacher_response_ids], dim=1)
        
        # 4. ç”Ÿæˆ attention_mask å’Œ position_ids
        teacher_attention_mask = (teacher_input_ids != self.tokenizer.pad_token_id).long()
        teacher_position_ids = compute_position_id_with_mask(teacher_attention_mask)
        
        # 5. æ·»åŠ åˆ° row_dict
        row_dict["teacher_response"] = teacher_response_ids[0]
        row_dict["teacher_input_ids"] = teacher_input_ids[0]
        row_dict["teacher_attention_mask"] = teacher_attention_mask[0]
        row_dict["teacher_position_ids"] = teacher_position_ids[0]
    
    return row_dict
```

---

## ä¸‰ã€ä½¿ç”¨æ–¹æ³•

### 3.1 æ ‡å‡† PPO è®­ç»ƒï¼ˆæ— éœ€ä¿®æ”¹ï¼‰

å¦‚æœæ•°æ®ä¸­**ä¸åŒ…å«** `teacher_response`ï¼Œä»£ç è‡ªåŠ¨ä½¿ç”¨æ ‡å‡† PPO æ¨¡å¼ï¼š

```bash
python -m verl.trainer.main_ppo \
  --config-path configs \
  --config-name ppo_config \
  data.train_files=/path/to/data_without_teacher.parquet
```

**æ•°æ®æ ¼å¼**ï¼š
```python
{
    "content": [{"role": "user", "content": "é—®é¢˜"}],
    # ä¸éœ€è¦ teacher_response
}
```

---

### 3.2 GAD + GSPO è®­ç»ƒ

å¦‚æœæ•°æ®ä¸­**åŒ…å«** `teacher_response`ï¼Œä»£ç è‡ªåŠ¨ä½¿ç”¨ GAD åˆ¤åˆ«å™¨æ¨¡å¼ï¼š

```bash
python -m verl.trainer.main_ppo \
  --config-path configs \
  --config-name gad_gspo_config \
  algorithm.adv_estimator=grpo \
  actor_rollout_ref.actor.policy_loss=gspo \
  actor_rollout_ref.actor.loss_agg_mode=seq-mean-token-mean \
  actor_rollout_ref.rollout.n=8 \
  trainer.critic_warmup=10 \
  data.train_files=/path/to/data_with_teacher.parquet
```

**æ•°æ®æ ¼å¼**ï¼š
```python
{
    "content": [{"role": "user", "content": "é—®é¢˜"}],
    "teacher_response": "æ•™å¸ˆæ¨¡å‹çš„é«˜è´¨é‡å›å¤"  # ğŸ”¥ å…³é”®å­—æ®µ
}
```

---

### 3.3 é…ç½®ç¤ºä¾‹

#### å®Œæ•´çš„ GAD + GSPO é…ç½®

```yaml
# æ•°æ®é…ç½®
data:
  train_files: /path/to/data_with_teacher_response.parquet
  train_batch_size: 256
  max_prompt_length: 2048
  max_response_length: 1536

# ç®—æ³•é…ç½®
algorithm:
  adv_estimator: grpo  # ğŸ”¥ GRPO ä¼˜åŠ¿ä¼°è®¡
  norm_adv_by_std_in_grpo: true
  gamma: 1.0
  lam: 0.95

# Actor é…ç½®
actor_rollout_ref:
  model:
    path: /path/to/model
  
  rollout:
    n: 8  # æ¯ä¸ª prompt ç”Ÿæˆ 8 ä¸ªå›å¤
    temperature: 0.8
  
  actor:
    policy_loss: gspo  # ğŸ”¥ GSPO ç­–ç•¥æŸå¤±
    loss_agg_mode: seq-mean-token-mean  # ğŸ”¥ åºåˆ—çº§èšåˆ
    clip_ratio: 0.2
    optim:
      lr: 1e-6
  
  ppo_mini_batch_size: 256
  ppo_micro_batch_size_per_gpu: 8
  ppo_epochs: 1

# Criticï¼ˆåˆ¤åˆ«å™¨ï¼‰é…ç½®
critic:
  model:
    path: /path/to/reward_model
  optim:
    lr: 1e-6
  ppo_epochs: 1
  cliprange_value: 0.2

# è®­ç»ƒé…ç½®
trainer:
  critic_warmup: 10  # ğŸ”¥ å‰ 10 æ­¥åªè®­ç»ƒåˆ¤åˆ«å™¨
  total_epochs: 2
  save_freq: 50
```

---

## å››ã€è®­ç»ƒæµç¨‹

### 4.1 GAD + GSPO å®Œæ•´æµç¨‹

```
æ•°æ®åŠ è½½ï¼ˆåŒ…å« teacher_responseï¼‰
  â†“
Actor ç”Ÿæˆ 8 ä¸ªå­¦ç”Ÿå›å¤
  â†“
åˆ¤åˆ«å™¨è¯„åˆ†ï¼ˆå­¦ç”Ÿ vs æ•™å¸ˆï¼‰
  â†“
GRPO è®¡ç®—ä¼˜åŠ¿ï¼ˆç»„å†…æ ‡å‡†åŒ–ï¼‰
  â†“
æ›´æ–°åˆ¤åˆ«å™¨ï¼ˆåˆ¤åˆ«å™¨æŸå¤±ï¼‰
  - åŒè·¯å‰å‘æ¨ç†
  - è®¡ç®— d_loss = -log(sigmoid(T_score - S_score))
  - åå‘ä¼ æ’­
  â†“
æ›´æ–° Actorï¼ˆGSPO æŸå¤±ï¼Œstep > 10 åï¼‰
  - è®¡ç®—åºåˆ—çº§é‡è¦æ€§é‡‡æ ·æ¯”ç‡
  - PPO clipping
  - åå‘ä¼ æ’­
```

### 4.2 Critic Warmup æœºåˆ¶

```
Step 0-9 (critic_warmup=10):
â”œâ”€ ç”Ÿæˆå­¦ç”Ÿå›å¤ (n=8)
â”œâ”€ åˆ¤åˆ«å™¨æ‰“åˆ†
â”œâ”€ è®¡ç®— GRPO ä¼˜åŠ¿
â”œâ”€ âœ… æ›´æ–°åˆ¤åˆ«å™¨
â””â”€ âŒ ä¸æ›´æ–° Actor

Step 10+:
â”œâ”€ ç”Ÿæˆå­¦ç”Ÿå›å¤ (n=8)
â”œâ”€ åˆ¤åˆ«å™¨æ‰“åˆ†
â”œâ”€ è®¡ç®— GRPO ä¼˜åŠ¿
â”œâ”€ âœ… æ›´æ–°åˆ¤åˆ«å™¨
â””â”€ âœ… æ›´æ–° Actorï¼ˆä½¿ç”¨ GSPOï¼‰
```

---

## äº”ã€ç›‘æ§æŒ‡æ ‡

### 5.1 GAD åˆ¤åˆ«å™¨æŒ‡æ ‡

- **`critic/d_loss`**ï¼šåˆ¤åˆ«å™¨æŸå¤±ï¼ˆåº”é€æ¸ä¸‹é™ï¼Œç†æƒ³å€¼ 0.3-0.5ï¼‰
- **`critic/d_acc`**ï¼šåˆ¤åˆ«å‡†ç¡®ç‡ï¼ˆåº”ä» 0.5 ä¸Šå‡åˆ° > 0.7ï¼‰
- **`critic/student_value_mean`**ï¼šå­¦ç”Ÿå›å¤å¹³å‡åˆ†ï¼ˆåº”é€æ¸ä¸Šå‡ï¼‰
- **`critic/teacher_value_mean`**ï¼šæ•™å¸ˆå›å¤å¹³å‡åˆ†ï¼ˆåº”ä¿æŒç¨³å®šä¸”è¾ƒé«˜ï¼‰

### 5.2 GSPO Actor æŒ‡æ ‡

- **`actor/pg_clipfrac`**ï¼šè£å‰ªæ¯”ä¾‹ï¼ˆåº”åœ¨ 0.1-0.3ï¼‰
- **`actor/ppo_kl`**ï¼šKL æ•£åº¦ï¼ˆåº”ä¿æŒè¾ƒå°ï¼‰
- **`actor/entropy`**ï¼šç­–ç•¥ç†µï¼ˆä¸åº”è¿‡å¿«ä¸‹é™ï¼‰

### 5.3 æ ‡å‡† PPO æŒ‡æ ‡ï¼ˆé GAD æ¨¡å¼ï¼‰

- **`critic/vf_loss`**ï¼šå€¼å‡½æ•°æŸå¤±
- **`critic/vf_clipfrac`**ï¼šå€¼å‡½æ•°è£å‰ªæ¯”ä¾‹
- **`critic/vpred_mean`**ï¼šå€¼é¢„æµ‹å¹³å‡å€¼

---

## å…­ã€å…³é”®è®¾è®¡å†³ç­–

### 6.1 å‘åå…¼å®¹æ€§

æ‰€æœ‰ä¿®æ”¹éƒ½æ˜¯**å‘åå…¼å®¹**çš„ï¼š
- é€šè¿‡æ£€æµ‹ `teacher_response` å­—æ®µè‡ªåŠ¨åˆ‡æ¢æ¨¡å¼
- ä¸å½±å“ç°æœ‰çš„æ ‡å‡† PPO è®­ç»ƒæµç¨‹
- å¯ä»¥åœ¨åŒä¸€ä»£ç åº“ä¸­è¿è¡Œ PPO å’Œ GAD

### 6.2 åºåˆ—çº§å¥–åŠ±æ¨¡å‹

åˆ¤åˆ«å™¨è¢«æ”¹é€ ä¸º**åºåˆ—çº§å¥–åŠ±æ¨¡å‹**ï¼š
- æ•´ä¸ªå›å¤çš„è´¨é‡ç”¨ä¸€ä¸ªæ ‡é‡è¡¨ç¤º
- è¿™ä¸ªæ ‡é‡æ”¾åœ¨æœ€åä¸€ä¸ªæœ‰æ•ˆ token çš„ä½ç½®
- é€šè¿‡ `last_token_mask` å®ç°

### 6.3 è‡ªåŠ¨æ¨¡å¼æ£€æµ‹

```python
# åœ¨ update_critic ä¸­è‡ªåŠ¨æ£€æµ‹
use_discriminator = "teacher_response" in data.batch

if use_discriminator:
    # GAD æ¨¡å¼ï¼šåˆ¤åˆ«å™¨æŸå¤±
    d_loss = compute_discriminator_loss(...)
else:
    # æ ‡å‡†æ¨¡å¼ï¼švalue loss
    vf_loss = compute_value_loss(...)
```

### 6.4 æ— å†—ä½™ä»£ç 

- å¤ç”¨ç°æœ‰çš„æ•°æ®å¤„ç†é€»è¾‘
- ä½¿ç”¨æ¡ä»¶åˆ†æ”¯è€Œéé‡å¤ä»£ç 
- ä¿æŒä»£ç ç®€æ´å’Œå¯ç»´æŠ¤æ€§

---

## ä¸ƒã€éªŒè¯æ¸…å•

### 7.1 ä»£ç å®Œæ•´æ€§

- [x] `compute_discriminator_loss` å·²æ·»åŠ åˆ° `core_algos.py`
- [x] `_forward_micro_batch` æ”¯æŒ `compute_teacher` å‚æ•°
- [x] æœ€å token mask é€»è¾‘å·²å®ç°
- [x] `_forward_batch_teacher_forcing_grpo` å·²æ·»åŠ 
- [x] `compute_values` æ”¯æŒ `compute_teacher` å…ƒä¿¡æ¯
- [x] `update_critic` è‡ªåŠ¨æ£€æµ‹ GAD æ¨¡å¼
- [x] `rl_dataset.py` æ”¯æŒ `teacher_response` åŠ è½½

### 7.2 åŠŸèƒ½éªŒè¯

- [ ] æ ‡å‡† PPO è®­ç»ƒæ­£å¸¸è¿è¡Œï¼ˆæ—  `teacher_response`ï¼‰
- [ ] GAD è®­ç»ƒæ­£å¸¸è¿è¡Œï¼ˆæœ‰ `teacher_response`ï¼‰
- [ ] GSPO æŸå¤±è®¡ç®—æ­£ç¡®
- [ ] åˆ¤åˆ«å™¨æŒ‡æ ‡æ­£å¸¸è¾“å‡º
- [ ] Critic warmup æœºåˆ¶ç”Ÿæ•ˆ

### 7.3 æ€§èƒ½éªŒè¯

- [ ] `d_acc` ä» 0.5 ä¸Šå‡åˆ° > 0.7
- [ ] `d_loss` é€æ¸ä¸‹é™
- [ ] æ•™å¸ˆå¾—åˆ† > å­¦ç”Ÿå¾—åˆ†
- [ ] Actor è®­ç»ƒç¨³å®šï¼ˆstep > 10 åï¼‰

---

## å…«ã€å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•åˆ¤æ–­æ˜¯å¦ä½¿ç”¨äº† GAD æ¨¡å¼ï¼Ÿ

**A**: æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ä¸­çš„æŒ‡æ ‡ï¼š
- GAD æ¨¡å¼ï¼šä¼šè¾“å‡º `critic/d_loss` å’Œ `critic/d_acc`
- æ ‡å‡†æ¨¡å¼ï¼šä¼šè¾“å‡º `critic/vf_loss` å’Œ `critic/vf_clipfrac`

### Q2: æ•°æ®ä¸­å¿…é¡»åŒ…å« `teacher_response` å—ï¼Ÿ

**A**: ä¸æ˜¯å¿…é¡»çš„ï¼š
- æœ‰ `teacher_response`ï¼šè‡ªåŠ¨ä½¿ç”¨ GAD æ¨¡å¼
- æ—  `teacher_response`ï¼šè‡ªåŠ¨ä½¿ç”¨æ ‡å‡† PPO æ¨¡å¼

### Q3: GSPO å¯ä»¥å•ç‹¬ä½¿ç”¨å—ï¼ˆä¸ç”¨ GADï¼‰ï¼Ÿ

**A**: å¯ä»¥ï¼åªéœ€ï¼š
```yaml
algorithm.adv_estimator: grpo
actor_rollout_ref.actor.policy_loss: gspo
# æ•°æ®ä¸­ä¸åŒ…å« teacher_response
```

### Q4: å¦‚ä½•è°ƒè¯•åˆ¤åˆ«å™¨è®­ç»ƒï¼Ÿ

**A**: æ£€æŸ¥ä»¥ä¸‹æŒ‡æ ‡ï¼š
1. `critic/d_acc` æ˜¯å¦ä» 0.5 å¼€å§‹ä¸Šå‡
2. `critic/teacher_value_mean` æ˜¯å¦ > `critic/student_value_mean`
3. `critic/d_loss` æ˜¯å¦é€æ¸ä¸‹é™

---

## ä¹ã€æ€»ç»“

### 9.1 ä¿®æ”¹çš„æ ¸å¿ƒä»·å€¼

1. **å®Œæ•´çš„ GAD æ”¯æŒ**ï¼šåˆ¤åˆ«å™¨è®­ç»ƒã€åºåˆ—çº§å¥–åŠ±æ¨¡å‹
2. **GSPO é›†æˆ**ï¼šåºåˆ—çº§ç­–ç•¥ä¼˜åŒ–
3. **å‘åå…¼å®¹**ï¼šä¸å½±å“ç°æœ‰ä»£ç 
4. **è‡ªåŠ¨æ£€æµ‹**ï¼šæ ¹æ®æ•°æ®è‡ªåŠ¨åˆ‡æ¢æ¨¡å¼
5. **ä»£ç ç®€æ´**ï¼šæ— å†—ä½™ï¼Œæ˜“ç»´æŠ¤

### 9.2 é€‚ç”¨åœºæ™¯

| åœºæ™¯ | é…ç½® | æ•°æ®è¦æ±‚ |
|------|------|---------|
| **æ ‡å‡† PPO** | `policy_loss=ppo` | æ—  `teacher_response` |
| **çº¯ GSPO** | `policy_loss=gspo, adv_estimator=grpo` | æ—  `teacher_response` |
| **GAD + PPO** | `policy_loss=ppo` | æœ‰ `teacher_response` |
| **GAD + GSPO** | `policy_loss=gspo, adv_estimator=grpo` | æœ‰ `teacher_response` |

### 9.3 æ¨èé…ç½®

**æœ€ä½³å®è·µ**ï¼šGAD + GSPO
```yaml
algorithm.adv_estimator: grpo
actor_rollout_ref.actor.policy_loss: gspo
actor_rollout_ref.actor.loss_agg_mode: seq-mean-token-mean
actor_rollout_ref.rollout.n: 8
trainer.critic_warmup: 10
data.train_files: /path/to/data_with_teacher_response.parquet
```

è¿™ä¸ªé…ç½®ç»“åˆäº†ï¼š
- **GAD**ï¼šåˆ¤åˆ«å™¨æŒ‡å¯¼å­¦ä¹ 
- **GSPO**ï¼šåºåˆ—çº§ç­–ç•¥ä¼˜åŒ–
- **GRPO**ï¼šç»„å†…æ ‡å‡†åŒ–ä¼˜åŠ¿

é€‚ç”¨äºéœ€è¦æ•™å¸ˆæŒ‡å¯¼çš„åºåˆ—çº§ä»»åŠ¡ï¼ˆä»£ç ç”Ÿæˆã€æ•°å­¦æ¨ç†ç­‰ï¼‰ã€‚
