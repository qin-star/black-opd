# Warmup é˜¶æ®µå®Œæ•´è®­ç»ƒæµç¨‹è§£æ

## ä¸€ã€Warmup é˜¶æ®µæ¦‚è¿°

### 1.1 æ ¸å¿ƒç›®æ ‡

Warmup (Stage 1) æ˜¯ GAD è®­ç»ƒçš„ç¬¬ä¸€é˜¶æ®µï¼Œä¸»è¦ç›®æ ‡ï¼š
1. **è®­ç»ƒåˆ¤åˆ«å™¨**ï¼šå­¦ä¹ åŒºåˆ†æ•™å¸ˆå›å¤å’Œå­¦ç”Ÿå›å¤çš„èƒ½åŠ›
2. **é¢„çƒ­å­¦ç”Ÿæ¨¡å‹**ï¼šè®©å­¦ç”Ÿæ¨¡å‹åˆæ­¥å­¦ä¼šç”Ÿæˆåˆç†å›å¤
3. **å»ºç«‹åŸºç¡€**ï¼šä¸º Stage 2 çš„å®Œå…¨å¯¹æŠ—è®­ç»ƒåšå‡†å¤‡

### 1.2 å…³é”®ç‰¹æ€§

- **å·²ä½¿ç”¨å¯¹æŠ—è®­ç»ƒ**ï¼šåˆ¤åˆ«å™¨ä½¿ç”¨ `compute_discriminator_loss`ï¼Œè€Œéä¼ ç»Ÿ value loss
- **Critic Warmup æœºåˆ¶**ï¼šå‰ 10 æ­¥åªè®­ç»ƒåˆ¤åˆ«å™¨ï¼Œä¸æ›´æ–° Actor
- **æ•°æ®è¦æ±‚**ï¼šå¿…é¡»åŒ…å«æ•™å¸ˆå›å¤ï¼ˆ`teacher_response`ï¼‰
- **è®­ç»ƒæ—¶é•¿**ï¼šçº¦ 2 epochsï¼Œ~800 steps

---

## äºŒã€è®­ç»ƒé…ç½®

### 2.1 å¯åŠ¨è„šæœ¬

```bash
bash scripts/train/gpt5-chat-filtered-7b-warmup-lr1e-6.sh \
  --model /tmp/Qwen2.5-7B-Instruct \
  --reward_model /tmp/Qwen2.5-7B-Instruct \
  --exp_name gpt5-chat-filtered-7b-warmup-lr1e-6 \
  --nnodes 1
```

### 2.2 å…³é”®å‚æ•°

```python
# ä¼˜åŠ¿ä¼°è®¡å™¨
algorithm.adv_estimator=grpo

# æ•°æ®é…ç½®
data.train_files=/tmp/lmsys_gpt5_chat_4k_filtered_train.parquet
data.train_batch_size=256
data.max_prompt_length=2048
data.max_response_length=1536

# Actor é…ç½®
actor_rollout_ref.model.path=$MODEL_PATH  # ä»é¢„è®­ç»ƒæ¨¡å‹åˆå§‹åŒ–
actor_rollout_ref.actor.optim.lr=1e-6
actor_rollout_ref.rollout.n=8  # æ¯ä¸ªpromptç”Ÿæˆ8ä¸ªå›å¤
actor_rollout_ref.rollout.temperature=0.8

# Critic (åˆ¤åˆ«å™¨) é…ç½®
critic.model.path=$REWARD_MODEL_PATH  # ä»é¢„è®­ç»ƒæ¨¡å‹åˆå§‹åŒ–
critic.optim.lr=1e-6

# è®­ç»ƒç­–ç•¥
trainer.critic_warmup=10  # ğŸ”¥ å‰10æ­¥åªè®­ç»ƒåˆ¤åˆ«å™¨
trainer.total_epochs=2
trainer.save_freq=50
```

---

## ä¸‰ã€æ•°æ®å‡†å¤‡

### 3.1 æ•°æ®æ ¼å¼è¦æ±‚

Parquet æ–‡ä»¶å¿…é¡»åŒ…å«ä»¥ä¸‹å­—æ®µï¼š

```python
{
    "content": [  # prompt (messagesæ ¼å¼)
        {"role": "user", "content": "é—®é¢˜å†…å®¹"}
    ],
    "teacher_response": "æ•™å¸ˆæ¨¡å‹çš„é«˜è´¨é‡å›å¤"
}
```

### 3.2 æ•°æ®åŠ è½½æµç¨‹

**ä½ç½®**: `verl/utils/dataset/rl_dataset.py:213-312`

```python
def __getitem__(self, item):
    row_dict = self.dataframe[item]
    messages = self._build_messages(row_dict)
    
    # æå–æ•™å¸ˆå›å¤
    teacher_response = None
    if 'teacher_response' in row_dict:
        teacher_response = row_dict.pop('teacher_response')
    
    # å¤„ç† prompt
    raw_prompt = self.tokenizer.apply_chat_template(
        messages, add_generation_prompt=True, tokenize=False
    )
    model_inputs = self.tokenizer(raw_prompt, return_tensors="pt")
    
    # å¤„ç†æ•™å¸ˆå›å¤
    if teacher_response is not None:
        teacher_response = self.tokenizer(
            teacher_response, return_tensors="pt", add_special_tokens=False
        )
        teacher_response_ids, _ = verl_F.postprocess_data(
            input_ids=teacher_response["input_ids"],
            max_length=self.max_response_length,
            pad_token_id=self.tokenizer.eos_token_id,
            left_pad=False,  # å›å¤ä»å³ä¾§å¡«å……
            truncation=self.truncation,
        )
        row_dict["teacher_response"] = teacher_response_ids[0]
    
    return row_dict
```

**è¾“å‡ºæ•°æ®**ï¼š
- `input_ids`: Prompt çš„ token IDs
- `attention_mask`: Prompt çš„ attention mask
- `position_ids`: Position IDs
- `teacher_response`: æ•™å¸ˆå›å¤çš„ token IDs

---

## å››ã€å•æ­¥è®­ç»ƒæµç¨‹

### 4.1 å®Œæ•´æµç¨‹å›¾

```
æ•°æ®åŠ è½½
  â†“
[Prompt] + [Teacher Response]
  â†“
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ä¸»è®­ç»ƒå¾ªç¯ (æ¯ä¸ª step)
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
  â†“
1. Actor ç”Ÿæˆ 8 ä¸ªå­¦ç”Ÿå›å¤
   [Prompt] â†’ Actor â†’ [Student Response 1-8]
  â†“
2. è®¡ç®—æ—§ç­–ç•¥ log_prob
   [Responses] â†’ Actor â†’ [Old Log Probs]
  â†“
3. åˆ¤åˆ«å™¨è¯„åˆ†ï¼ˆå­¦ç”Ÿå›å¤ï¼‰
   [Student Responses] â†’ Discriminator â†’ [Scores]
  â†“
4. GRPO è®¡ç®—ä¼˜åŠ¿
   åŒä¸€ prompt çš„ 8 ä¸ªå›å¤ç›¸å¯¹æ¯”è¾ƒ
   â†’ [Advantages: +/- based on group mean]
  â†“
5. æ›´æ–°åˆ¤åˆ«å™¨ï¼ˆå§‹ç»ˆæ‰§è¡Œï¼‰
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ Forward: Student Response â†’ S_score â”‚
   â”‚ Forward: Teacher Response â†’ T_score â”‚
   â”‚ Loss: -log(sigmoid(T - S))      â”‚
   â”‚ Backward & Update                â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  â†“
6. æ›´æ–° Actorï¼ˆstep > 10 åæ‰§è¡Œï¼‰
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ PPO Loss: clip(ratio * advantage) â”‚
   â”‚ KL Loss: KL(Ï€ || Ï€_ref)          â”‚
   â”‚ Backward & Update                â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 ä»£ç å®ç°

**ä½ç½®**: `verl/trainer/ppo/ray_trainer.py:950-1185`

```python
for epoch in range(total_epochs):
    for batch in train_dataloader:
        
        # Step 1: ç”Ÿæˆå­¦ç”Ÿå›å¤
        gen_batch_output = self.actor_rollout_wg.generate_sequences(gen_batch)
        batch = batch.union(gen_batch_output)
        
        # Step 2: è®¡ç®—æ—§ç­–ç•¥çš„ log_prob
        batch.meta_info["compute_teacher"] = False
        old_log_prob = self.actor_rollout_wg.compute_log_prob(batch)
        batch = batch.union(old_log_prob)
        
        # Step 3: ä½¿ç”¨åˆ¤åˆ«å™¨è®¡ç®—å¥–åŠ±
        batch.meta_info["compute_teacher"] = False
        values = self.critic_wg.compute_values(batch)
        batch = batch.union(values)
        reward_tensor = batch.batch["values"]
        
        # Step 4: è®¡ç®—ä¼˜åŠ¿å‡½æ•° (GRPO)
        batch.batch["token_level_scores"] = reward_tensor
        batch.batch["token_level_rewards"] = reward_tensor
        
        batch = compute_advantage(
            batch,
            adv_estimator='grpo',
            num_repeat=8,
        )
        
        # Step 5: æ›´æ–°åˆ¤åˆ«å™¨ï¼ˆå§‹ç»ˆæ‰§è¡Œï¼‰
        critic_output = self.critic_wg.update_critic(batch)
        
        # Step 6: æ›´æ–° Actorï¼ˆwarmup æœŸåï¼‰
        if self.config.trainer.critic_warmup <= self.global_steps:
            actor_output = self.actor_rollout_wg.update_actor(batch)
```

---

## äº”ã€åˆ¤åˆ«å™¨æ›´æ–°è¯¦è§£

### 5.1 æ ¸å¿ƒè®¤çŸ¥

**é‡è¦**ï¼šåˆ¤åˆ«å™¨æ›´æ–°**ä¸ä¾èµ–** GRPO ä¼˜åŠ¿ï¼

- **GRPO ä¼˜åŠ¿**ï¼šç”¨äºæ›´æ–° Actorï¼ˆå­¦ç”Ÿæ¨¡å‹ï¼‰
- **åˆ¤åˆ«å™¨æŸå¤±**ï¼šç”¨äºæ›´æ–° Criticï¼ˆåˆ¤åˆ«å™¨ï¼‰

ä¸¤è€…æ˜¯ç‹¬ç«‹çš„è¿‡ç¨‹ã€‚

### 5.2 åˆ¤åˆ«å™¨æ›´æ–°æµç¨‹

**ä½ç½®**: `verl/workers/critic/dp_critic.py:247-337`

```python
def update_critic(self, data: DataProto):
    self.critic_module.train()
    
    # å‡†å¤‡æ•°æ®ï¼šåŒæ—¶éœ€è¦å­¦ç”Ÿå’Œæ•™å¸ˆçš„å›å¤
    select_keys = [
        "input_ids", "responses", "attention_mask", "position_ids",
        "teacher_input_ids", "teacher_response", 
        "teacher_attention_mask", "teacher_position_ids"
    ]
    
    for epoch in range(self.config.ppo_epochs):
        for mini_batch in dataloader:
            for micro_batch in micro_batches:
                
                # 1. åŒè·¯å‰å‘æ¨ç†
                student_vpreds = self._forward_micro_batch(
                    micro_batch, compute_teacher=False
                )
                teacher_vpreds = self._forward_micro_batch(
                    micro_batch, compute_teacher=True
                )
                
                # 2. è®¡ç®—åˆ¤åˆ«å‡†ç¡®ç‡ï¼ˆç›‘æ§ï¼‰
                d_acc = (teacher_vpreds.sum(dim=-1) > 
                        student_vpreds.sum(dim=-1)).float().mean()
                
                # 3. è®¡ç®—åˆ¤åˆ«å™¨æŸå¤±
                d_loss = core_algos.compute_discriminator_loss(
                    student_vpreds=student_vpreds,
                    teacher_vpreds=teacher_vpreds,
                    response_mask=response_mask,
                    teacher_response_mask=teacher_response_mask,
                )
                
                # 4. åå‘ä¼ æ’­
                loss = d_loss / self.gradient_accumulation
                loss.backward()
                
                # 5. è®°å½•æŒ‡æ ‡
                metrics = {
                    "critic/d_loss": d_loss.item(),
                    "critic/d_acc": d_acc.item(),
                    "critic/student_value_mean": ...,
                    "critic/teacher_value_mean": ...,
                }
            
            # 6. ä¼˜åŒ–å™¨æ­¥è¿›
            self._optimizer_step()
```

### 5.3 åˆ¤åˆ«å™¨æŸå¤±å‡½æ•°

**ä½ç½®**: `verl/trainer/ppo/core_algos.py:850-854`

```python
def compute_discriminator_loss(student_vpreds, teacher_vpreds, 
                               response_mask, teacher_response_mask):
    # è®¡ç®—æ€»åˆ†æ•°
    teacher_reward = torch.sum(teacher_vpreds * teacher_response_mask, dim=-1)
    student_reward = torch.sum(student_vpreds * response_mask, dim=-1)
    
    # å¯¹æŠ—æŸå¤±
    d_loss = -nn.functional.logsigmoid(teacher_reward - student_reward).mean()
    return d_loss
```

**æ•°å­¦åŸç†**ï¼š

```
sigmoid(x) = 1 / (1 + exp(-x))

d_loss = -log(sigmoid(teacher_reward - student_reward))
       = log(1 + exp(student_reward - teacher_reward))
```

**ä¼˜åŒ–ç›®æ ‡**ï¼š
- æœ€å°åŒ– `d_loss`
- ç­‰ä»·äºæœ€å¤§åŒ– `teacher_reward - student_reward`
- è®©æ•™å¸ˆå¾—åˆ†å°½å¯èƒ½é«˜äºå­¦ç”Ÿå¾—åˆ†

**æ¢¯åº¦æ–¹å‘**ï¼š
- å¯¹æ•™å¸ˆå›å¤ï¼šæ¢¯åº¦ä¸ºè´Ÿï¼Œé¼“åŠ±ç»™**æ›´é«˜åˆ†æ•°**
- å¯¹å­¦ç”Ÿå›å¤ï¼šæ¢¯åº¦ä¸ºæ­£ï¼Œé¼“åŠ±ç»™**æ›´ä½åˆ†æ•°**

### 5.4 åˆ¤åˆ«å™¨å‰å‘æ¨ç†

**ä½ç½®**: `verl/workers/critic/dp_critic.py:58-149`

```python
def _forward_micro_batch(self, micro_batch, compute_teacher):
    # æ ¹æ® compute_teacher é€‰æ‹©è¾“å…¥
    if compute_teacher:
        input_ids = micro_batch["teacher_input_ids"]
        attention_mask = micro_batch["teacher_attention_mask"]
        response_length = micro_batch["teacher_response"].size(-1)
    else:
        input_ids = micro_batch["input_ids"]
        attention_mask = micro_batch["attention_mask"]
        response_length = micro_batch["responses"].size(-1)
    
    # å‰å‘æ¨ç†
    with torch.autocast(device_type=self.device_name, dtype=torch.bfloat16):
        output = self.critic_module(
            input_ids=input_ids,
            attention_mask=attention_mask,
            position_ids=position_ids,
            use_cache=False,
        )
        values = output.logits
        
        # å…³é”®ï¼šåªä¿ç•™æœ€åä¸€ä¸ªæœ‰æ•ˆ token çš„å€¼
        values = values[:, -response_length:]
        response_mask = attention_mask[:, -response_length:]
        response_lengths = response_mask.sum(dim=1).long()
        last_token_indices = response_lengths - 1
        
        # åˆ›å»º maskï¼šåªæœ‰æœ€åä¸€ä¸ª token ä¸º True
        last_token_mask = torch.zeros_like(response_mask, dtype=torch.bool)
        batch_indices = torch.arange(response_mask.size(0))
        last_token_mask[batch_indices, last_token_indices] = True
        
        # åªä¿ç•™æœ€å token çš„å€¼
        values = values * last_token_mask.type_as(values)
    
    return values
```

**ä¸ºä»€ä¹ˆåªä¿ç•™æœ€å tokenï¼Ÿ**
- åˆ¤åˆ«å™¨è¢«æ”¹é€ ä¸º**åºåˆ—çº§å¥–åŠ±æ¨¡å‹**
- æ•´ä¸ªå›å¤çš„è´¨é‡ç”¨ä¸€ä¸ªæ ‡é‡è¡¨ç¤º
- è¿™ä¸ªæ ‡é‡æ”¾åœ¨æœ€åä¸€ä¸ªæœ‰æ•ˆ token çš„ä½ç½®

---

## å…­ã€GRPO ä¼˜åŠ¿è®¡ç®—ï¼ˆä»…ç”¨äº Actorï¼‰

### 6.1 GRPO ä¼˜åŠ¿çš„ä½œç”¨

GRPO ä¼˜åŠ¿**ä»…ç”¨äº Actor æ›´æ–°**ï¼Œä¸ç”¨äºåˆ¤åˆ«å™¨æ›´æ–°ã€‚

### 6.2 è®¡ç®—æµç¨‹

**ä½ç½®**: `verl/trainer/ppo/core_algos.py:202-263`

```python
def compute_grpo_outcome_advantage(token_level_rewards, response_mask, 
                                   index, norm_adv_by_std_in_grpo=True):
    # 1. è·å–æ¯ä¸ªå›å¤çš„æ€»åˆ†æ•°ï¼ˆæ¥è‡ªåˆ¤åˆ«å™¨ï¼‰
    scores = token_level_rewards.sum(dim=-1)
    
    # 2. æŒ‰ uid åˆ†ç»„ï¼ˆåŒä¸€ prompt çš„ 8 ä¸ªå›å¤ï¼‰
    id2score = defaultdict(list)
    for i in range(bsz):
        id2score[index[i]].append(scores[i])
    
    # 3. è®¡ç®—ç»„å†…ç»Ÿè®¡é‡
    id2mean = {}
    id2std = {}
    for idx in id2score:
        id2mean[idx] = torch.mean(torch.tensor(id2score[idx]))
        id2std[idx] = torch.std(torch.tensor([id2score[idx]]))
    
    # 4. æ ‡å‡†åŒ–ä¼˜åŠ¿ï¼ˆç›¸å¯¹äºç»„å†…å¹³å‡ï¼‰
    for i in range(bsz):
        if norm_adv_by_std_in_grpo:
            scores[i] = (scores[i] - id2mean[index[i]]) / (id2std[index[i]] + eps)
        else:
            scores[i] = scores[i] - id2mean[index[i]]
    
    # 5. å¹¿æ’­åˆ° token çº§åˆ«
    advantages = scores.unsqueeze(-1) * response_mask
    return advantages, advantages
```

### 6.3 GRPO ç¤ºä¾‹

```
å‡è®¾ä¸€ä¸ª prompt ç”Ÿæˆäº† 8 ä¸ªå›å¤ï¼Œåˆ¤åˆ«å™¨ç»™åˆ†ï¼š
scores = [0.5, 0.6, 0.3, 0.7, 0.4, 0.8, 0.5, 0.6]

ç»„å†…å‡å€¼: mean = 0.55
ç»„å†…æ ‡å‡†å·®: std = 0.15

GRPO ä¼˜åŠ¿:
advantages[0] = (0.5 - 0.55) / 0.15 = -0.33  (ä½äºå¹³å‡ï¼Œè´Ÿä¼˜åŠ¿)
advantages[1] = (0.6 - 0.55) / 0.15 = +0.33  (é«˜äºå¹³å‡ï¼Œæ­£ä¼˜åŠ¿)
advantages[3] = (0.7 - 0.55) / 0.15 = +1.00  (è¿œé«˜äºå¹³å‡)
advantages[5] = (0.8 - 0.55) / 0.15 = +1.67  (æœ€é«˜åˆ†ï¼Œæœ€å¤§æ­£ä¼˜åŠ¿)
```

**GRPO çš„ä½œç”¨**ï¼š
- å¯¹åŒä¸€ prompt çš„å¤šä¸ªå›å¤è¿›è¡Œ**ç›¸å¯¹æ¯”è¾ƒ**
- é«˜äºå¹³å‡çš„å›å¤è·å¾—æ­£ä¼˜åŠ¿ï¼ˆé¼“åŠ±ï¼‰
- ä½äºå¹³å‡çš„å›å¤è·å¾—è´Ÿä¼˜åŠ¿ï¼ˆæƒ©ç½šï¼‰
- å‡å°‘æ–¹å·®ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§

---

## ä¸ƒã€Actor æ›´æ–°ï¼ˆä»…åœ¨ Warmup æœŸåï¼‰

### 7.1 Warmup æœºåˆ¶

```python
# åªæœ‰åœ¨ warmup æœŸè¿‡åæ‰æ›´æ–° Actor
if self.config.trainer.critic_warmup <= self.global_steps:
    actor_output = self.actor_rollout_wg.update_actor(batch)
```

**Warmup æœŸæ—¶é—´çº¿**ï¼š
```
Step 0-9 (critic_warmup=10):
â”œâ”€ ç”Ÿæˆå­¦ç”Ÿå›å¤ (n=8)
â”œâ”€ åˆ¤åˆ«å™¨æ‰“åˆ†
â”œâ”€ è®¡ç®— GRPO ä¼˜åŠ¿
â”œâ”€ âœ… æ›´æ–°åˆ¤åˆ«å™¨
â””â”€ âŒ ä¸æ›´æ–° Actor

Step 10+:
â”œâ”€ ç”Ÿæˆå­¦ç”Ÿå›å¤ (n=8)
â”œâ”€ åˆ¤åˆ«å™¨æ‰“åˆ†
â”œâ”€ è®¡ç®— GRPO ä¼˜åŠ¿
â”œâ”€ âœ… æ›´æ–°åˆ¤åˆ«å™¨
â””â”€ âœ… æ›´æ–° Actor
```

### 7.2 Actor æ›´æ–°ä½¿ç”¨çš„æ•°æ®

```python
{
    "responses": ...,           # å­¦ç”Ÿå›å¤
    "old_log_probs": ...,       # æ—§ç­–ç•¥çš„ log æ¦‚ç‡
    "advantages": ...,          # GRPO ä¼˜åŠ¿
    "returns": ...,             # å›æŠ¥ï¼ˆåœ¨ GRPO ä¸­ç­‰äº advantagesï¼‰
}
```

### 7.3 PPO æŸå¤±å‡½æ•°

```python
def compute_policy_loss(log_probs, old_log_probs, advantages, cliprange):
    # è®¡ç®—é‡è¦æ€§é‡‡æ ·æ¯”ç‡
    ratio = torch.exp(log_probs - old_log_probs)
    
    # PPO clipped loss
    pg_losses1 = -advantages * ratio
    pg_losses2 = -advantages * torch.clamp(
        ratio, 1.0 - cliprange, 1.0 + cliprange
    )
    pg_loss = torch.max(pg_losses1, pg_losses2).mean()
    
    return pg_loss
```

---

## å…«ã€ä¸¤ä¸ªæ›´æ–°è¿‡ç¨‹å¯¹æ¯”

### 8.1 åˆ¤åˆ«å™¨æ›´æ–° vs Actor æ›´æ–°

| ç»´åº¦ | åˆ¤åˆ«å™¨æ›´æ–° (Critic) | Actor æ›´æ–° (Student Model) |
|------|-------------------|---------------------------|
| **è¾“å…¥æ•°æ®** | Student + Teacher Responses | Student Responses |
| **æŸå¤±å‡½æ•°** | `-log(sigmoid(T_score - S_score))` | `PPO Loss + KL Loss` |
| **ä¼˜åŒ–ç›®æ ‡** | åŒºåˆ†æ•™å¸ˆå’Œå­¦ç”Ÿ | å¢åŠ é«˜ä¼˜åŠ¿å›å¤æ¦‚ç‡ |
| **æ˜¯å¦ä½¿ç”¨ä¼˜åŠ¿** | âŒ ä¸ä½¿ç”¨ | âœ… ä½¿ç”¨ GRPO ä¼˜åŠ¿ |
| **æ˜¯å¦ä½¿ç”¨æ•™å¸ˆå›å¤** | âœ… ä½¿ç”¨ | âŒ ä¸ä½¿ç”¨ |
| **æ›´æ–°æ—¶æœº** | æ¯ä¸ª step | Step > 10 å |

### 8.2 å®Œæ•´ä¿¡æ¯æµå›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ•°æ®å‡†å¤‡é˜¶æ®µ                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                                   â†“
  [Prompt]                          [Teacher Response]
        â”‚                                   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Actor ç”Ÿæˆå­¦ç”Ÿå›å¤                           â”‚
â”‚  Prompt â†’ Actor â†’ [Student Response 1-8]                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           åˆ¤åˆ«å™¨è¯„åˆ†ï¼ˆä¸º GRPO æä¾›åŸå§‹åˆ†æ•°ï¼‰               â”‚
â”‚  Student Responses â†’ Discriminator â†’ [Scores]            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              GRPO è®¡ç®—ç›¸å¯¹ä¼˜åŠ¿                            â”‚
â”‚  Scores â†’ Group Normalize â†’ [Advantages]                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ›´æ–°åˆ¤åˆ«å™¨      â”‚          â”‚   æ›´æ–° Actor     â”‚
â”‚  (å§‹ç»ˆæ‰§è¡Œ)      â”‚          â”‚  (Step > 10)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                               â”‚
        â†“                               â†“
    ä½¿ç”¨åˆ¤åˆ«å™¨æŸå¤±                   ä½¿ç”¨ PPO æŸå¤±
   (ä¸ä¾èµ–ä¼˜åŠ¿)                     (ä¾èµ–ä¼˜åŠ¿)
```

---

## ä¹ã€ç›‘æ§æŒ‡æ ‡

### 9.1 åˆ¤åˆ«å™¨æŒ‡æ ‡

- **`critic/d_loss`**: åˆ¤åˆ«å™¨æŸå¤±
  - åˆæœŸè¾ƒå¤§ï¼ˆåˆ¤åˆ«èƒ½åŠ›å¼±ï¼‰
  - é€æ¸ä¸‹é™ï¼ˆå­¦ä¼šåŒºåˆ†ï¼‰
  - ç†æƒ³å€¼ï¼š0.3-0.5

- **`critic/d_acc`**: åˆ¤åˆ«å‡†ç¡®ç‡
  - æ•™å¸ˆå¾—åˆ† > å­¦ç”Ÿå¾—åˆ†çš„æ¯”ä¾‹
  - åˆæœŸæ¥è¿‘ 0.5ï¼ˆéšæœºçŒœæµ‹ï¼‰
  - é€æ¸ä¸Šå‡ï¼ˆå­¦ä¼šåˆ¤åˆ«ï¼‰
  - ç†æƒ³å€¼ï¼š> 0.7

- **`critic/student_value_mean`**: å­¦ç”Ÿå›å¤å¹³å‡åˆ†
  - åˆæœŸè¾ƒä½
  - éšè®­ç»ƒé€æ¸ä¸Šå‡

- **`critic/teacher_value_mean`**: æ•™å¸ˆå›å¤å¹³å‡åˆ†
  - åº”ä¿æŒç¨³å®šä¸”è¾ƒé«˜
  - ä½œä¸ºå­¦ç”Ÿçš„ç›®æ ‡

### 9.2 Actor æŒ‡æ ‡ï¼ˆStep > 10 åï¼‰

- **`actor/loss`**: PPO ç­–ç•¥æŸå¤±
- **`actor/kl`**: ä¸å‚è€ƒç­–ç•¥çš„ KL æ•£åº¦
- **`actor/entropy`**: ç­–ç•¥ç†µ
- **`actor/clipfrac`**: è¢«è£å‰ªçš„æ¯”ä¾‹

### 9.3 æ•°æ®æŒ‡æ ‡

- **`data/response_length`**: ç”Ÿæˆå›å¤çš„å¹³å‡é•¿åº¦
- **`training/global_token_num`**: æœ‰æ•ˆ token æ•°é‡

---

## åã€Warmup é˜¶æ®µçš„è®­ç»ƒç›®æ ‡

### 10.1 çŸ­æœŸç›®æ ‡ï¼ˆå‰ 10 æ­¥ï¼‰

1. åˆ¤åˆ«å™¨å­¦ä¼šåŒºåˆ†æ•™å¸ˆå’Œå­¦ç”Ÿå›å¤
2. `d_acc` ä» 0.5 ä¸Šå‡åˆ° > 0.6
3. å»ºç«‹ç¨³å®šçš„å¥–åŠ±ä¿¡å·

### 10.2 ä¸­æœŸç›®æ ‡ï¼ˆ10-800 æ­¥ï¼‰

1. åˆ¤åˆ«å™¨æŒç»­æ”¹è¿›åˆ¤åˆ«èƒ½åŠ›
2. Actor æ ¹æ®åˆ¤åˆ«å™¨åé¦ˆä¼˜åŒ–ç­–ç•¥
3. å­¦ç”Ÿå›å¤è´¨é‡é€æ¸æå‡

### 10.3 é•¿æœŸç›®æ ‡ï¼ˆæ•´ä¸ª Warmupï¼‰

1. `d_acc` è¾¾åˆ° 0.7-0.8
2. å­¦ç”Ÿå›å¤åˆ†æ•°æ¥è¿‘æ•™å¸ˆï¼ˆä½†ä»æœ‰å·®è·ï¼‰
3. ä¸º GAD Stage 2 çš„å®Œå…¨å¯¹æŠ—è®­ç»ƒåšå¥½å‡†å¤‡

---

## åä¸€ã€å…³é”®è®¾è®¡ç»†èŠ‚

### 11.1 ä¸ºä»€ä¹ˆåˆ¤åˆ«å™¨ä¸ä½¿ç”¨ GRPO ä¼˜åŠ¿ï¼Ÿ

**åˆ¤åˆ«å™¨çš„è®­ç»ƒç›®æ ‡**ï¼š
- å­¦ä¹ ä¸€ä¸ªè¯„åˆ†å‡½æ•°ï¼š`f(response) â†’ score`
- ä½¿å¾—ï¼š`f(teacher_response) > f(student_response)`
- è¿™æ˜¯ä¸€ä¸ª**äºŒåˆ†ç±»é—®é¢˜**

**GRPO ä¼˜åŠ¿çš„å±€é™æ€§**ï¼š
- ä¼˜åŠ¿æ˜¯**ç›¸å¯¹çš„**ï¼ˆç›¸å¯¹äºç»„å†…å¹³å‡ï¼‰
- ä¸åŒ prompt çš„ä¼˜åŠ¿ä¸å¯æ¯”è¾ƒ
- ä¼˜åŠ¿å¯èƒ½ä¸ºæ­£ï¼Œä½†ç»å¯¹åˆ†æ•°ä»ç„¶å¾ˆä½

**åˆ¤åˆ«å™¨éœ€è¦å­¦ä¹ ç»å¯¹è´¨é‡**ï¼Œè€Œä¸æ˜¯ç›¸å¯¹æ’åã€‚

### 11.2 ä¸ºä»€ä¹ˆéœ€è¦ Critic Warmupï¼Ÿ

**Warmup æœŸçš„æ„ä¹‰**ï¼š
1. è®©åˆ¤åˆ«å™¨å…ˆå»ºç«‹åˆæ­¥çš„åˆ¤åˆ«èƒ½åŠ›
2. é¿å…è®­ç»ƒåˆæœŸåˆ¤åˆ«å™¨å’Œ Actor éƒ½ä¸ç¨³å®š
3. ç»™åˆ¤åˆ«å™¨ä¸€ä¸ª"é¢†å…ˆä¼˜åŠ¿"
4. å»ºç«‹ç¨³å®šçš„å¥–åŠ±ä¿¡å·

**è¯¾ç¨‹å­¦ä¹ ç­–ç•¥**ï¼š
- å…ˆæ˜“åéš¾
- åˆ¤åˆ«å™¨å…ˆè¡Œ
- é€æ­¥å¼•å…¥å¯¹æŠ—

---

## åäºŒã€æ€»ç»“

### 12.1 Warmup é˜¶æ®µçš„æœ¬è´¨

1. **å·²ç»æ˜¯å¯¹æŠ—è®­ç»ƒ**ï¼Œä¸æ˜¯ä¼ ç»Ÿçš„ RL
2. ä½¿ç”¨åˆ¤åˆ«å™¨æŸå¤±ï¼Œè€Œé value loss
3. é€šè¿‡ `critic_warmup=10` è®©åˆ¤åˆ«å™¨å…ˆå»ºç«‹èƒ½åŠ›
4. ä¸ºåç»­çš„ GAD Stage 2 æ‰“å¥½åŸºç¡€

### 12.2 æ ¸å¿ƒè¦ç‚¹

- **åˆ¤åˆ«å™¨æ›´æ–°ä¸ä¾èµ– GRPO ä¼˜åŠ¿**
  - ä½¿ç”¨åˆ¤åˆ«å™¨æŸå¤±ï¼š`-log(sigmoid(T_score - S_score))`
  - ç›´æ¥å¯¹æ¯”æ•™å¸ˆå’Œå­¦ç”Ÿå›å¤
  - å­¦ä¹ ç»å¯¹è´¨é‡è¯„ä¼°

- **GRPO ä¼˜åŠ¿ä»…ç”¨äº Actor æ›´æ–°**
  - å°†åˆ¤åˆ«å™¨çš„ç»å¯¹åˆ†æ•°è½¬æ¢ä¸ºç›¸å¯¹ä¼˜åŠ¿
  - ç”¨äº PPO æŸå¤±è®¡ç®—
  - æŒ‡å¯¼å­¦ç”Ÿæ¨¡å‹æ”¹è¿›

- **ä¸¤ä¸ªæ›´æ–°è¿‡ç¨‹ç‹¬ç«‹**
  - åˆ¤åˆ«å™¨ï¼šå­¦ä¹ åŒºåˆ†èƒ½åŠ›
  - Actorï¼šå­¦ä¹ ç”Ÿæˆèƒ½åŠ›
  - é€šè¿‡åˆ¤åˆ«å™¨çš„åˆ†æ•°è¿æ¥

### 12.3 ä¸ GAD Stage 2 çš„åŒºåˆ«

| ç»´åº¦ | Warmup (Stage 1) | GAD (Stage 2) |
|------|------------------|---------------|
| **Critic Warmup** | 10 steps | 0 steps |
| **åˆå§‹åŒ–** | ä»é¢„è®­ç»ƒæ¨¡å‹ | ä» Warmup checkpoint |
| **è®­ç»ƒéš¾åº¦** | ç®€å•ï¼ˆåˆ¤åˆ«å™¨æœ‰ä¼˜åŠ¿ï¼‰ | å›°éš¾ï¼ˆå®Œå…¨å¯¹æŠ—ï¼‰ |
| **è®­ç»ƒæ—¶é•¿** | 2 epochs (~800 steps) | 4 epochs |

### 12.4 è®­ç»ƒç­–ç•¥

- **è¯¾ç¨‹å­¦ä¹ **ï¼šå…ˆæ˜“åéš¾
- **åˆ¤åˆ«å™¨å…ˆè¡Œ**ï¼šå»ºç«‹ç¨³å®šçš„å¥–åŠ±ä¿¡å·
- **å¯¹æŠ—è®­ç»ƒ**ï¼šå­¦ç”Ÿå’Œåˆ¤åˆ«å™¨ç›¸äº’åšå¼ˆ

è¿™æ˜¯ä¸€ä¸ªç²¾å¿ƒè®¾è®¡çš„è®­ç»ƒæµç¨‹ï¼Œä¸º GAD çš„æˆåŠŸå¥ å®šäº†åŸºç¡€ï¼
