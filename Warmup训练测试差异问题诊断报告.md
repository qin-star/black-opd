# Warmupè®­ç»ƒä¸æµ‹è¯•å‡†ç¡®ç‡å·®å¼‚é—®é¢˜è¯Šæ–­æŠ¥å‘Š

## ğŸš¨ é—®é¢˜ä¸¥é‡æ€§è¯„ä¼°ï¼šé«˜

## å…³é”®äº‹å®

1. **Warmupé˜¶æ®µActorå®Œå…¨å†»ç»“**ï¼Œåªè®­ç»ƒCritic
2. **è®­ç»ƒå’Œæµ‹è¯•ä½¿ç”¨å®Œå…¨ç›¸åŒçš„æ•°æ®é›†**
3. **æµ‹è¯•æ˜¯ä»è®­ç»ƒé›†éšæœºé‡‡æ ·100æ¡**
4. **Studentæ¨¡å‹åœ¨è®­ç»ƒå‰åå®Œå…¨ç›¸åŒ**

## æ ¸å¿ƒçŸ›ç›¾

```
ç›¸åŒçš„æ•°æ®é›† + ç›¸åŒçš„Studentæ¨¡å‹ + ç›¸åŒçš„Criticæ¨¡å‹
â†“
ä¸ºä»€ä¹ˆå‡†ç¡®ç‡å·®å¼‚å¦‚æ­¤å·¨å¤§ï¼Ÿ

è®­ç»ƒé›†ï¼ˆå…¨é‡ï¼‰: d_acc = 76.8%, score_diff = 5.85
æµ‹è¯•é›†ï¼ˆé‡‡æ ·100æ¡ï¼‰: d_acc = 45.75%, score_diff = 0.01
```

**è¿™è¯´æ˜å­˜åœ¨ä¸¥é‡é—®é¢˜ï¼**

---

## ä¸€ã€é—®é¢˜æ ¹æºåˆ†æ

### 1.1 æ’é™¤çš„å¯èƒ½æ€§

âŒ **æ•°æ®é›†ä¸åŒ** - å·²ç¡®è®¤ä½¿ç”¨ç›¸åŒæ•°æ®é›†
âŒ **Studentæ¨¡å‹å˜åŒ–** - Warmupé˜¶æ®µActorå†»ç»“
âŒ **ä»£ç å®ç°ä¸ä¸€è‡´** - å·²éªŒè¯è®¡ç®—æ–¹å¼ç›¸åŒ

### 1.2 å‰©ä½™çš„å¯èƒ½åŸå› 

#### ğŸ”´ åŸå› 1ï¼šè®­ç»ƒæ—¶ä½¿ç”¨çš„æ˜¯å®æ—¶ç”Ÿæˆçš„Student Responseï¼ˆæœ€å¯èƒ½ï¼‰

**å…³é”®å‘ç°ï¼š**

æŸ¥çœ‹è®­ç»ƒé…ç½®ï¼š
```bash
rollout_name=vllm
n_resp_per_prompt=4  # æ¯ä¸ªpromptç”Ÿæˆ4ä¸ªstudent response
temperature=0.6
```

**è¿™è¯´æ˜ä»€ä¹ˆï¼Ÿ**

```python
# è®­ç»ƒæ—¶çš„æ•°æ®æµ
è®­ç»ƒè¿‡ç¨‹ï¼š
1. è¯»å–promptå’Œteacher_responseï¼ˆæ¥è‡ªæ•°æ®é›†ï¼‰
2. ä½¿ç”¨vLLMå®æ—¶ç”Ÿæˆstudent_responseï¼ˆtemperature=0.6ï¼‰
3. Criticå¯¹æ¯”teacher_responseå’Œstudent_response
4. è®¡ç®—d_accå’Œloss

å…³é”®ç‚¹ï¼š
- Student responseæ˜¯å®æ—¶ç”Ÿæˆçš„ï¼Œæ¯æ¬¡éƒ½ä¸åŒï¼
- å³ä½¿Actorå†»ç»“ï¼Œç”Ÿæˆçš„responseä¹Ÿæœ‰éšæœºæ€§ï¼ˆtemperature=0.6ï¼‰
- è®­ç»ƒæ—¶çœ‹åˆ°çš„student_response â‰  æµ‹è¯•æ—¶çœ‹åˆ°çš„student_response
```

**æµ‹è¯•æ—¶çš„æ•°æ®æµï¼š**

```python
# æµ‹è¯•è„šæœ¬
æµ‹è¯•è¿‡ç¨‹ï¼š
1. è¯»å–promptå’Œteacher_responseï¼ˆæ¥è‡ªæ•°æ®é›†ï¼‰
2. è°ƒç”¨APIç”Ÿæˆstudent_responseï¼ˆtemperature=0.8ï¼‰
3. Criticå¯¹æ¯”teacher_responseå’Œstudent_response
4. è®¡ç®—å‡†ç¡®ç‡

å…³é”®ç‚¹ï¼š
- Student responseä¹Ÿæ˜¯å®æ—¶ç”Ÿæˆçš„
- ä½†ç”Ÿæˆçš„å†…å®¹ä¸è®­ç»ƒæ—¶ä¸åŒï¼ˆéšæœºæ€§ï¼‰
- temperatureä¹Ÿä¸åŒï¼ˆ0.6 vs 0.8ï¼‰
```

**é—®é¢˜æ‰€åœ¨ï¼š**

```
è®­ç»ƒæ—¶ï¼š
- Criticçœ‹åˆ°çš„æ˜¯temperature=0.6ç”Ÿæˆçš„responses
- å¯èƒ½è´¨é‡è¾ƒä½ï¼Œå¤šæ ·æ€§è¾ƒå°
- Criticå­¦ä¼šåŒºåˆ†è¿™äº›ç‰¹å®šçš„responses

æµ‹è¯•æ—¶ï¼š
- Criticçœ‹åˆ°çš„æ˜¯temperature=0.8ç”Ÿæˆçš„responses
- è´¨é‡å’Œé£æ ¼å¯èƒ½ä¸åŒ
- Criticçš„åˆ¤æ–­æ ‡å‡†å¯èƒ½ä¸é€‚ç”¨
```

#### ğŸ”´ åŸå› 2ï¼šè®­ç»ƒæ—¶çš„æ‰¹æ¬¡æ•ˆåº”

**è®­ç»ƒé…ç½®ï¼š**
```bash
train_batch_size=128
ppo_mini_batch_size=64
n_resp_per_prompt=4
```

**åˆ†æï¼š**

```python
# è®­ç»ƒæ—¶çš„æ‰¹æ¬¡æ„æˆ
æ¯ä¸ªbatchï¼š
- 128ä¸ªæ ·æœ¬
- æ¯ä¸ªpromptç”Ÿæˆ4ä¸ªresponses
- å®é™…ä¸Šæ˜¯32ä¸ªä¸åŒçš„prompts Ã— 4ä¸ªresponses

Criticçœ‹åˆ°çš„å¯¹æ¯”ï¼š
- åŒä¸€ä¸ªpromptçš„4ä¸ªä¸åŒstudent responses
- ä¸åŒä¸€ä¸ªteacher responseå¯¹æ¯”
- å¯èƒ½å­¦ä¼šäº†"ç›¸å¯¹æ’åº"è€Œé"ç»å¯¹è´¨é‡åˆ¤æ–­"

æµ‹è¯•æ—¶ï¼š
- æ¯ä¸ªpromptåªç”Ÿæˆ8ä¸ªresponses
- ä½†æ˜¯ç‹¬ç«‹è¯„ä¼°ï¼Œæ²¡æœ‰æ‰¹æ¬¡å†…çš„å¯¹æ¯”
- Criticçš„ç›¸å¯¹æ’åºèƒ½åŠ›æ— æ³•å‘æŒ¥
```

#### ğŸ”´ åŸå› 3ï¼šåˆ†æ•°åˆ†å¸ƒçš„å·¨å¤§å·®å¼‚ï¼ˆæœ€ä¸¥é‡çš„é—®é¢˜ï¼‰

**è§‚å¯Ÿåˆ°çš„ç°è±¡ï¼š**

```python
è®­ç»ƒæ—¶ï¼š
Teacherå¹³å‡åˆ†: 2.77
Studentå¹³å‡åˆ†: -3.08
åˆ†å·®: 5.85

æµ‹è¯•æ—¶ï¼ˆç›¸åŒæ•°æ®é›†ï¼ï¼‰ï¼š
Teacherå¹³å‡åˆ†: -0.50
Studentå¹³å‡åˆ†: -0.49
åˆ†å·®: 0.01
```

**è¿™æ˜¯æå…¶å¼‚å¸¸çš„ï¼**

å¦‚æœæ˜¯ç›¸åŒçš„æ•°æ®é›†ï¼Œç›¸åŒçš„Criticæ¨¡å‹ï¼Œä¸ºä»€ä¹ˆï¼š
1. Teacheråˆ†æ•°ä»2.77å˜æˆ-0.50ï¼ˆå·®3.27ï¼‰
2. Studentåˆ†æ•°ä»-3.08å˜æˆ-0.49ï¼ˆå·®2.59ï¼‰
3. åˆ†å·®ä»5.85å˜æˆ0.01ï¼ˆå·®5.84ï¼‰

**å¯èƒ½çš„è§£é‡Šï¼š**

##### è§£é‡ŠAï¼šè®­ç»ƒæ—¶çš„åˆ†æ•°æ˜¯èšåˆç»Ÿè®¡

```python
# è®­ç»ƒæ—¥å¿—ä¸­çš„åˆ†æ•°å¯èƒ½æ˜¯ï¼š
critic/teacher_score_mean: 2.772  # æ‰€æœ‰è®­ç»ƒæ­¥éª¤çš„å¹³å‡ï¼Ÿ
critic/student_score_mean: -3.077  # æ‰€æœ‰è®­ç»ƒæ­¥éª¤çš„å¹³å‡ï¼Ÿ

# è€Œä¸æ˜¯æœ€åä¸€æ­¥çš„åˆ†æ•°
# å¦‚æœæ˜¯è¿™æ ·ï¼Œéœ€è¦çœ‹æœ€åä¸€æ­¥çš„å…·ä½“åˆ†æ•°
```

##### è§£é‡ŠBï¼šè®­ç»ƒè¿‡ç¨‹ä¸­åˆ†æ•°åˆ†å¸ƒå‘ç”Ÿäº†å·¨å¤§å˜åŒ–

```python
# å¯èƒ½çš„è®­ç»ƒæ›²çº¿
Step 1:   Teacher=5.0,  Student=-5.0,  diff=10.0
Step 100: Teacher=3.0,  Student=-3.0,  diff=6.0
Step 200: Teacher=1.0,  Student=-1.0,  diff=2.0
Step 310: Teacher=-0.5, Student=-0.5,  diff=0.0

# æ—¥å¿—ä¸­çš„2.77å’Œ-3.08å¯èƒ½æ˜¯æ—©æœŸæ­¥éª¤çš„å€¼
# æœ€åä¸€æ­¥å¯èƒ½å·²ç»æ¥è¿‘0
```

##### è§£é‡ŠCï¼šCriticçš„è¯„åˆ†æ ‡å‡†å´©æºƒ

```python
# æœ€ä¸¥é‡çš„æƒ…å†µ
è®­ç»ƒåˆæœŸï¼š
- Criticå­¦ä¼šäº†åŒºåˆ†ï¼ˆåˆ†å·®å¤§ï¼‰

è®­ç»ƒåæœŸï¼š
- Criticè¿‡æ‹Ÿåˆæˆ–å´©æºƒ
- å¯¹æ‰€æœ‰æ ·æœ¬éƒ½ç»™ç›¸ä¼¼çš„åˆ†æ•°
- å¤±å»äº†åŒºåˆ†èƒ½åŠ›
```

---

## äºŒã€éªŒè¯å®éªŒè®¾è®¡

### å®éªŒ1ï¼šæ£€æŸ¥è®­ç»ƒæ—¥å¿—çš„æœ€åä¸€æ­¥

**ç›®æ ‡ï¼š** ç¡®è®¤2.77å’Œ-3.08æ˜¯æœ€åä¸€æ­¥çš„å€¼è¿˜æ˜¯å¹³å‡å€¼

```python
# æŸ¥çœ‹TensorBoardæˆ–æ—¥å¿—æ–‡ä»¶
# æ‰¾åˆ°step=310çš„å…·ä½“æŒ‡æ ‡
# ç¡®è®¤ï¼š
# - critic/teacher_score_mean åœ¨step 310çš„å€¼
# - critic/student_score_mean åœ¨step 310çš„å€¼
# - critic/score_diff åœ¨step 310çš„å€¼
```

**å¦‚æœæœ€åä¸€æ­¥çš„åˆ†æ•°æ¥è¿‘0ï¼š**
- è¯´æ˜Criticåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é€æ¸å¤±å»äº†åŒºåˆ†èƒ½åŠ›
- è¿™æ˜¯ä¸¥é‡çš„è®­ç»ƒé—®é¢˜

**å¦‚æœæœ€åä¸€æ­¥çš„åˆ†æ•°ä»ç„¶æ˜¯2.77å’Œ-3.08ï¼š**
- è¯´æ˜æµ‹è¯•è„šæœ¬æœ‰é—®é¢˜
- æˆ–è€…ç”Ÿæˆçš„responseså®Œå…¨ä¸åŒ

### å®éªŒ2ï¼šä½¿ç”¨è®­ç»ƒæ—¶ä¿å­˜çš„responsesè¿›è¡Œæµ‹è¯•

**ç›®æ ‡ï¼š** æ’é™¤responseç”Ÿæˆéšæœºæ€§çš„å½±å“

```python
# ä¿®æ”¹è®­ç»ƒä»£ç ï¼Œä¿å­˜ä¸€äº›æ ·æœ¬çš„responses
# åœ¨è®­ç»ƒçš„æœ€åä¸€æ­¥ï¼š
save_samples = {
    'prompts': prompts[:10],
    'teacher_responses': teacher_responses[:10],
    'student_responses': student_responses[:10],
    'teacher_scores': teacher_scores[:10],
    'student_scores': student_scores[:10],
}
torch.save(save_samples, 'training_samples.pt')

# ç„¶ååœ¨æµ‹è¯•è„šæœ¬ä¸­åŠ è½½è¿™äº›æ ·æœ¬
# ä½¿ç”¨å®Œå…¨ç›¸åŒçš„responsesè¿›è¡Œæµ‹è¯•
# çœ‹åˆ†æ•°æ˜¯å¦ä¸€è‡´
```

**é¢„æœŸç»“æœï¼š**
- å¦‚æœåˆ†æ•°ä¸€è‡´ â†’ é—®é¢˜åœ¨äºresponseç”Ÿæˆçš„éšæœºæ€§
- å¦‚æœåˆ†æ•°ä¸ä¸€è‡´ â†’ é—®é¢˜åœ¨äºCriticæ¨¡å‹æœ¬èº«

### å®éªŒ3ï¼šæ£€æŸ¥Criticæ¨¡å‹çš„åŠ è½½

**ç›®æ ‡ï¼š** ç¡®è®¤æµ‹è¯•æ—¶ä½¿ç”¨çš„æ˜¯æ­£ç¡®çš„Criticæ¨¡å‹

```python
# åœ¨æµ‹è¯•è„šæœ¬ä¸­æ·»åŠ 
print("Criticæ¨¡å‹è·¯å¾„:", critic_config['model_path'])
print("Criticæ¨¡å‹å‚æ•°æ•°é‡:", sum(p.numel() for p in self.critic_model.parameters()))

# æ£€æŸ¥æ¨¡å‹çš„æŸä¸ªå‚æ•°å€¼
print("ç¬¬ä¸€å±‚æƒé‡:", self.critic_model.pretrained_model.model.layers[0].self_attn.q_proj.weight[0, :5])

# åœ¨è®­ç»ƒç»“æŸæ—¶ä¹Ÿæ‰“å°ç›¸åŒçš„ä¿¡æ¯
# å¯¹æ¯”æ˜¯å¦ä¸€è‡´
```

### å®éªŒ4ï¼šé€æ­¥è°ƒè¯•æµ‹è¯•è„šæœ¬

**ç›®æ ‡ï¼š** æ‰¾å‡ºæµ‹è¯•è„šæœ¬ä¸­å¯èƒ½çš„é—®é¢˜

```python
# ä¿®æ”¹æµ‹è¯•è„šæœ¬ï¼Œæ·»åŠ è¯¦ç»†æ—¥å¿—
def get_critic_score(self, prompt: str, response: str) -> tuple:
    # ... ç°æœ‰ä»£ç  ...
    
    # æ·»åŠ è°ƒè¯•ä¿¡æ¯
    print(f"Input text length: {len(input_text)}")
    print(f"Response length: {response_length}")
    print(f"Values shape: {values.shape}")
    print(f"Values range: [{values.min().item():.4f}, {values.max().item():.4f}]")
    print(f"Mask sum: {response_mask_no_eos.sum().item()}")
    print(f"Score: {score_avg:.4f}")
    
    return score_avg, length
```

---

## ä¸‰ã€æœ€å¯èƒ½çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### 3.1 é—®é¢˜è¯Šæ–­

åŸºäºä»¥ä¸Šåˆ†æï¼Œæœ€å¯èƒ½çš„é—®é¢˜æ˜¯ï¼š

**ğŸ”´ é—®é¢˜ï¼šè®­ç»ƒæ—¶çš„responseså’Œæµ‹è¯•æ—¶çš„responseså®Œå…¨ä¸åŒ**

```python
åŸå› ï¼š
1. è®­ç»ƒæ—¶ä½¿ç”¨vLLMå®æ—¶ç”Ÿæˆï¼ˆtemperature=0.6ï¼‰
2. æµ‹è¯•æ—¶ä½¿ç”¨APIç”Ÿæˆï¼ˆtemperature=0.8ï¼‰
3. å³ä½¿æ˜¯ç›¸åŒçš„promptï¼Œç”Ÿæˆçš„å†…å®¹ä¹Ÿä¸åŒ
4. Criticåœ¨è®­ç»ƒæ—¶å­¦ä¼šåŒºåˆ†çš„æ˜¯"è®­ç»ƒæ—¶ç”Ÿæˆçš„ç‰¹å®šresponses"
5. æµ‹è¯•æ—¶çš„responsesä¸åŒï¼ŒCriticçš„åˆ¤æ–­æ ‡å‡†ä¸é€‚ç”¨
```

**éªŒè¯æ–¹æ³•ï¼š**

```python
# åœ¨è®­ç»ƒæ—¥å¿—ä¸­æŸ¥çœ‹å®é™…çš„responses
# å¯¹æ¯”æµ‹è¯•æ—¶ç”Ÿæˆçš„responses
# çœ‹æ˜¯å¦æœ‰æ˜¾è‘—å·®å¼‚
```
![1770175530530](image/Warmupè®­ç»ƒæµ‹è¯•å·®å¼‚é—®é¢˜è¯Šæ–­æŠ¥å‘Š/1770175530530.png)
### 3.2 è§£å†³æ–¹æ¡ˆ

#### æ–¹æ¡ˆAï¼šä½¿ç”¨å›ºå®šçš„responsesè¿›è¡Œæµ‹è¯•ï¼ˆæ¨èï¼‰â­â­â­â­â­

```python
# 1. åœ¨è®­ç»ƒå¼€å§‹å‰ï¼Œé¢„å…ˆç”Ÿæˆä¸€æ‰¹test responses
# 2. ä¿å­˜è¿™äº›responses
# 3. åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå®šæœŸä½¿ç”¨è¿™äº›å›ºå®šçš„responsesè¯„ä¼°Critic
# 4. è¿™æ ·å¯ä»¥å‡†ç¡®è¡¡é‡Criticçš„å­¦ä¹ è¿›åº¦

å®ç°ï¼š
# åœ¨è®­ç»ƒè„šæœ¬ä¸­æ·»åŠ 
if step % 50 == 0:  # æ¯50æ­¥è¯„ä¼°ä¸€æ¬¡
    eval_metrics = evaluate_critic_on_fixed_samples(
        critic_model=critic_model,
        test_samples=fixed_test_samples  # é¢„å…ˆç”Ÿæˆå¹¶ä¿å­˜çš„
    )
    print(f"Eval d_acc: {eval_metrics['d_acc']:.2%}")
```

#### æ–¹æ¡ˆBï¼šåœ¨è®­ç»ƒé›†ä¸Šç›´æ¥è¯„ä¼°ï¼ˆæ¬¡ä¼˜ï¼‰â­â­â­

```python
# åœ¨è®­ç»ƒçš„æœ€åä¸€æ­¥
# ä½¿ç”¨å½“å‰batchçš„æ•°æ®è®¡ç®—å‡†ç¡®ç‡
# è¿™æ ·å¯ä»¥ç¡®ä¿ä½¿ç”¨çš„æ˜¯è®­ç»ƒæ—¶å®é™…çœ‹åˆ°çš„responses

å®ç°ï¼š
# åœ¨ dp_critic.py çš„ update_critic æ–¹æ³•ä¸­
if self._update_step == total_steps:  # æœ€åä¸€æ­¥
    with torch.no_grad():
        # ä½¿ç”¨å½“å‰batchè¯„ä¼°
        d_acc_final = (teacher_score > student_score).float().mean()
        print(f"Final batch d_acc: {d_acc_final:.2%}")
```

#### æ–¹æ¡ˆCï¼šä¿®æ”¹æµ‹è¯•è„šæœ¬ï¼Œä½¿ç”¨ç›¸åŒçš„ç”Ÿæˆå‚æ•°ï¼ˆä¸´æ—¶æ–¹æ¡ˆï¼‰â­â­

```python
# ä¿®æ”¹æµ‹è¯•è„šæœ¬çš„APIè°ƒç”¨
API_CONFIGS = {
    "student_model": {
        "temperature": 0.6,  # æ”¹ä¸ºä¸è®­ç»ƒä¸€è‡´
        "top_p": 0.9,
        "repetition_penalty": 1.2,
    }
}

# ä½†è¿™ä»ç„¶æ— æ³•ä¿è¯ç”Ÿæˆç›¸åŒçš„responses
# å› ä¸ºéšæœºç§å­ä¸åŒ
```

---

## å››ã€è®­ç»ƒæ–¹æ¡ˆä¼˜åŒ–å»ºè®®

### 4.1 å½“å‰è®­ç»ƒçš„é—®é¢˜

åŸºäºä»¥ä¸Šåˆ†æï¼Œå½“å‰è®­ç»ƒå­˜åœ¨ä»¥ä¸‹é—®é¢˜ï¼š

#### é—®é¢˜1ï¼šæ— æ³•å‡†ç¡®è¯„ä¼°Criticçš„å­¦ä¹ æ•ˆæœ

```python
é—®é¢˜ï¼š
- è®­ç»ƒæ—¶çš„d_acc=76.8%å¯èƒ½æ˜¯è¯¯å¯¼æ€§çš„
- å› ä¸ºæµ‹è¯•æ—¶responsesä¸åŒï¼Œå‡†ç¡®ç‡å˜æˆ45.75%
- æ— æ³•åˆ¤æ–­Criticæ˜¯å¦çœŸçš„å­¦ä¼šäº†åŒºåˆ†è´¨é‡

å½±å“ï¼š
- ä¸çŸ¥é“ä½•æ—¶åœæ­¢è®­ç»ƒ
- ä¸çŸ¥é“Criticæ˜¯å¦è¿‡æ‹Ÿåˆ
- ä¸çŸ¥é“Criticçš„æ³›åŒ–èƒ½åŠ›å¦‚ä½•
```

#### é—®é¢˜2ï¼šCriticå¯èƒ½å­¦åˆ°äº†é”™è¯¯çš„ç‰¹å¾

```python
é—®é¢˜ï¼š
- Criticå¯èƒ½å­¦ä¼šäº†è¯†åˆ«"è®­ç»ƒæ—¶ç”Ÿæˆçš„ç‰¹å®šresponses"
- è€Œä¸æ˜¯å­¦ä¼šäº†"è¯„ä¼°responseçš„çœŸå®è´¨é‡"

ä¾‹å¦‚ï¼š
- è®­ç»ƒæ—¶çš„responseså¯èƒ½æœ‰ç‰¹å®šçš„æ ¼å¼
- Criticå­¦ä¼šäº†"è¿™ç§æ ¼å¼=ä½è´¨é‡"
- ä½†æµ‹è¯•æ—¶çš„responsesæ ¼å¼ä¸åŒ
- Criticçš„åˆ¤æ–­å¤±æ•ˆ
```

#### é—®é¢˜3ï¼šåˆ†æ•°åˆ†å¸ƒä¸ç¨³å®š

```python
é—®é¢˜ï¼š
- Teacheråˆ†æ•°ä»2.77å˜æˆ-0.50
- Studentåˆ†æ•°ä»-3.08å˜æˆ-0.49
- è¯´æ˜Criticçš„è¯„åˆ†æ ‡å‡†ä¸ç¨³å®š

å½±å“ï¼š
- åç»­çš„Actorè®­ç»ƒå¯èƒ½å—åˆ°å½±å“
- å› ä¸ºActorä¾èµ–Criticçš„åˆ†æ•°ä½œä¸ºreward
- ä¸ç¨³å®šçš„åˆ†æ•°ä¼šå¯¼è‡´ä¸ç¨³å®šçš„è®­ç»ƒ
```

### 4.2 ä¼˜åŒ–å»ºè®®

#### å»ºè®®1ï¼šæ·»åŠ å›ºå®šæµ‹è¯•é›†è¯„ä¼°ï¼ˆå¿…é¡»ï¼‰â­â­â­â­â­

```python
# åœ¨è®­ç»ƒå¼€å§‹å‰
# 1. ä»è®­ç»ƒé›†ä¸­é‡‡æ ·100ä¸ªprompts
# 2. ä½¿ç”¨Studentæ¨¡å‹ç”Ÿæˆresponsesï¼ˆå›ºå®šéšæœºç§å­ï¼‰
# 3. ä¿å­˜è¿™äº›å›ºå®šçš„test samples

# åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­
# æ¯Næ­¥ä½¿ç”¨è¿™äº›å›ºå®šsamplesè¯„ä¼°Critic
# è¿™æ ·å¯ä»¥å‡†ç¡®è¡¡é‡Criticçš„å­¦ä¹ è¿›åº¦

å®ç°ä»£ç ï¼š
# prepare_fixed_test_set.py
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

def prepare_fixed_test_set(
    data_path: str,
    student_model_path: str,
    num_samples: int = 100,
    output_path: str = "fixed_test_set.pt"
):
    # åŠ è½½æ•°æ®
    df = pd.read_parquet(data_path)
    df_sample = df.sample(n=num_samples, random_state=42)
    
    # åŠ è½½Studentæ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(student_model_path)
    model = AutoModelForCausalLM.from_pretrained(student_model_path)
    
    # ç”Ÿæˆå›ºå®šçš„responses
    torch.manual_seed(42)  # å›ºå®šéšæœºç§å­
    test_samples = []
    
    for _, row in df_sample.iterrows():
        prompt = row['content']
        teacher_response = row['teacher_response']
        
        # ç”Ÿæˆstudent responseï¼ˆå›ºå®šå‚æ•°ï¼‰
        inputs = tokenizer(prompt, return_tensors="pt")
        outputs = model.generate(
            **inputs,
            max_length=512,
            temperature=0.6,
            top_p=0.9,
            do_sample=True,
        )
        student_response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        test_samples.append({
            'prompt': prompt,
            'teacher_response': teacher_response,
            'student_response': student_response,
        })
    
    # ä¿å­˜
    torch.save(test_samples, output_path)
    print(f"Fixed test set saved to {output_path}")

# åœ¨è®­ç»ƒè„šæœ¬ä¸­ä½¿ç”¨
def evaluate_on_fixed_test_set(critic_model, test_samples):
    correct = 0
    total = 0
    
    for sample in test_samples:
        teacher_score = get_critic_score(
            critic_model, 
            sample['prompt'], 
            sample['teacher_response']
        )
        student_score = get_critic_score(
            critic_model,
            sample['prompt'],
            sample['student_response']
        )
        
        if teacher_score > student_score:
            correct += 1
        total += 1
    
    return correct / total
```

#### å»ºè®®2ï¼šç›‘æ§åˆ†æ•°åˆ†å¸ƒçš„ç¨³å®šæ€§ï¼ˆå¿…é¡»ï¼‰â­â­â­â­â­

```python
# åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œè®°å½•æ¯ä¸€æ­¥çš„åˆ†æ•°åˆ†å¸ƒ
# è§‚å¯Ÿæ˜¯å¦ç¨³å®š

# åœ¨ dp_critic.py ä¸­æ·»åŠ 
metrics.update({
    'critic/teacher_score_std': teacher_score.std().item(),
    'critic/student_score_std': student_score.std().item(),
    'critic/score_range': (teacher_score.max() - teacher_score.min()).item(),
})

# å¥åº·çš„è®­ç»ƒåº”è¯¥çœ‹åˆ°ï¼š
# - åˆ†æ•°å‡å€¼é€æ¸ç¨³å®š
# - åˆ†æ•°æ ‡å‡†å·®ä¿æŒåˆç†èŒƒå›´
# - åˆ†æ•°èŒƒå›´ä¸ä¼šè¿‡å¤§æˆ–è¿‡å°
```

#### å»ºè®®3ï¼šé™ä½temperatureï¼Œå‡å°‘éšæœºæ€§ï¼ˆå¯é€‰ï¼‰â­â­â­

```python
# å½“å‰é…ç½®
temperature=0.6  # è®­ç»ƒæ—¶
temperature=0.8  # æµ‹è¯•æ—¶

# å»ºè®®
temperature=0.3  # é™ä½éšæœºæ€§ï¼Œç”Ÿæˆæ›´ç¡®å®šçš„responses

# ä¼˜ç‚¹ï¼š
# - å‡å°‘responsesçš„å˜åŒ–
# - Criticæ›´å®¹æ˜“å­¦ä¹ ç¨³å®šçš„æ¨¡å¼

# ç¼ºç‚¹ï¼š
# - å¤šæ ·æ€§é™ä½
# - å¯èƒ½å½±å“åç»­çš„Actorè®­ç»ƒ
```

#### å»ºè®®4ï¼šå¢åŠ è®­ç»ƒè½®æ•°ï¼Œè§‚å¯Ÿæ”¶æ•›ï¼ˆå¿…é¡»ï¼‰â­â­â­â­

```python
# å½“å‰é…ç½®
trainer.total_epochs=1  # åªè®­ç»ƒ1ä¸ªepoch

# å»ºè®®
trainer.total_epochs=3  # å¢åŠ åˆ°3ä¸ªepoch

# åŸå› ï¼š
# - 1ä¸ªepochå¯èƒ½ä¸å¤ŸCriticæ”¶æ•›
# - éœ€è¦è§‚å¯Ÿd_accå’Œscore_diffçš„å˜åŒ–è¶‹åŠ¿
# - åˆ¤æ–­Criticæ˜¯å¦çœŸçš„å­¦ä¼šäº†

# é¢„æœŸï¼š
# - d_accåº”è¯¥å…ˆä¸Šå‡åç¨³å®š
# - score_diffåº”è¯¥ä¿æŒç¨³å®šï¼ˆä¸åº”è¯¥è¶‹å‘0ï¼Œå› ä¸ºActorå†»ç»“ï¼‰
# - å¦‚æœscore_diffè¶‹å‘0ï¼Œè¯´æ˜Criticå´©æºƒäº†
```

#### å»ºè®®5ï¼šæ£€æŸ¥Criticçš„è¾“å‡ºåˆ†å¸ƒï¼ˆè¯Šæ–­ï¼‰â­â­â­â­

```python
# åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå®šæœŸæ£€æŸ¥Criticçš„è¾“å‡º
# çœ‹æ˜¯å¦å‡ºç°å¼‚å¸¸

# åœ¨ dp_critic.py ä¸­æ·»åŠ 
if self._update_step % 10 == 0:
    # æ£€æŸ¥valuesçš„åˆ†å¸ƒ
    print(f"Values stats:")
    print(f"  Mean: {values.mean().item():.4f}")
    print(f"  Std: {values.std().item():.4f}")
    print(f"  Min: {values.min().item():.4f}")
    print(f"  Max: {values.max().item():.4f}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
    if values.abs().max() > 10:
        print("âš ï¸  Warning: Large values detected!")
    
    if values.std() < 0.1:
        print("âš ï¸  Warning: Values collapsed!")
```

---

## äº”ã€ç«‹å³è¡ŒåŠ¨è®¡åˆ’

### ç¬¬ä¸€æ­¥ï¼šè¯Šæ–­å½“å‰çŠ¶æ€ï¼ˆä»Šå¤©ï¼‰

```bash
# 1. æ£€æŸ¥è®­ç»ƒæ—¥å¿—çš„æœ€åä¸€æ­¥
# æŸ¥çœ‹step=310æ—¶çš„å…·ä½“æŒ‡æ ‡
grep "step:310" training.log

# 2. æŸ¥çœ‹TensorBoard
# è§‚å¯Ÿscore_diffå’Œd_accçš„å®Œæ•´æ›²çº¿
tensorboard --logdir=/path/to/logs

# 3. è¿è¡Œè¯Šæ–­è„šæœ¬
python diagnose_critic.py
```

### ç¬¬äºŒæ­¥ï¼šå‡†å¤‡å›ºå®šæµ‹è¯•é›†ï¼ˆä»Šå¤©ï¼‰

```bash
# ç”Ÿæˆå›ºå®šçš„test samples
python prepare_fixed_test_set.py \
    --data_path=/path/to/data.parquet \
    --student_model_path=/path/to/student \
    --num_samples=100 \
    --output_path=fixed_test_set.pt
```

### ç¬¬ä¸‰æ­¥ï¼šé‡æ–°è®­ç»ƒï¼ˆæ˜å¤©ï¼‰

```bash
# ä½¿ç”¨ä¼˜åŒ–åçš„é…ç½®é‡æ–°è®­ç»ƒ
# 1. å¢åŠ è®­ç»ƒè½®æ•°
# 2. æ·»åŠ å›ºå®šæµ‹è¯•é›†è¯„ä¼°
# 3. ç›‘æ§åˆ†æ•°åˆ†å¸ƒ

bash scripts/train/A3b_gspo/content_merge_trainning/A3b-warmup-gspo-v2.sh
```

### ç¬¬å››æ­¥ï¼šåˆ†æç»“æœï¼ˆæ˜å¤©ï¼‰

```python
# å¯¹æ¯”æ–°æ—§è®­ç»ƒçš„ç»“æœ
# çœ‹æ˜¯å¦è§£å†³äº†é—®é¢˜
python analyze_training_results.py
```

---

## å…­ã€æ€»ç»“

### æ ¸å¿ƒé—®é¢˜

**Warmupè®­ç»ƒçš„d_acc=76.8%å¯èƒ½æ˜¯è™šé«˜çš„ï¼Œå› ä¸ºï¼š**

1. è®­ç»ƒæ—¶ä½¿ç”¨å®æ—¶ç”Ÿæˆçš„responsesï¼ˆæ¯æ¬¡ä¸åŒï¼‰
2. æµ‹è¯•æ—¶ä¹Ÿä½¿ç”¨å®æ—¶ç”Ÿæˆçš„responsesï¼ˆä¸è®­ç»ƒæ—¶ä¸åŒï¼‰
3. Criticå¯èƒ½å­¦ä¼šäº†è¯†åˆ«"è®­ç»ƒæ—¶çš„ç‰¹å®šresponses"è€Œé"çœŸå®è´¨é‡"
4. å¯¼è‡´æµ‹è¯•æ—¶å‡†ç¡®ç‡å¤§å¹…ä¸‹é™

### å…³é”®å»ºè®®

1. **å¿…é¡»æ·»åŠ å›ºå®šæµ‹è¯•é›†è¯„ä¼°**ï¼Œæ‰èƒ½å‡†ç¡®è¡¡é‡Criticçš„å­¦ä¹ æ•ˆæœ
2. **å¿…é¡»ç›‘æ§åˆ†æ•°åˆ†å¸ƒçš„ç¨³å®šæ€§**ï¼Œç¡®ä¿Criticæ²¡æœ‰å´©æºƒ
3. **å¿…é¡»å¢åŠ è®­ç»ƒè½®æ•°**ï¼Œè§‚å¯Ÿå®Œæ•´çš„è®­ç»ƒæ›²çº¿
4. **å¿…é¡»æ£€æŸ¥æœ€åä¸€æ­¥çš„æŒ‡æ ‡**ï¼Œç¡®è®¤2.77å’Œ-3.08æ˜¯å¦æ˜¯æœ€ç»ˆå€¼

### é¢„æœŸç»“æœ

å¦‚æœæŒ‰ç…§å»ºè®®ä¼˜åŒ–åï¼š

```
å¥åº·çš„è®­ç»ƒï¼š
- å›ºå®šæµ‹è¯•é›†d_acc: 70-80%ï¼ˆç¨³å®šï¼‰
- score_diff: 3-5ï¼ˆç¨³å®šï¼Œä¸è¶‹å‘0ï¼‰
- åˆ†æ•°åˆ†å¸ƒ: ç¨³å®šï¼Œæ— å¼‚å¸¸å€¼

ä¸å¥åº·çš„è®­ç»ƒï¼š
- å›ºå®šæµ‹è¯•é›†d_acc: æ³¢åŠ¨å¤§æˆ–è¶‹å‘50%
- score_diff: è¶‹å‘0ï¼ˆCriticå´©æºƒï¼‰
- åˆ†æ•°åˆ†å¸ƒ: ä¸ç¨³å®šæˆ–å‡ºç°å¼‚å¸¸å€¼
```

---

## é™„å½•ï¼šå¿«é€Ÿè¯Šæ–­è„šæœ¬

```python
# diagnose_critic.py
import torch
import pandas as pd
from transformers import AutoTokenizer
from trl import AutoModelForCausalLMWithValueHead

def diagnose_critic(
    critic_path: str,
    data_path: str,
    num_samples: int = 10
):
    """å¿«é€Ÿè¯Šæ–­Criticçš„çŠ¶æ€"""
    
    print("="*80)
    print("Criticè¯Šæ–­æŠ¥å‘Š")
    print("="*80)
    
    # åŠ è½½æ¨¡å‹
    print("\n1. åŠ è½½Criticæ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(critic_path)
    model = AutoModelForCausalLMWithValueHead.from_pretrained(critic_path)
    model.eval()
    
    # åŠ è½½æ•°æ®
    print("\n2. åŠ è½½æ•°æ®...")
    df = pd.read_parquet(data_path)
    df_sample = df.sample(n=num_samples, random_state=42)
    
    # è¯„ä¼°
    print("\n3. è¯„ä¼°æ ·æœ¬...")
    teacher_scores = []
    student_scores = []
    
    for idx, row in df_sample.iterrows():
        prompt = row['content']
        teacher_response = row['teacher_response']
        
        # è·å–teacheråˆ†æ•°
        messages = [
            {"role": "user", "content": prompt},
            {"role": "assistant", "content": teacher_response}
        ]
        input_text = tokenizer.apply_chat_template(messages, tokenize=False)
        inputs = tokenizer(input_text, return_tensors="pt")
        
        with torch.no_grad():
            outputs = model(**inputs)
            values = outputs[2]
            
            # è®¡ç®—å¹³å‡åˆ†æ•°
            response_length = len(tokenizer(teacher_response, add_special_tokens=False)['input_ids'])
            response_values = values[:, -response_length:]
            score = response_values.mean().item()
            teacher_scores.append(score)
        
        print(f"  Sample {idx}: Teacher score = {score:.4f}")
    
    # ç»Ÿè®¡
    print("\n4. ç»Ÿè®¡ç»“æœ:")
    print(f"  Teacherå¹³å‡åˆ†: {sum(teacher_scores)/len(teacher_scores):.4f}")
    print(f"  Teacheræ ‡å‡†å·®: {pd.Series(teacher_scores).std():.4f}")
    print(f"  åˆ†æ•°èŒƒå›´: [{min(teacher_scores):.4f}, {max(teacher_scores):.4f}]")
    
    # è¯Šæ–­
    print("\n5. è¯Šæ–­:")
    avg_score = sum(teacher_scores) / len(teacher_scores)
    std_score = pd.Series(teacher_scores).std()
    
    if abs(avg_score) > 5:
        print("  âš ï¸  è­¦å‘Š: å¹³å‡åˆ†æ•°è¿‡å¤§ï¼Œå¯èƒ½å­˜åœ¨æ•°å€¼ä¸ç¨³å®š")
    elif abs(avg_score) < 0.1:
        print("  âš ï¸  è­¦å‘Š: å¹³å‡åˆ†æ•°æ¥è¿‘0ï¼ŒCriticå¯èƒ½å´©æºƒ")
    else:
        print("  âœ… å¹³å‡åˆ†æ•°æ­£å¸¸")
    
    if std_score < 0.1:
        print("  âš ï¸  è­¦å‘Š: æ ‡å‡†å·®è¿‡å°ï¼ŒCriticå¯èƒ½å¤±å»åŒºåˆ†èƒ½åŠ›")
    elif std_score > 5:
        print("  âš ï¸  è­¦å‘Š: æ ‡å‡†å·®è¿‡å¤§ï¼Œè¯„åˆ†ä¸ç¨³å®š")
    else:
        print("  âœ… æ ‡å‡†å·®æ­£å¸¸")
    
    print("\n" + "="*80)

if __name__ == "__main__":
    diagnose_critic(
        critic_path="/path/to/critic",
        data_path="/path/to/data.parquet",
        num_samples=10
    )
```

ä½¿ç”¨æ–¹æ³•ï¼š
```bash
python diagnose_critic.py
```
