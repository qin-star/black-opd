# Stage 1 (SeqKD) é€‚é…åˆ†æä¸ä¿®æ”¹æ–¹æ¡ˆ

## ä¸€ã€SeqKD é˜¶æ®µæ¦‚è¿°

### 1.1 æ ¸å¿ƒç‰¹ç‚¹

**SeqKD (Sequence Knowledge Distillation)** æ˜¯ GAD è®­ç»ƒçš„ç¬¬ä¸€é˜¶æ®µï¼Œæœ¬è´¨ä¸Šæ˜¯**ä½¿ç”¨ GRPO åŸºç¡€è®¾æ–½çš„çº¯ SFT è®­ç»ƒ**ã€‚

**å…³é”®ç‰¹å¾**ï¼š
- âœ… ç”Ÿæˆ 8 ä¸ªå“åº”ç”¨äºç›‘æ§è´¨é‡ï¼ˆRouge-Lï¼‰
- âœ… è®­ç»ƒæ—¶åªä½¿ç”¨ `teacher_response`ï¼Œä¸¢å¼ƒ 8 ä¸ªé‡‡æ ·
- âœ… ä½¿ç”¨ SFT æŸå¤±ï¼š`-mean(log P(teacher_response))`
- âŒ ä¸ä½¿ç”¨ GRPO ä¼˜åŠ¿è®¡ç®—
- âŒ ä¸ä½¿ç”¨å¼ºåŒ–å­¦ä¹ 

### 1.2 è®­ç»ƒæµç¨‹

```
æ•°æ®åŠ è½½ (32 prompts + teacher_response)
  â†“
VLLM ç”Ÿæˆ (n=8, å…± 256 ä¸ªå“åº”) â†’ ç”¨äº Rouge-L ç›‘æ§
  â†“
æ•°æ®æ‰©å±• (repeat n=8, å…± 256 ä¸ªæ ·æœ¬)
  â†“
é€‰æ‹© teacher æ•°æ® (ä¸¢å¼ƒ 8 ä¸ªé‡‡æ ·çš„ responses)
  â†“
SFT è®­ç»ƒ (Loss = -mean(log P(teacher_response)))
```

---

## äºŒã€ä¸ Warmup/GAD é˜¶æ®µçš„åŒºåˆ«

| ç»´åº¦ | Stage 1 (SeqKD) | Stage 2 (Warmup) | Stage 3 (GAD) |
|------|----------------|------------------|---------------|
| **è®­ç»ƒç›®æ ‡** | æ¨¡ä»¿ teacher | è®­ç»ƒåˆ¤åˆ«å™¨ + Actor | å¯¹æŠ—è®­ç»ƒ |
| **æŸå¤±å‡½æ•°** | SFT æŸå¤± | åˆ¤åˆ«å™¨æŸå¤± + PPO | åˆ¤åˆ«å™¨æŸå¤± + GSPO |
| **ä½¿ç”¨é‡‡æ ·** | âŒ åªç”¨ teacher | âŒ åªç”¨ teacher | âœ… ä½¿ç”¨ 8 ä¸ªé‡‡æ · |
| **Critic ä½œç”¨** | âŒ ä¸ä½¿ç”¨ | âœ… åˆ¤åˆ«å™¨ | âœ… åˆ¤åˆ«å™¨ |
| **ä¼˜åŠ¿ä¼°è®¡** | âŒ ä¸ä½¿ç”¨ | âœ… GRPO | âœ… GRPO |
| **Critic Warmup** | N/A | 10 æ­¥ | 0 æ­¥ |

---

## ä¸‰ã€æ–°æ¡†æ¶çš„ç°çŠ¶åˆ†æ

### 3.1 å·²å®Œæˆçš„åŠŸèƒ½ï¼ˆWarmup/GAD é€‚é…ï¼‰

ä»ä¹‹å‰çš„ä¿®æ”¹ä¸­ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆï¼š
- âœ… `dp_critic.py`ï¼šåˆ¤åˆ«å™¨è®­ç»ƒ
- âœ… `rl_dataset.py`ï¼š`teacher_response` åŠ è½½
- âœ… `core_algos.py`ï¼š`compute_discriminator_loss`
- âœ… GRPO ä¼˜åŠ¿ä¼°è®¡ï¼ˆæ¡†æ¶å†…ç½®ï¼‰
- âœ… GSPO ç­–ç•¥æŸå¤±ï¼ˆæ¡†æ¶å†…ç½®ï¼‰

### 3.2 SeqKD é˜¶æ®µç¼ºå¤±çš„åŠŸèƒ½

ç»è¿‡æ£€æŸ¥ï¼Œæ–°æ¡†æ¶**ç¼ºå°‘ä»¥ä¸‹ SeqKD ç‰¹å®šåŠŸèƒ½**ï¼š

#### âŒ 1. `compute_sft_loss` å‡½æ•°

**æ—§æ¡†æ¶ä½ç½®**ï¼š`verl/trainer/ppo/core_algos.py`

**åŠŸèƒ½**ï¼š
```python
def compute_sft_loss(log_prob, response_mask):
    """
    è®¡ç®—ç›‘ç£å¾®è°ƒæŸå¤±
    Loss = -mean(log_prob * mask)
    """
    sft_loss = -masked_mean(log_prob, response_mask)
    return sft_loss
```

**çŠ¶æ€**ï¼šâŒ **æ–°æ¡†æ¶ä¸­ä¸å­˜åœ¨**

---

#### âŒ 2. Actor çš„ SFT è®­ç»ƒæ¨¡å¼

**æ—§æ¡†æ¶ä½ç½®**ï¼š`verl/workers/actor/dp_actor.py` çš„ `update_policy` æ–¹æ³•

**åŠŸèƒ½**ï¼š
```python
def update_policy(self, data):
    # é€‰æ‹© teacher æ•°æ®
    select_keys = [
        "teacher_response",
        "teacher_input_ids",
        "teacher_attention_mask",
        "teacher_position_ids"
    ]
    # æ³¨æ„ï¼šä¸¢å¼ƒ responsesï¼ˆ8 ä¸ªé‡‡æ ·ï¼‰
    
    # å‰å‘ä¼ æ’­ä½¿ç”¨ teacher æ•°æ®
    log_prob = self._forward_micro_batch(teacher_data)
    
    # è®¡ç®— SFT æŸå¤±
    sft_loss = compute_sft_loss(log_prob, teacher_response_mask)
    
    # åå‘ä¼ æ’­
    sft_loss.backward()
```

**çŠ¶æ€**ï¼šâŒ **æ–°æ¡†æ¶çš„ Actor ä¸æ”¯æŒ teacher æ•°æ®è®­ç»ƒ**

---

#### âŒ 3. æ•°æ®é€‰æ‹©é€»è¾‘

**æ—§æ¡†æ¶é€»è¾‘**ï¼š
```python
# åœ¨ update_policy ä¸­
if use_sft_mode:  # SeqKD é˜¶æ®µ
    select_keys = ["teacher_response", "teacher_input_ids", ...]
    # ä¸¢å¼ƒ responses
else:  # Warmup/GAD é˜¶æ®µ
    select_keys = ["responses", "input_ids", ...]
    # ä½¿ç”¨å­¦ç”Ÿå“åº”
```

**çŠ¶æ€**ï¼šâŒ **æ–°æ¡†æ¶æ²¡æœ‰è¿™ä¸ªåˆ†æ”¯é€»è¾‘**

---

#### âœ… 4. Rouge-L è¯„ä¼°ï¼ˆå¯èƒ½å·²å­˜åœ¨ï¼‰

**åŠŸèƒ½**ï¼šåœ¨éªŒè¯é˜¶æ®µè®¡ç®— Rouge-L åˆ†æ•°

**çŠ¶æ€**ï¼šâ“ **éœ€è¦æ£€æŸ¥ï¼Œå¯èƒ½å·²åœ¨æ¡†æ¶ä¸­**

---

## å››ã€éœ€è¦æ·»åŠ çš„ä¿®æ”¹

### ğŸ”´ å¿…é¡»ä¿®æ”¹ï¼ˆæ ¸å¿ƒåŠŸèƒ½ï¼‰

#### ä¿®æ”¹ 1ï¼šæ·»åŠ  `compute_sft_loss` å‡½æ•°

**æ–‡ä»¶**ï¼š`verl/trainer/ppo/core_algos.py`

**ä½ç½®**ï¼šåœ¨ `compute_discriminator_loss` ä¹‹å

**ä»£ç **ï¼š
```python
def compute_sft_loss(
    log_prob: torch.Tensor,
    response_mask: torch.Tensor,
    loss_agg_mode: str = "token-mean",
) -> torch.Tensor:
    """
    Compute supervised fine-tuning loss for SeqKD stage.
    
    Args:
        log_prob (torch.Tensor):
            Log probabilities of tokens, shape (batch_size, response_length).
        response_mask (torch.Tensor):
            Mask for valid response tokens, shape (batch_size, response_length).
        loss_agg_mode (str):
            Loss aggregation mode. Defaults to "token-mean".
    
    Returns:
        sft_loss (torch.Tensor):
            Scalar SFT loss.
    """
    # SFT loss: maximize log probability of teacher response
    # Equivalent to minimizing negative log likelihood
    sft_loss = -agg_loss(loss_mat=log_prob, loss_mask=response_mask, loss_agg_mode=loss_agg_mode)
    
    return sft_loss
```

---

#### ä¿®æ”¹ 2ï¼šä¿®æ”¹ Actor çš„ `update_policy` æ–¹æ³•

**æ–‡ä»¶**ï¼š`verl/workers/actor/dp_actor.py`

**ä¿®æ”¹ä½ç½®**ï¼š`update_policy` æ–¹æ³•çš„å¼€å¤´

**ä¿®æ”¹å†…å®¹**ï¼šæ·»åŠ  SFT æ¨¡å¼æ”¯æŒ

```python
def update_policy(self, data: DataProto):
    """Update the policy network using PPO or SFT."""
    self.actor_module.train()
    
    # Check if using SFT mode (SeqKD stage)
    use_sft_mode = data.meta_info.get("use_sft_mode", False)
    
    if use_sft_mode:
        # SeqKD stage: use teacher data only
        select_keys = [
            "teacher_response",
            "teacher_input_ids",
            "teacher_attention_mask",
            "teacher_position_ids",
        ]
    else:
        # Warmup/GAD stage: use student responses
        select_keys = [
            "responses",
            "input_ids",
            "attention_mask",
            "position_ids",
            "old_log_probs",
            "advantages",
        ]
        if self.config.use_kl_loss:
            select_keys.append("ref_log_prob")
    
    # ... ç»§ç»­åŸæœ‰é€»è¾‘ ...
```

**ä¿®æ”¹ä½ç½® 2**ï¼šå‰å‘ä¼ æ’­éƒ¨åˆ†

```python
# åœ¨å‰å‘ä¼ æ’­æ—¶
if use_sft_mode:
    # Use teacher data
    input_ids = model_inputs["teacher_input_ids"]
    attention_mask = model_inputs["teacher_attention_mask"]
    position_ids = model_inputs["teacher_position_ids"]
    response_length = model_inputs["teacher_response"].size(-1)
else:
    # Use student data
    input_ids = model_inputs["input_ids"]
    attention_mask = model_inputs["attention_mask"]
    position_ids = model_inputs["position_ids"]
    response_length = model_inputs["responses"].size(-1)

# å‰å‘ä¼ æ’­
output = self.actor_module(
    input_ids=input_ids,
    attention_mask=attention_mask,
    position_ids=position_ids,
    use_cache=False,
)
```

**ä¿®æ”¹ä½ç½® 3**ï¼šæŸå¤±è®¡ç®—éƒ¨åˆ†

```python
# è®¡ç®— log_prob
logits = output.logits
log_prob = compute_log_prob(logits, input_ids, response_length)

if use_sft_mode:
    # SeqKD stage: use SFT loss
    response_mask = attention_mask[:, -response_length:]
    policy_loss = compute_sft_loss(
        log_prob=log_prob,
        response_mask=response_mask,
        loss_agg_mode=self.config.loss_agg_mode,
    )
    
    micro_batch_metrics = {
        "actor/sft_loss": policy_loss.detach().item(),
        "actor/teacher_pg_loss": policy_loss.detach().item(),  # å…¼å®¹æ—§æ—¥å¿—
    }
else:
    # Warmup/GAD stage: use PPO/GSPO loss
    old_log_prob = model_inputs["old_log_probs"]
    advantages = model_inputs["advantages"]
    
    policy_loss, pg_metrics = compute_policy_loss(
        old_log_prob=old_log_prob,
        log_prob=log_prob,
        advantages=advantages,
        response_mask=response_mask,
        ...
    )
    
    micro_batch_metrics = pg_metrics
```

---

#### ä¿®æ”¹ 3ï¼šä¿®æ”¹è®­ç»ƒè„šæœ¬é…ç½®

**æ–‡ä»¶**ï¼šå¯åŠ¨è„šæœ¬ï¼ˆå¦‚ `gpt5-8b-seqkd.sh`ï¼‰

**æ·»åŠ é…ç½®**ï¼š
```bash
# SeqKD é˜¶æ®µç‰¹å®šé…ç½®
+actor_rollout_ref.actor.use_sft_mode=true  # å¯ç”¨ SFT æ¨¡å¼
trainer.critic_warmup=-1  # ä¸ä½¿ç”¨ Criticï¼ˆæˆ–è®¾ç½®å¾ˆå¤§çš„å€¼ï¼‰
```

---

### ğŸŸ¡ å¯é€‰ä¿®æ”¹ï¼ˆå¢å¼ºåŠŸèƒ½ï¼‰

#### ä¿®æ”¹ 4ï¼šæ·»åŠ  Rouge-L è¯„ä¼°ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰

**æ–‡ä»¶**ï¼š`verl/trainer/ppo/ray_trainer.py`

**ä½ç½®**ï¼šéªŒè¯å¾ªç¯ä¸­

**åŠŸèƒ½**ï¼š
```python
def validate(self, dataloader):
    # ... ç”Ÿæˆå“åº” ...
    
    # è®¡ç®— Rouge-L åˆ†æ•°
    from rouge_score import rouge_scorer
    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    
    rouge_scores = []
    for gen_response, teacher_response in zip(generated, teachers):
        score = scorer.score(teacher_response, gen_response)
        rouge_scores.append(score['rougeL'].fmeasure)
    
    metrics['val/rouge-L/mean'] = np.mean(rouge_scores)
    
    return metrics
```

---

## äº”ã€ä¿®æ”¹ä¼˜å…ˆçº§

### ğŸ”´ é«˜ä¼˜å…ˆçº§ï¼ˆå¿…é¡»å®Œæˆï¼‰

1. **æ·»åŠ  `compute_sft_loss` å‡½æ•°**
   - ä½ç½®ï¼š`core_algos.py`
   - éš¾åº¦ï¼šâ­ ç®€å•
   - å½±å“ï¼šæ ¸å¿ƒåŠŸèƒ½

2. **ä¿®æ”¹ Actor çš„ `update_policy` æ–¹æ³•**
   - ä½ç½®ï¼š`dp_actor.py`
   - éš¾åº¦ï¼šâ­â­â­ ä¸­ç­‰
   - å½±å“ï¼šæ ¸å¿ƒåŠŸèƒ½

3. **æ·»åŠ  `use_sft_mode` é…ç½®æ”¯æŒ**
   - ä½ç½®ï¼šé…ç½®æ–‡ä»¶å’Œ `meta_info`
   - éš¾åº¦ï¼šâ­ ç®€å•
   - å½±å“ï¼šæ ¸å¿ƒåŠŸèƒ½

### ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ï¼ˆå»ºè®®å®Œæˆï¼‰

4. **æ·»åŠ  Rouge-L è¯„ä¼°**
   - ä½ç½®ï¼š`ray_trainer.py`
   - éš¾åº¦ï¼šâ­â­ ç®€å•
   - å½±å“ï¼šç›‘æ§è´¨é‡

### ğŸŸ¢ ä½ä¼˜å…ˆçº§ï¼ˆå¯é€‰ï¼‰

5. **ä¼˜åŒ–æ—¥å¿—è¾“å‡º**
   - æ·»åŠ  `actor/sft_loss` æŒ‡æ ‡
   - æ·»åŠ  `actor/teacher_pg_loss` æŒ‡æ ‡ï¼ˆå…¼å®¹æ€§ï¼‰

---

## å…­ã€å®Œæ•´çš„ä¿®æ”¹æ–¹æ¡ˆ

### æ–¹æ¡ˆ Aï¼šæœ€å°ä¿®æ”¹ï¼ˆæ¨èï¼‰

**åªä¿®æ”¹ Actorï¼Œä¸æ·»åŠ æ–°å‡½æ•°**

```python
# åœ¨ dp_actor.py çš„ update_policy ä¸­
if use_sft_mode:
    # ç›´æ¥ä½¿ç”¨è´Ÿ log_prob ä½œä¸ºæŸå¤±
    policy_loss = -masked_mean(log_prob, response_mask)
else:
    # ä½¿ç”¨ PPO/GSPO æŸå¤±
    policy_loss = compute_policy_loss(...)
```

**ä¼˜ç‚¹**ï¼š
- âœ… ä¿®æ”¹æœ€å°
- âœ… ä¸éœ€è¦æ·»åŠ æ–°å‡½æ•°
- âœ… é€»è¾‘æ¸…æ™°

**ç¼ºç‚¹**ï¼š
- âŒ ä»£ç é‡å¤
- âŒ ä¸å¤Ÿæ¨¡å—åŒ–

---

### æ–¹æ¡ˆ Bï¼šå®Œæ•´ä¿®æ”¹ï¼ˆæ ‡å‡†ï¼‰

**æ·»åŠ  `compute_sft_loss` å‡½æ•° + ä¿®æ”¹ Actor**

```python
# core_algos.py
def compute_sft_loss(log_prob, response_mask, loss_agg_mode="token-mean"):
    return -agg_loss(loss_mat=log_prob, loss_mask=response_mask, loss_agg_mode=loss_agg_mode)

# dp_actor.py
if use_sft_mode:
    policy_loss = compute_sft_loss(log_prob, response_mask)
else:
    policy_loss = compute_policy_loss(...)
```

**ä¼˜ç‚¹**ï¼š
- âœ… ä»£ç æ¨¡å—åŒ–
- âœ… æ˜“äºç»´æŠ¤
- âœ… ä¸æ—§æ¡†æ¶ä¸€è‡´

**ç¼ºç‚¹**ï¼š
- âŒ éœ€è¦ä¿®æ”¹ä¸¤ä¸ªæ–‡ä»¶

---

## ä¸ƒã€æ•°æ®æµéªŒè¯

### 7.1 SeqKD é˜¶æ®µçš„æ•°æ®æµ

```python
# 1. æ•°æ®åŠ è½½ï¼ˆrl_dataset.pyï¼‰
{
    "content": [...],
    "teacher_response": "æ•™å¸ˆå›å¤",  # âœ… å·²æ”¯æŒ
    "teacher_input_ids": [...],     # âœ… å·²æ”¯æŒ
    "teacher_attention_mask": [...], # âœ… å·²æ”¯æŒ
    "teacher_position_ids": [...],   # âœ… å·²æ”¯æŒ
}

# 2. Rollout ç”Ÿæˆï¼ˆå¯é€‰ï¼Œç”¨äºç›‘æ§ï¼‰
gen_batch = actor_rollout_wg.generate_sequences(batch)  # ç”Ÿæˆ 8 ä¸ªå“åº”
# è®¡ç®— Rouge-Lï¼ˆä»…ç›‘æ§ï¼‰

# 3. æ•°æ®æ‰©å±•
batch = batch.repeat(n=8)  # 256 ä¸ªæ ·æœ¬

# 4. Actor è®­ç»ƒ
batch.meta_info["use_sft_mode"] = True  # â† å…³é”®ï¼šè®¾ç½® SFT æ¨¡å¼
actor_output = actor_rollout_wg.update_actor(batch)
```

### 7.2 éœ€è¦éªŒè¯çš„ç‚¹

- âœ… `teacher_response` æ˜¯å¦æ­£ç¡®åŠ è½½ï¼ˆå·²å®Œæˆï¼‰
- âœ… `teacher_input_ids` ç­‰å­—æ®µæ˜¯å¦æ­£ç¡®æ„å»ºï¼ˆå·²å®Œæˆï¼‰
- â“ `meta_info["use_sft_mode"]` æ˜¯å¦æ­£ç¡®ä¼ é€’
- â“ Actor æ˜¯å¦æ­£ç¡®ä½¿ç”¨ teacher æ•°æ®

---

## å…«ã€é…ç½®æ–‡ä»¶ç¤ºä¾‹

### SeqKD é˜¶æ®µé…ç½®

```yaml
# ç®—æ³•é…ç½®
algorithm:
  adv_estimator: grpo  # ä½¿ç”¨ GRPO æ¡†æ¶ï¼ˆä½†ä¸ç”¨ä¼˜åŠ¿è®¡ç®—ï¼‰

# æ•°æ®é…ç½®
data:
  train_files: /path/to/data_with_teacher.parquet
  train_batch_size: 256  # 32 prompts Ã— 8
  max_prompt_length: 2048
  max_response_length: 1536

# Actor é…ç½®
actor_rollout_ref:
  model:
    path: /path/to/base/model
  
  actor:
    use_sft_mode: true  # ğŸ”¥ å¯ç”¨ SFT æ¨¡å¼
    optim.lr: 5e-6
    ppo_mini_batch_size: 256
    use_dynamic_bsz: true
  
  rollout:
    n: 8  # ç”Ÿæˆ 8 ä¸ªå“åº”ï¼ˆç”¨äºç›‘æ§ï¼‰
    temperature: 0.8

# Trainer é…ç½®
trainer:
  critic_warmup: -1  # ä¸ä½¿ç”¨ Criticï¼ˆæˆ–è®¾ç½®å¾ˆå¤§çš„å€¼ï¼‰
  total_epochs: 4
  save_freq: 50
  test_freq: 50
```

---

## ä¹ã€ä¸å…¶ä»–é˜¶æ®µçš„å…¼å®¹æ€§

### 9.1 å‘åå…¼å®¹

**Warmup/GAD é˜¶æ®µä¸å—å½±å“**ï¼š
```yaml
actor_rollout_ref.actor.use_sft_mode: false  # æˆ–ä¸è®¾ç½®ï¼ˆé»˜è®¤ falseï¼‰
```

### 9.2 æ£€æŸ¥ç‚¹å…¼å®¹

**SeqKD â†’ Warmup**ï¼š
```bash
# SeqKD è®­ç»ƒ
bash seqkd_script.sh --exp_name seqkd_exp

# ä» SeqKD æ£€æŸ¥ç‚¹ç»§ç»­ Warmup è®­ç»ƒ
bash warmup_script.sh \
  --model /path/to/seqkd/checkpoint/actor \
  --reward_model /path/to/reward_model \
  --exp_name warmup_exp \
  actor_rollout_ref.actor.use_sft_mode=false  # å…³é—­ SFT æ¨¡å¼
```

---

## åã€æµ‹è¯•éªŒè¯

### 10.1 å•å…ƒæµ‹è¯•

```python
# æµ‹è¯• compute_sft_loss
def test_compute_sft_loss():
    log_prob = torch.randn(4, 10)  # (batch, seq_len)
    response_mask = torch.ones(4, 10)
    
    loss = compute_sft_loss(log_prob, response_mask)
    
    assert loss.dim() == 0  # æ ‡é‡
    assert loss < 0  # è´Ÿ log prob
```

### 10.2 é›†æˆæµ‹è¯•

```bash
# 1. è¿è¡Œå°è§„æ¨¡ SeqKD è®­ç»ƒ
bash seqkd_script.sh \
  --model /path/to/model \
  --exp_name seqkd_test \
  trainer.total_epochs=1

# 2. æ£€æŸ¥æ—¥å¿—æŒ‡æ ‡
# - actor/sft_loss åº”è¯¥å­˜åœ¨
# - actor/teacher_pg_loss åº”è¯¥å­˜åœ¨
# - val/rouge-L/mean åº”è¯¥å­˜åœ¨ï¼ˆå¦‚æœå®ç°ï¼‰

# 3. æ£€æŸ¥æ£€æŸ¥ç‚¹
# - åº”è¯¥èƒ½æ­£å¸¸ä¿å­˜å’ŒåŠ è½½
```

---

## åä¸€ã€æ€»ç»“

### 11.1 æ ¸å¿ƒä¿®æ”¹

**å¿…é¡»å®Œæˆçš„ 2 ä¸ªä¿®æ”¹**ï¼š
1. âœ… æ·»åŠ  `compute_sft_loss` å‡½æ•°åˆ° `core_algos.py`
2. âœ… ä¿®æ”¹ `dp_actor.py` çš„ `update_policy` æ–¹æ³•ï¼Œæ”¯æŒ `use_sft_mode`

### 11.2 ä¿®æ”¹å½±å“

| ä¿®æ”¹ | å½±å“èŒƒå›´ | é£é™© |
|------|---------|------|
| `compute_sft_loss` | æ–°å¢å‡½æ•° | âœ… ä½ï¼ˆä¸å½±å“ç°æœ‰åŠŸèƒ½ï¼‰ |
| `update_policy` | Actor è®­ç»ƒé€»è¾‘ | âš ï¸ ä¸­ï¼ˆéœ€è¦ä»”ç»†æµ‹è¯•ï¼‰ |
| é…ç½®å‚æ•° | å¯åŠ¨è„šæœ¬ | âœ… ä½ï¼ˆå‘åå…¼å®¹ï¼‰ |

### 11.3 ä¸å·²å®Œæˆä¿®æ”¹çš„å…³ç³»

**å·²å®Œæˆçš„ä¿®æ”¹ï¼ˆWarmup/GADï¼‰**ï¼š
- âœ… `dp_critic.py`ï¼šåˆ¤åˆ«å™¨è®­ç»ƒ
- âœ… `rl_dataset.py`ï¼šteacher æ•°æ®åŠ è½½
- âœ… `core_algos.py`ï¼šåˆ¤åˆ«å™¨æŸå¤±

**æ–°å¢ä¿®æ”¹ï¼ˆSeqKDï¼‰**ï¼š
- ğŸ†• `core_algos.py`ï¼šSFT æŸå¤±
- ğŸ†• `dp_actor.py`ï¼šSFT è®­ç»ƒæ¨¡å¼

**å…³ç³»**ï¼š
- âœ… äº’ä¸å†²çª
- âœ… å…±äº«æ•°æ®åŠ è½½é€»è¾‘
- âœ… é€šè¿‡é…ç½®åˆ‡æ¢æ¨¡å¼

### 11.4 æ¨èçš„å®ç°é¡ºåº

1. **ç¬¬ä¸€æ­¥**ï¼šæ·»åŠ  `compute_sft_loss` å‡½æ•°ï¼ˆ5 åˆ†é’Ÿï¼‰
2. **ç¬¬äºŒæ­¥**ï¼šä¿®æ”¹ `dp_actor.py` çš„ `update_policy` æ–¹æ³•ï¼ˆ30 åˆ†é’Ÿï¼‰
3. **ç¬¬ä¸‰æ­¥**ï¼šåˆ›å»º SeqKD å¯åŠ¨è„šæœ¬ï¼ˆ10 åˆ†é’Ÿï¼‰
4. **ç¬¬å››æ­¥**ï¼šè¿è¡Œæµ‹è¯•éªŒè¯ï¼ˆ1 å°æ—¶ï¼‰
5. **ç¬¬äº”æ­¥**ï¼ˆå¯é€‰ï¼‰ï¼šæ·»åŠ  Rouge-L è¯„ä¼°ï¼ˆ30 åˆ†é’Ÿï¼‰

**æ€»è®¡**ï¼šçº¦ 2-3 å°æ—¶å®Œæˆæ ¸å¿ƒåŠŸèƒ½

---

## åäºŒã€ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ç«‹å³å¯ä»¥åšçš„

1. âœ… **æ·»åŠ  `compute_sft_loss` å‡½æ•°**
   - ç®€å•ç›´æ¥
   - ä¸å½±å“ç°æœ‰åŠŸèƒ½

2. âœ… **ä¿®æ”¹ `dp_actor.py`**
   - æ·»åŠ  `use_sft_mode` åˆ†æ”¯
   - æ”¯æŒ teacher æ•°æ®è®­ç»ƒ

3. âœ… **åˆ›å»ºå¯åŠ¨è„šæœ¬**
   - åŸºäºç°æœ‰è„šæœ¬ä¿®æ”¹
   - æ·»åŠ  `use_sft_mode=true`

### å¦‚æœéœ€è¦å¸®åŠ©

æˆ‘å¯ä»¥ï¼š
1. æä¾›å®Œæ•´çš„ä»£ç å®ç°
2. å¸®åŠ©è°ƒè¯•é”™è¯¯
3. ä¼˜åŒ–è®­ç»ƒé…ç½®
4. æ·»åŠ ç›‘æ§æŒ‡æ ‡

---

**æœ€ç»ˆç»“è®º**ï¼šSeqKD é˜¶æ®µéœ€è¦ **2 ä¸ªæ ¸å¿ƒä¿®æ”¹**ï¼Œä¿®æ”¹é‡ä¸å¤§ï¼Œä¸å·²å®Œæˆçš„ Warmup/GAD ä¿®æ”¹äº’ä¸å†²çªã€‚
