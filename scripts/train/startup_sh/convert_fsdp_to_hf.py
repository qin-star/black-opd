#!/usr/bin/env python3
"""
å°† FSDP åˆ†ç‰‡æ ¼å¼çš„æ¨¡å‹è½¬æ¢ä¸º HuggingFace æ ‡å‡†æ ¼å¼
"""

import os
import sys
import torch
import json
import shutil
from pathlib import Path
import argparse
from transformers import AutoConfig, AutoTokenizer

def convert_fsdp_to_hf(fsdp_path, output_path):
    """
    å°† FSDP åˆ†ç‰‡æ¨¡å‹è½¬æ¢ä¸º HuggingFace æ ‡å‡†æ ¼å¼
    
    Args:
        fsdp_path: FSDP åˆ†ç‰‡æ¨¡å‹è·¯å¾„
        output_path: è¾“å‡ºçš„æ ‡å‡†æ ¼å¼æ¨¡å‹è·¯å¾„
    """
    print(f"ğŸ”„ è½¬æ¢ FSDP æ¨¡å‹: {fsdp_path}")
    print(f"ğŸ“ è¾“å‡ºè·¯å¾„: {output_path}")
    
    # æ£€æŸ¥è¾“å…¥è·¯å¾„
    fsdp_path = Path(fsdp_path)
    if not fsdp_path.exists():
        raise FileNotFoundError(f"FSDP æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {fsdp_path}")
    
    # æŸ¥æ‰¾æ‰€æœ‰åˆ†ç‰‡æ–‡ä»¶
    shard_files = list(fsdp_path.glob("model_world_size_*_rank_*.pt"))
    if not shard_files:
        raise FileNotFoundError(f"æœªæ‰¾åˆ° FSDP åˆ†ç‰‡æ–‡ä»¶: {fsdp_path}")
    
    print(f"ğŸ“¦ æ‰¾åˆ° {len(shard_files)} ä¸ªåˆ†ç‰‡æ–‡ä»¶")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_path = Path(output_path)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # å¤åˆ¶é…ç½®æ–‡ä»¶å’Œåˆ†è¯å™¨æ–‡ä»¶
    config_files = [
        "config.json",
        "tokenizer_config.json", 
        "tokenizer.json",
        "special_tokens_map.json",
        "vocab.json",
        "merges.txt",
        "added_tokens.json",
        "generation_config.json",
        "chat_template.jinja"
    ]
    
    print("ğŸ“‹ å¤åˆ¶é…ç½®æ–‡ä»¶...")
    for file_name in config_files:
        src_file = fsdp_path / file_name
        if src_file.exists():
            dst_file = output_path / file_name
            shutil.copy2(src_file, dst_file)
            print(f"  âœ“ {file_name}")
    
    # åŠ è½½å’Œåˆå¹¶æ¨¡å‹æƒé‡
    print("ğŸ”— åˆå¹¶æ¨¡å‹æƒé‡...")
    
    try:
        # å°è¯•ä½¿ç”¨ torch.load åŠ è½½åˆ†ç‰‡
        merged_state_dict = {}
        
        # æŒ‰ rank é¡ºåºåŠ è½½åˆ†ç‰‡
        shard_files.sort(key=lambda x: int(x.name.split('_rank_')[1].split('.')[0]))
        
        for i, shard_file in enumerate(shard_files):
            print(f"  ğŸ“¦ åŠ è½½åˆ†ç‰‡ {i+1}/{len(shard_files)}: {shard_file.name}")
            
            try:
                # åŠ è½½åˆ†ç‰‡æ•°æ®
                shard_data = torch.load(shard_file, map_location='cpu')
                
                # æå–æ¨¡å‹æƒé‡ (å¯èƒ½åœ¨ä¸åŒçš„é”®ä¸‹)
                if isinstance(shard_data, dict):
                    # æŸ¥æ‰¾æ¨¡å‹æƒé‡
                    model_weights = None
                    for key in ['model_state_dict', 'state_dict', 'model']:
                        if key in shard_data:
                            model_weights = shard_data[key]
                            break
                    
                    if model_weights is None:
                        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç‰¹å®šé”®ï¼Œå‡è®¾æ•´ä¸ªå­—å…¸å°±æ˜¯æƒé‡
                        model_weights = shard_data
                    
                    # åˆå¹¶æƒé‡
                    for param_name, param_tensor in model_weights.items():
                        if param_name in merged_state_dict:
                            # å¦‚æœå‚æ•°å·²å­˜åœ¨ï¼Œå¯èƒ½éœ€è¦æ‹¼æ¥
                            print(f"    âš ï¸  å‚æ•° {param_name} å·²å­˜åœ¨ï¼Œè·³è¿‡")
                        else:
                            merged_state_dict[param_name] = param_tensor
                            
            except Exception as e:
                print(f"    âŒ åŠ è½½åˆ†ç‰‡å¤±è´¥: {e}")
                continue
        
        if not merged_state_dict:
            raise ValueError("æœªèƒ½ä»åˆ†ç‰‡ä¸­æå–ä»»ä½•æ¨¡å‹æƒé‡")
        
        print(f"âœ… æˆåŠŸåˆå¹¶ {len(merged_state_dict)} ä¸ªå‚æ•°")
        
        # ä¿å­˜åˆå¹¶åçš„æ¨¡å‹
        output_model_file = output_path / "pytorch_model.bin"
        print(f"ğŸ’¾ ä¿å­˜æ¨¡å‹: {output_model_file}")
        
        torch.save(merged_state_dict, output_model_file)
        
        # éªŒè¯ä¿å­˜çš„æ¨¡å‹
        file_size = output_model_file.stat().st_size / (1024**3)  # GB
        print(f"ğŸ“Š æ¨¡å‹æ–‡ä»¶å¤§å°: {file_size:.2f} GB")
        
        if file_size < 1.0:
            print("âš ï¸  è­¦å‘Š: æ¨¡å‹æ–‡ä»¶å¤§å°å¼‚å¸¸å°ï¼Œå¯èƒ½è½¬æ¢ä¸å®Œæ•´")
        
        print("âœ… è½¬æ¢å®Œæˆï¼")
        return True
        
    except Exception as e:
        print(f"âŒ è½¬æ¢å¤±è´¥: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description="å°† FSDP åˆ†ç‰‡æ¨¡å‹è½¬æ¢ä¸º HuggingFace æ ‡å‡†æ ¼å¼")
    parser.add_argument("fsdp_path", help="FSDP åˆ†ç‰‡æ¨¡å‹è·¯å¾„")
    parser.add_argument("output_path", help="è¾“å‡ºçš„æ ‡å‡†æ ¼å¼æ¨¡å‹è·¯å¾„")
    parser.add_argument("--force", action="store_true", help="å¼ºåˆ¶è¦†ç›–è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥è¾“å‡ºè·¯å¾„
    if Path(args.output_path).exists() and not args.force:
        response = input(f"è¾“å‡ºè·¯å¾„å·²å­˜åœ¨: {args.output_path}\næ˜¯å¦è¦†ç›–? (y/N): ")
        if response.lower() != 'y':
            print("âŒ æ“ä½œå·²å–æ¶ˆ")
            return
    
    # æ‰§è¡Œè½¬æ¢
    success = convert_fsdp_to_hf(args.fsdp_path, args.output_path)
    
    if success:
        print("\nğŸ‰ è½¬æ¢æˆåŠŸå®Œæˆï¼")
        print(f"ğŸ“ æ ‡å‡†æ ¼å¼æ¨¡å‹å·²ä¿å­˜åˆ°: {args.output_path}")
        print("\nğŸ“‹ ç°åœ¨å¯ä»¥ä½¿ç”¨æ ‡å‡†æ ¼å¼æ¨¡å‹è¿›è¡Œè®­ç»ƒ:")
        print(f"   STAGE1_MODEL=\"{args.output_path}\"")
    else:
        print("\nâŒ è½¬æ¢å¤±è´¥")
        sys.exit(1)

if __name__ == "__main__":
    main()
