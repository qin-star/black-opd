#!/usr/bin/env python3
"""
é€šç”¨æ•°æ®è½¬æ¢è„šæœ¬ï¼šæ”¯æŒ Excel å’Œ JSONL æ ¼å¼è½¬æ¢ä¸º GAD è®­ç»ƒæ‰€éœ€çš„ parquet æ ¼å¼

æ”¯æŒçš„è¾“å…¥æ ¼å¼ï¼š
1. Excel (.xlsx, .xls): ä¸¤åˆ—æ ¼å¼ prompt || teacher_response
2. JSONL (.jsonl, .json): {"messages":[{"role":"user","content":"..."},{"role":"assistant","content":"..."}]}

è¾“å‡ºï¼šè®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„ parquet æ–‡ä»¶ï¼ˆå¯é…ç½®æ¯”ä¾‹åˆ‡åˆ†ï¼‰

"""

import pandas as pd
import json
import os
import random
from typing import List, Dict, Tuple, Optional
from pathlib import Path


def convert_excel_row(row: pd.Series) -> Dict:
    """
    è½¬æ¢ Excel å•è¡Œæ•°æ®
    
    Args:
        row: Excel ä¸­çš„ä¸€è¡Œæ•°æ®ï¼ˆç¬¬ä¸€åˆ—ä¸ºpromptï¼Œç¬¬äºŒåˆ—ä¸ºteacher_responseï¼‰
        
    Returns:
        è½¬æ¢åçš„å­—å…¸
    """
    prompt = str(row.iloc[0]) if pd.notna(row.iloc[0]) else ""
    teacher_response = str(row.iloc[1]) if pd.notna(row.iloc[1]) else ""
    
    content = [{"content": prompt, "role": "user"}]
    
    return {
        "content": content,
        "teacher_response": teacher_response
    }


def convert_jsonl_row(data: Dict) -> Dict:
    """
    è½¬æ¢ JSONL å•è¡Œæ•°æ®
    
    Args:
        data: JSONL ä¸­çš„ä¸€è¡Œæ•°æ®ï¼Œæ ¼å¼ä¸º {"messages": [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]}
        
    Returns:
        è½¬æ¢åçš„å­—å…¸
    """
    messages = data.get("messages", [])
    
    prompt = ""
    teacher_response = ""
    
    for msg in messages:
        role = msg.get("role", "")
        content = msg.get("content", "")
        
        if role == "user":
            prompt = content
        elif role == "assistant":
            teacher_response = content
    
    content = [{"content": prompt, "role": "user"}]
    
    return {
        "content": content,
        "teacher_response": teacher_response
    }


def load_excel_data(file_path: str) -> List[Dict]:
    """åŠ è½½ Excel æ–‡ä»¶æ•°æ®"""
    print(f"ğŸ“– è¯»å– Excel: {file_path}")
    df = pd.read_excel(file_path)
    print(f"   âœ… è¯»å–æˆåŠŸï¼Œå…± {len(df)} è¡Œ")
    print(f"   åˆ—å: {df.columns.tolist()}")
    
    converted_data = []
    for idx, row in df.iterrows():
        try:
            converted_row = convert_excel_row(row)
            converted_data.append(converted_row)
        except Exception as e:
            print(f"   âš ï¸  ç¬¬ {idx} è¡Œè½¬æ¢å¤±è´¥: {e}")
            continue
    
    return converted_data


def load_jsonl_data(file_path: str) -> List[Dict]:
    """åŠ è½½ JSONL æ–‡ä»¶æ•°æ®"""
    print(f"ğŸ“– è¯»å– JSONL: {file_path}")
    
    converted_data = []
    line_count = 0
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for idx, line in enumerate(f):
            line = line.strip()
            if not line:
                continue
            
            line_count += 1
            try:
                data = json.loads(line)
                converted_row = convert_jsonl_row(data)
                converted_data.append(converted_row)
            except json.JSONDecodeError as e:
                print(f"   âš ï¸  ç¬¬ {idx + 1} è¡Œ JSON è§£æå¤±è´¥: {e}")
                continue
            except Exception as e:
                print(f"   âš ï¸  ç¬¬ {idx + 1} è¡Œè½¬æ¢å¤±è´¥: {e}")
                continue
    
    print(f"   âœ… è¯»å–æˆåŠŸï¼Œå…± {line_count} è¡Œï¼ŒæˆåŠŸè½¬æ¢ {len(converted_data)} è¡Œ")
    
    return converted_data


def split_dataset(data: List[Dict], train_ratio: float = 0.9, random_seed: int = 42) -> Tuple[List[Dict], List[Dict]]:
    """
    éšæœºåˆ‡åˆ†æ•°æ®é›†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†
    
    Args:
        data: åŸå§‹æ•°æ®åˆ—è¡¨
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹ï¼Œé»˜è®¤0.9
        random_seed: éšæœºç§å­
        
    Returns:
        è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å…ƒç»„ (train_data, test_data)
    """
    random.seed(random_seed)
    
    shuffled_data = data.copy()
    random.shuffle(shuffled_data)
    
    split_idx = int(len(shuffled_data) * train_ratio)
    
    train_data = shuffled_data[:split_idx]
    test_data = shuffled_data[split_idx:]
    
    return train_data, test_data


def save_to_parquet(data: List[Dict], output_path: str, base_name: str, dataset_type: str, save_excel: bool = False) -> Tuple[str, Optional[str]]:
    """
    ä¿å­˜æ•°æ®ä¸º Parquet æ ¼å¼ï¼ˆå¯é€‰åŒæ—¶ä¿å­˜ Excelï¼‰
    
    Args:
        data: è¦ä¿å­˜çš„æ•°æ®åˆ—è¡¨
        output_path: è¾“å‡ºç›®å½•
        base_name: åŸºç¡€æ–‡ä»¶å
        dataset_type: æ•°æ®é›†ç±»å‹ï¼ˆtrain/testï¼‰
        save_excel: æ˜¯å¦åŒæ—¶ä¿å­˜ Excel æ–‡ä»¶
        
    Returns:
        (Parquet æ–‡ä»¶è·¯å¾„, Excel æ–‡ä»¶è·¯å¾„æˆ–None)
    """
    os.makedirs(output_path, exist_ok=True)
    
    parquet_filename = f"{base_name}_{dataset_type}.parquet"
    parquet_path = os.path.join(output_path, parquet_filename)
    
    df = pd.DataFrame(data)
    df['id'] = [f"{base_name}_{dataset_type}_{i:06d}" for i in range(len(df))]
    df = df[['id', 'content', 'teacher_response']]
    
    df.to_parquet(parquet_path, index=False)
    
    excel_path = None
    if save_excel:
        excel_filename = f"{base_name}_{dataset_type}.xlsx"
        excel_path = os.path.join(output_path, excel_filename)
        # ä¸ºExcelåˆ›å»ºå¯è¯»æ ¼å¼ï¼šå°†contentåˆ—è¡¨è½¬ä¸ºå­—ç¬¦ä¸²
        excel_df = df.copy()
        excel_df['content'] = excel_df['content'].apply(lambda x: x[0]['content'] if x else '')
        excel_df.columns = ['id', 'prompt', 'teacher_response']
        excel_df.to_excel(excel_path, index=False)
    
    return parquet_path, excel_path


def detect_file_type(file_path: str) -> str:
    """æ£€æµ‹æ–‡ä»¶ç±»å‹"""
    ext = Path(file_path).suffix.lower()
    
    if ext in ['.xlsx', '.xls']:
        return 'excel'
    elif ext in ['.jsonl', '.json']:
        return 'jsonl'
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {ext}ï¼Œæ”¯æŒ .xlsx, .xls, .jsonl, .json")


def process_file(
    input_path: str,
    output_dir: Optional[str] = None,
    train_ratio: float = 0.9,
    random_seed: int = 42,
    split_data: bool = True,
    save_excel: bool = False
) -> None:
    """
    å¤„ç†å•ä¸ªæ–‡ä»¶
    
    Args:
        input_path: è¾“å…¥æ–‡ä»¶è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ä¸ºè¾“å…¥æ–‡ä»¶åŒç›®å½•ä¸‹çš„ processed æ–‡ä»¶å¤¹ï¼‰
        train_ratio: è®­ç»ƒé›†æ¯”ä¾‹
        random_seed: éšæœºç§å­
        split_data: æ˜¯å¦åˆ‡åˆ†æ•°æ®é›†
        save_excel: æ˜¯å¦åŒæ—¶è¾“å‡º Excel æ–‡ä»¶
    """
    print("=" * 60)
    print("é€šç”¨æ•°æ®è½¬æ¢å·¥å…·ï¼šExcel/JSONL -> GAD Parquet æ ¼å¼")
    print("=" * 60)
    
    if not os.path.exists(input_path):
        print(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
        return
    
    # æ£€æµ‹æ–‡ä»¶ç±»å‹
    file_type = detect_file_type(input_path)
    print(f"\nğŸ“ æ£€æµ‹åˆ°æ–‡ä»¶ç±»å‹: {file_type.upper()}")
    
    # ç¡®å®šè¾“å‡ºç›®å½•
    if output_dir is None:
        input_dir = os.path.dirname(input_path)
        output_dir = os.path.join(input_dir, "processed")
    
    # è·å–åŸºç¡€æ–‡ä»¶å
    base_name = Path(input_path).stem
    
    # åŠ è½½æ•°æ®
    if file_type == 'excel':
        data = load_excel_data(input_path)
    else:
        data = load_jsonl_data(input_path)
    
    if not data:
        print("âŒ é”™è¯¯ï¼šæ²¡æœ‰æˆåŠŸè½¬æ¢ä»»ä½•æ•°æ®")
        return
    
    print(f"\nğŸ“Š æˆåŠŸè½¬æ¢ {len(data)} æ¡æ•°æ®")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    print(f"   è¾“å‡ºExcel: {'æ˜¯' if save_excel else 'å¦'}")
    
    if split_data:
        # åˆ‡åˆ†æ•°æ®é›†
        print(f"\nğŸ”„ åˆ‡åˆ†æ•°æ®é›†ï¼ˆè®­ç»ƒé›†æ¯”ä¾‹: {train_ratio*100:.0f}%ï¼‰...")
        train_data, test_data = split_dataset(data, train_ratio, random_seed)
        print(f"   âœ… åˆ‡åˆ†å®Œæˆ")
        print(f"   è®­ç»ƒé›†: {len(train_data)} æ¡ ({len(train_data)/len(data)*100:.1f}%)")
        print(f"   æµ‹è¯•é›†: {len(test_data)} æ¡ ({len(test_data)/len(data)*100:.1f}%)")
        
        # ä¿å­˜è®­ç»ƒé›†
        print(f"\nğŸ’¾ ä¿å­˜è®­ç»ƒé›†...")
        train_parquet, train_excel = save_to_parquet(train_data, output_dir, base_name, "train", save_excel)
        print(f"   âœ… Parquet: {train_parquet}")
        if train_excel:
            print(f"   âœ… Excel: {train_excel}")
        
        # ä¿å­˜æµ‹è¯•é›†
        print(f"\nğŸ’¾ ä¿å­˜æµ‹è¯•é›†...")
        test_parquet, test_excel = save_to_parquet(test_data, output_dir, base_name, "test", save_excel)
        print(f"   âœ… Parquet: {test_parquet}")
        if test_excel:
            print(f"   âœ… Excel: {test_excel}")
        
        # éªŒè¯
        verify_and_show_sample(train_parquet, "è®­ç»ƒé›†")
        verify_and_show_sample(test_parquet, "æµ‹è¯•é›†")
    else:
        # ä¸åˆ‡åˆ†ï¼Œç›´æ¥ä¿å­˜å…¨éƒ¨æ•°æ®
        print(f"\nğŸ’¾ ä¿å­˜å…¨éƒ¨æ•°æ®...")
        output_parquet, output_excel = save_to_parquet(data, output_dir, base_name, "all", save_excel)
        print(f"   âœ… Parquet: {output_parquet}")
        if output_excel:
            print(f"   âœ… Excel: {output_excel}")
        
        verify_and_show_sample(output_parquet, "å…¨éƒ¨æ•°æ®")
    
    print("\n" + "=" * 60)
    print("âœ… æ•°æ®è½¬æ¢å®Œæˆï¼")
    print("=" * 60)


def verify_and_show_sample(parquet_path: str, dataset_name: str) -> None:
    """éªŒè¯å¹¶æ˜¾ç¤ºç¤ºä¾‹æ•°æ®"""
    try:
        df = pd.read_parquet(parquet_path)
        print(f"\nğŸ” éªŒè¯ {dataset_name}: {len(df)} è¡Œï¼Œåˆ—: {df.columns.tolist()}")
        
        if len(df) > 0:
            first_row = df.iloc[0]
            print(f"\nğŸ“ {dataset_name}ç¬¬ä¸€è¡Œç¤ºä¾‹:")
            print(f"   ID: {first_row['id']}")
            print(f"   Content (å‰200å­—ç¬¦): {str(first_row['content'])[:200]}...")
            print(f"   Teacher Response (å‰200å­—ç¬¦): {str(first_row['teacher_response'])[:200]}...")
    except Exception as e:
        print(f"   âš ï¸  {dataset_name}éªŒè¯å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•° - ç›´æ¥ä¿®æ”¹ä¸‹æ–¹å‚æ•°å³å¯è¿è¡Œ"""
    
    # ==================== é…ç½®å‚æ•°ï¼ˆç›´æ¥ä¿®æ”¹è¿™é‡Œï¼‰====================
    
    # è¾“å…¥æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒ .xlsx, .xls, .jsonl, .jsonï¼‰
    INPUT_PATH = "/path/to/your/data.jsonl"
    
    # è¾“å‡ºç›®å½•ï¼ˆè®¾ä¸º None åˆ™é»˜è®¤ä¸ºè¾“å…¥æ–‡ä»¶åŒç›®å½•ä¸‹çš„ processed æ–‡ä»¶å¤¹ï¼‰
    OUTPUT_DIR = None
    
    # è®­ç»ƒé›†æ¯”ä¾‹ï¼ˆ0.0 ~ 1.0ï¼‰
    TRAIN_RATIO = 0.9
    
    # éšæœºç§å­
    RANDOM_SEED = 42
    
    # æ˜¯å¦åˆ‡åˆ†æ•°æ®é›†ï¼ˆTrue: åˆ‡åˆ†ä¸ºè®­ç»ƒé›†/æµ‹è¯•é›†ï¼ŒFalse: è¾“å‡ºå…¨éƒ¨æ•°æ®ï¼‰
    SPLIT_DATA = True
    
    # æ˜¯å¦åŒæ—¶è¾“å‡º Excel æ–‡ä»¶ï¼ˆTrue: è¾“å‡º parquet + xlsxï¼ŒFalse: ä»…è¾“å‡º parquetï¼‰
    SAVE_EXCEL = True
    
    # ================================================================
    
    process_file(
        input_path=INPUT_PATH,
        output_dir=OUTPUT_DIR,
        train_ratio=TRAIN_RATIO,
        random_seed=RANDOM_SEED,
        split_data=SPLIT_DATA,
        save_excel=SAVE_EXCEL
    )


if __name__ == "__main__":
    main()
