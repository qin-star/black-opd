#!/usr/bin/env python3
"""
åˆå¹¶å¤šä¸ªè®­ç»ƒé›† parquet æ–‡ä»¶çš„è„šæœ¬
æ”¯æŒä»»æ„æ•°é‡çš„ parquet æ–‡ä»¶åˆå¹¶ï¼Œå¯é€‰æ‹©éšæœºæ‰“ä¹±å’Œåˆ‡åˆ†è®­ç»ƒ/æµ‹è¯•é›†

ç›´æ¥ä¿®æ”¹ä¸‹æ–¹é…ç½®å‚æ•°åè¿è¡Œï¼š
python scripts/data_process/merge_trainning_parquet.py
"""

import pandas as pd
import os
from typing import List, Tuple
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def read_parquet_file(file_path: str) -> pd.DataFrame:
    """è¯»å– parquet æ–‡ä»¶"""
    try:
        logger.info(f"æ­£åœ¨è¯»å–æ–‡ä»¶: {file_path}")
        df = pd.read_parquet(file_path)
        logger.info(f"æˆåŠŸè¯»å–æ–‡ä»¶ï¼ŒåŒ…å« {len(df)} è¡Œæ•°æ®")
        return df
    except Exception as e:
        logger.error(f"è¯»å–æ–‡ä»¶ {file_path} å¤±è´¥: {e}")
        raise


def validate_data_structure(df: pd.DataFrame, file_name: str) -> bool:
    """éªŒè¯æ•°æ®ç»“æ„æ˜¯å¦ç¬¦åˆé¢„æœŸ"""
    expected_columns = ["content", "teacher_response"]

    if not all(col in df.columns for col in expected_columns):
        missing_cols = [col for col in expected_columns if col not in df.columns]
        logger.warning(f"{file_name} ç¼ºå°‘åˆ—: {missing_cols}")
        logger.info(f"{file_name} å®é™…åˆ—å: {list(df.columns)}")
        return False

    logger.info(f"{file_name} æ•°æ®ç»“æ„éªŒè¯é€šè¿‡ï¼Œå½¢çŠ¶: {df.shape}")
    return True


def add_dataset_identifier(df: pd.DataFrame, dataset_name: str) -> pd.DataFrame:
    """ä¸ºæ•°æ®é›†æ·»åŠ æ ‡è¯†ç¬¦"""
    df_copy = df.copy()
    df_copy["dataset_source"] = dataset_name
    df_copy["id"] = [f"{dataset_name}_{i:06d}" for i in range(len(df_copy))]
    return df_copy


def merge_datasets(dfs: List[pd.DataFrame], dataset_names: List[str]) -> pd.DataFrame:
    """åˆå¹¶å¤šä¸ªæ•°æ®é›†"""
    logger.info("å¼€å§‹åˆå¹¶æ•°æ®é›†...")

    processed_dfs = []
    for df, name in zip(dfs, dataset_names):
        processed_df = add_dataset_identifier(df, name)
        processed_dfs.append(processed_df)
        logger.info(f"  {name}: {len(df)} è¡Œæ•°æ®")

    merged_df = pd.concat(processed_dfs, ignore_index=True)
    logger.info(f"åˆå¹¶å®Œæˆï¼Œæ€»å…± {len(merged_df)} è¡Œæ•°æ®")

    return merged_df


def shuffle_dataset(df: pd.DataFrame, random_seed: int = 42) -> pd.DataFrame:
    """æ‰“ä¹±æ•°æ®é›†é¡ºåº"""
    logger.info("æ­£åœ¨æ‰“ä¹±æ•°æ®é›†é¡ºåº...")
    shuffled_df = df.sample(frac=1, random_state=random_seed).reset_index(drop=True)
    logger.info("æ•°æ®é›†æ‰“ä¹±å®Œæˆ")
    return shuffled_df


def sample_test_set(
    df: pd.DataFrame, test_ratio: float = 0.02, random_seed: int = 42
) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """ä»åˆå¹¶åçš„æ•°æ®é›†ä¸­é‡‡æ ·æµ‹è¯•é›†"""
    logger.info(f"æ­£åœ¨ä»æ•°æ®é›†ä¸­é‡‡æ · {test_ratio*100}% ä½œä¸ºæµ‹è¯•é›†...")

    test_size = max(1, int(len(df) * test_ratio))
    logger.info(f"æµ‹è¯•é›†å¤§å°: {test_size} è¡Œ")

    test_df = df.sample(n=test_size, random_state=random_seed)
    train_df = df.drop(test_df.index).reset_index(drop=True)
    test_df = test_df.reset_index(drop=True)

    logger.info(f"é‡‡æ ·å®Œæˆ - è®­ç»ƒé›†: {len(train_df)} è¡Œ, æµ‹è¯•é›†: {len(test_df)} è¡Œ")

    if "dataset_source" in test_df.columns:
        logger.info("æµ‹è¯•é›†æ•°æ®æ¥æºåˆ†å¸ƒ:")
        test_source_counts = test_df["dataset_source"].value_counts()
        for source, count in test_source_counts.items():
            percentage = (count / len(test_df)) * 100
            logger.info(f"  {source}: {count} è¡Œ ({percentage:.1f}%)")

    return train_df, test_df


def save_merged_dataset(df: pd.DataFrame, output_path: str) -> None:
    """ä¿å­˜åˆå¹¶åçš„æ•°æ®é›†"""
    try:
        logger.info(f"æ­£åœ¨ä¿å­˜åˆå¹¶åçš„æ•°æ®é›†åˆ°: {output_path}")
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        df.to_parquet(output_path, index=False)

        logger.info(f"æˆåŠŸä¿å­˜åˆå¹¶åçš„æ•°æ®é›†ï¼ŒåŒ…å« {len(df)} è¡Œæ•°æ®")
        if "dataset_source" in df.columns:
            logger.info("æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯:")
            source_counts = df["dataset_source"].value_counts()
            for source, count in source_counts.items():
                logger.info(f"  {source}: {count} è¡Œ")

    except Exception as e:
        logger.error(f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
        raise


def preview_samples(df: pd.DataFrame, num_samples: int = 3, random_seed: int = 42) -> None:
    """æŠ½æ ·é¢„è§ˆåˆå¹¶åçš„æ•°æ®"""
    print("\n" + "=" * 80)
    print(f"ğŸ“‹ æŠ½æ ·é¢„è§ˆï¼ˆéšæœºæŠ½å– {num_samples} æ¡æ•°æ®ï¼‰")
    print("=" * 80)

    samples = df.sample(n=min(num_samples, len(df)), random_state=random_seed)

    for idx, (_, row) in enumerate(samples.iterrows(), 1):
        print(f"\n{'â”€' * 80}")
        print(f"ã€æ ·æœ¬ {idx}ã€‘")
        print(f"  ID: {row.get('id', 'N/A')}")
        print(f"  æ¥æº: {row.get('dataset_source', 'N/A')}")

        # å¤„ç† content å­—æ®µ
        content = row.get("content", "")
        if isinstance(content, list) and len(content) > 0:
            prompt = content[0].get("content", "") if isinstance(content[0], dict) else str(content[0])
        else:
            prompt = str(content)

        # æˆªæ–­æ˜¾ç¤º
        max_len = 300
        prompt_display = prompt[:max_len] + "..." if len(prompt) > max_len else prompt
        response = str(row.get("teacher_response", ""))
        response_display = response[:max_len] + "..." if len(response) > max_len else response

        print(f"  Prompt ({len(prompt)} å­—ç¬¦):")
        print(f"    {prompt_display}")
        print(f"  Teacher Response ({len(response)} å­—ç¬¦):")
        print(f"    {response_display}")

    print("\n" + "=" * 80)


def save_split_datasets(
    train_df: pd.DataFrame, test_df: pd.DataFrame, output_dir: str, base_name: str
) -> None:
    """ä¿å­˜åˆ‡åˆ†åçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†"""
    try:
        os.makedirs(output_dir, exist_ok=True)

        train_path = os.path.join(output_dir, f"{base_name}_train.parquet")
        train_df.to_parquet(train_path, index=False)
        logger.info(f"è®­ç»ƒé›†å·²ä¿å­˜åˆ°: {train_path} ({len(train_df)} è¡Œ)")

        test_path = os.path.join(output_dir, f"{base_name}_test.parquet")
        test_df.to_parquet(test_path, index=False)
        logger.info(f"æµ‹è¯•é›†å·²ä¿å­˜åˆ°: {test_path} ({len(test_df)} è¡Œ)")

        logger.info("æ•°æ®é›†åˆ‡åˆ†å®Œæˆç»Ÿè®¡:")
        total = len(train_df) + len(test_df)
        logger.info(f"æ€»æ•°æ®é‡: {total} è¡Œ")
        logger.info(f"è®­ç»ƒé›†æ¯”ä¾‹: {len(train_df) / total * 100:.1f}%")
        logger.info(f"æµ‹è¯•é›†æ¯”ä¾‹: {len(test_df) / total * 100:.1f}%")

        if "dataset_source" in train_df.columns:
            logger.info("\nè®­ç»ƒé›†æ•°æ®æ¥æº:")
            for source, count in train_df["dataset_source"].value_counts().items():
                logger.info(f"  {source}: {count} è¡Œ")

            logger.info("\næµ‹è¯•é›†æ•°æ®æ¥æº:")
            for source, count in test_df["dataset_source"].value_counts().items():
                logger.info(f"  {source}: {count} è¡Œ")

    except Exception as e:
        logger.error(f"ä¿å­˜åˆ‡åˆ†æ•°æ®é›†å¤±è´¥: {e}")
        raise


def main():
    """ä¸»å‡½æ•° - ç›´æ¥ä¿®æ”¹ä¸‹æ–¹å‚æ•°å³å¯è¿è¡Œ"""

    # ==================== é…ç½®å‚æ•°ï¼ˆç›´æ¥ä¿®æ”¹è¿™é‡Œï¼‰====================

    # è¾“å…¥æ–‡ä»¶åˆ—è¡¨ï¼š(æ–‡ä»¶è·¯å¾„, æ•°æ®é›†åç§°)
    # å¯ä»¥æ·»åŠ ä»»æ„æ•°é‡çš„ parquet æ–‡ä»¶
    INPUT_FILES = [
        (
            "/home/jovyan/JQ/gad_gspo_B300/scripts/data_process/trainning_data/reply_generation/processed/reply_id_0a1_train.parquet",
            "dataset1",
        ),
        (
            "/home/jovyan/JQ/gad_gspo_B300/scripts/data_process/trainning_data/reply_generation/processed/reply_id_1_1_train.parquet",
            "dataset2",
        ),
        (
            "/home/jovyan/JQ/gad_gspo_B300/scripts/data_process/trainning_data/reply_generation/processed/reply_id_9_train.parquet",
            "dataset3",
        ),
        (
            "/home/jovyan/JQ/gad_gspo_B300/scripts/data_process/trainning_data/semantic_understanding/processed/semantic_understanding-id-8_2_train.parquet",
            "dataset4",
        ),
        (
            "/home/jovyan/JQ/gad_gspo_B300/scripts/data_process/trainning_data/semantic_understanding/processed/semantic_understanding_1224_train.parquet",
            "dataset5",
        ),
        # æ·»åŠ æ›´å¤šæ–‡ä»¶åªéœ€ç»§ç»­æ·»åŠ å…ƒç»„å³å¯
        # ("/path/to/dataset3_train.parquet", "dataset3"),
    ]

    # è¾“å‡ºè·¯å¾„ï¼ˆå¦‚æœåˆ‡åˆ†ï¼Œä¼šè‡ªåŠ¨ç”Ÿæˆ _train.parquet å’Œ _test.parquetï¼‰
    OUTPUT_PATH = "/home/jovyan/JQ/gad_gspo_B300/scripts/data_process/trainning_data/merged/merge-1225.parquet"

    # æ˜¯å¦æ‰“ä¹±æ•°æ®é›†é¡ºåº
    SHUFFLE = True

    # éšæœºç§å­
    RANDOM_SEED = 42

    # æ˜¯å¦åˆ‡åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
    SPLIT_TRAIN_TEST = True

    # æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆä»…åœ¨ SPLIT_TRAIN_TEST=True æ—¶ç”Ÿæ•ˆï¼‰
    TEST_RATIO = 0.02

    # æ˜¯å¦é¢„è§ˆæŠ½æ ·æ•°æ®
    PREVIEW_SAMPLES = True

    # é¢„è§ˆæŠ½æ ·æ•°é‡
    PREVIEW_NUM = 3

    # ================================================================

    try:
        # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        for file_path, name in INPUT_FILES:
            if not os.path.exists(file_path):
                logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return

        # è¯»å–æ‰€æœ‰æ•°æ®é›†
        dfs = []
        names = []
        for file_path, name in INPUT_FILES:
            df = read_parquet_file(file_path)
            validate_data_structure(df, name)
            dfs.append(df)
            names.append(name)

        # åˆå¹¶æ•°æ®é›†
        merged_df = merge_datasets(dfs, names)

        # å¯é€‰ï¼šæ‰“ä¹±æ•°æ®é›†
        if SHUFFLE:
            merged_df = shuffle_dataset(merged_df, RANDOM_SEED)

        # æŠ½æ ·é¢„è§ˆ
        if PREVIEW_SAMPLES:
            preview_samples(merged_df, PREVIEW_NUM, RANDOM_SEED)

        # æ ¹æ®å‚æ•°å†³å®šæ˜¯å¦åˆ‡åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        if SPLIT_TRAIN_TEST:
            logger.info(f"æ­£åœ¨æŒ‰ {TEST_RATIO*100}% æ¯”ä¾‹åˆ‡åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†...")
            train_df, test_df = sample_test_set(merged_df, TEST_RATIO, RANDOM_SEED)

            output_dir = os.path.dirname(OUTPUT_PATH)
            base_name = os.path.splitext(os.path.basename(OUTPUT_PATH))[0]
            save_split_datasets(train_df, test_df, output_dir, base_name)

            logger.info("æ•°æ®é›†åˆå¹¶å’Œåˆ‡åˆ†å®Œæˆï¼")
            logger.info(f"è®­ç»ƒé›†: {os.path.join(output_dir, f'{base_name}_train.parquet')}")
            logger.info(f"æµ‹è¯•é›†: {os.path.join(output_dir, f'{base_name}_test.parquet')}")
        else:
            save_merged_dataset(merged_df, OUTPUT_PATH)
            logger.info("æ•°æ®é›†åˆå¹¶å®Œæˆï¼")
            logger.info(f"è¾“å‡ºæ–‡ä»¶: {OUTPUT_PATH}")

    except Exception as e:
        logger.error(f"åˆå¹¶è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise


if __name__ == "__main__":
    main()
