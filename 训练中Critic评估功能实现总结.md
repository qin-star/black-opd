# è®­ç»ƒä¸­ Critic è¯„ä¼°åŠŸèƒ½å®ç°æ€»ç»“

## ğŸ“‹ åŠŸèƒ½æ¦‚è¿°

å®ç°äº†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å®šæœŸè¯„ä¼° Critic æ¨¡å‹æ‰“åˆ†èƒ½åŠ›çš„å®Œæ•´åŠŸèƒ½ï¼Œæ ¸å¿ƒç‰¹ç‚¹ï¼š

âœ… **ç›´æ¥ä½¿ç”¨ FSDP åˆ‡ç‰‡æ¨¡å‹** - æ— éœ€åˆå¹¶æ¨¡å‹ï¼Œç›´æ¥ä½¿ç”¨è®­ç»ƒä¸­çš„ Critic å’Œ Actor  
âœ… **è‡ªåŠ¨åŒ–è¯„ä¼°** - åœ¨æŒ‡å®šæ­¥æ•°è‡ªåŠ¨è§¦å‘è¯„ä¼°  
âœ… **çœŸå®è®­ç»ƒçŠ¶æ€** - ä½¿ç”¨å½“å‰è®­ç»ƒä¸­çš„ Actor æ¨¡å‹ç”Ÿæˆ student responses  
âœ… **è¯¦ç»†æŠ¥å‘Š** - ç”Ÿæˆå‡†ç¡®ç‡ã€åˆ†æ•°åˆ†å¸ƒç­‰è¯¦ç»†æŒ‡æ ‡  
âœ… **å¯è§†åŒ–æ”¯æŒ** - è‡ªåŠ¨è®°å½•åˆ° TensorBoard/WandB  

## ğŸ—‚ï¸ æ–‡ä»¶ç»“æ„

### æ ¸å¿ƒä»£ç 

```
verl/verl/trainer/ppo/
â”œâ”€â”€ critic_evaluator.py              # è¯„ä¼°å™¨æ ¸å¿ƒå®ç°
â””â”€â”€ critic_eval_integration.py       # é›†æˆåˆ°è®­ç»ƒæµç¨‹çš„è¾…åŠ©å‡½æ•°
```

### é…ç½®å’Œæ–‡æ¡£

```
tools/
â”œâ”€â”€ critic_eval_config_example.yaml  # é…ç½®ç¤ºä¾‹
â””â”€â”€ test_critic_evaluator.py         # æµ‹è¯•è„šæœ¬

CRITIC_EVALUATION_GUIDE.md           # å®Œæ•´ä½¿ç”¨æŒ‡å—
è®­ç»ƒä¸­Criticè¯„ä¼°åŠŸèƒ½å®ç°æ€»ç»“.md      # æœ¬æ–‡æ¡£
```

## ğŸ”§ æ ¸å¿ƒå®ç°

### 1. CriticEvaluator ç±»

**æ–‡ä»¶**: `verl/verl/trainer/ppo/critic_evaluator.py`

**ä¸»è¦åŠŸèƒ½**:
- ç®¡ç†è¯„ä¼°æ•°æ®é›†å’Œé…ç½®
- ä½¿ç”¨ Actor æ¨¡å‹ç”Ÿæˆ student responses
- ä½¿ç”¨ Critic æ¨¡å‹å¯¹ teacher å’Œ student æ‰“åˆ†
- è®¡ç®—å‡†ç¡®ç‡å’Œåˆ†æ•°ç»Ÿè®¡
- ä¿å­˜è¯¦ç»†è¯„ä¼°ç»“æœ

**å…³é”®æ–¹æ³•**:

```python
class CriticEvaluator:
    def __init__(
        self,
        config,
        critic_module,      # æ”¯æŒ FSDP åˆ‡ç‰‡æ¨¡å‹
        actor_module,       # æ”¯æŒ FSDP åˆ‡ç‰‡æ¨¡å‹
        tokenizer,
        eval_data_path,
        eval_freq=100,
        ...
    )
    
    def should_evaluate(self, step: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥åœ¨å½“å‰æ­¥æ•°è¯„ä¼°"""
    
    def _generate_student_responses(self, prompts, n_responses):
        """ä½¿ç”¨ Actor æ¨¡å‹ç”Ÿæˆ student responses"""
    
    def _get_critic_scores_batch(self, prompts, responses):
        """æ‰¹é‡è·å– Critic åˆ†æ•°"""
    
    def evaluate(self, step: int) -> Dict[str, float]:
        """æ‰§è¡Œå®Œæ•´è¯„ä¼°æµç¨‹"""
```

**å…³é”®ç‰¹æ€§**:

1. **æ”¯æŒ FSDP æ¨¡å‹**:
```python
# è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹å’Œè®¾å¤‡
if hasattr(self.actor_module, 'pretrained_model'):
    if hasattr(self.actor_module.pretrained_model, 'hf_device_map'):
        device = list(self.actor_module.pretrained_model.hf_device_map.values())[0]
    else:
        device = next(self.actor_module.pretrained_model.parameters()).device
```

2. **ä¸è®­ç»ƒæ—¶ä¸€è‡´çš„è¯„åˆ†é€»è¾‘**:
```python
# æ’é™¤ EOS tokenï¼Œè®¡ç®—å¹³å‡åˆ†æ•°ï¼ˆä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
eos_token_id = self.tokenizer.eos_token_id
is_eos = (response_ids == eos_token_id)
response_mask_no_eos = response_mask & (~is_eos)

values_sum = (values * response_mask_no_eos).sum(dim=-1)
length = response_mask_no_eos.sum(dim=-1).clamp(min=1)
score_avg = (values_sum / length).item()
```

3. **æ··åˆ batch ç»“æ„**ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰:
```python
# å…ˆæ·»åŠ æ‰€æœ‰ teachersï¼Œå†æ·»åŠ æ‰€æœ‰ students
mixed_prompts = []
mixed_responses = []

# Teachers first
for prompt, teacher_resp in zip(batch_prompts, batch_teacher_responses):
    mixed_prompts.append(prompt)
    mixed_responses.append(teacher_resp)

# Then students
for prompt, student_resps in zip(batch_prompts, batch_student_responses):
    for student_resp in student_resps:
        mixed_prompts.append(prompt)
        mixed_responses.append(student_resp)
```

### 2. é›†æˆè¾…åŠ©å‡½æ•°

**æ–‡ä»¶**: `verl/verl/trainer/ppo/critic_eval_integration.py`

**ä¸»è¦åŠŸèƒ½**:
- ä»é…ç½®åˆ›å»ºè¯„ä¼°å™¨
- åœ¨è®­ç»ƒå¾ªç¯ä¸­è§¦å‘è¯„ä¼°
- è®°å½•è¯„ä¼°æŒ‡æ ‡åˆ°æ—¥å¿—ç³»ç»Ÿ

**ä½¿ç”¨ç¤ºä¾‹**:

```python
from verl.trainer.ppo.critic_eval_integration import (
    setup_critic_evaluator,
    maybe_evaluate_critic
)

# åœ¨è®­ç»ƒå™¨åˆå§‹åŒ–æ—¶
self.critic_evaluator = setup_critic_evaluator(
    config=self.config,
    critic_module=self.critic_wg.critic_module,
    actor_module=self.actor_rollout_wg.actor_module,
    tokenizer=self.tokenizer,
)

# åœ¨è®­ç»ƒå¾ªç¯ä¸­
eval_metrics = maybe_evaluate_critic(
    evaluator=self.critic_evaluator,
    step=self.global_steps,
    logger_obj=logger,
)
```![1770255959412](image/è®­ç»ƒä¸­Criticè¯„ä¼°åŠŸèƒ½å®ç°æ€»ç»“/1770255959412.png)

## ğŸ“Š è¯„ä¼°æŒ‡æ ‡

### æ ¸å¿ƒæŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ | æœŸæœ›å€¼ |
|------|------|--------|
| `eval/accuracy` | Critic åˆ¤æ–­å‡†ç¡®ç‡ | > 70% |
| `eval/score_diff` | Teacher - Student å¹³å‡åˆ†å·® | > 0.3 |
| `eval/teacher_score_mean` | Teacher å¹³å‡åˆ†æ•° | - |
| `eval/student_score_mean` | Student å¹³å‡åˆ†æ•° | - |

### åˆ†å¸ƒæŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ |
|------|------|
| `eval/teacher_score_std` | Teacher åˆ†æ•°æ ‡å‡†å·® |
| `eval/student_score_std` | Student åˆ†æ•°æ ‡å‡†å·® |

### ç»Ÿè®¡ä¿¡æ¯

| æŒ‡æ ‡ | è¯´æ˜ |
|------|------|
| `eval/num_samples` | è¯„ä¼°æ ·æœ¬æ•° |
| `eval/num_comparisons` | æ€»æ¯”è¾ƒæ¬¡æ•° |

## ğŸš€ ä½¿ç”¨æ–¹æ³•

### æ–¹å¼ 1: å‘½ä»¤è¡Œå‚æ•°ï¼ˆæ¨èï¼‰

```bash
python -m verl.trainer.main_ppo \
    # ... å…¶ä»–è®­ç»ƒå‚æ•° ...
    critic_evaluation.enable=True \
    critic_evaluation.eval_freq=100 \
    critic_evaluation.eval_data_path=/path/to/eval.parquet \
    critic_evaluation.num_eval_samples=100 \
    critic_evaluation.n_resp_per_prompt=4 \
    critic_evaluation.batch_size=8 \
    critic_evaluation.generation_config.temperature=0.6 \
    critic_evaluation.generation_config.max_new_tokens=512
```

### æ–¹å¼ 2: é…ç½®æ–‡ä»¶

```yaml
# config.yaml
critic_evaluation:
  enable: true
  eval_freq: 100
  eval_data_path: "/path/to/eval.parquet"
  num_eval_samples: 100
  n_resp_per_prompt: 4
  batch_size: 8
  generation_config:
    temperature: 0.6
    max_new_tokens: 512
```

### æ–¹å¼ 3: ä»£ç é›†æˆï¼ˆé«˜çº§ï¼‰

å‚è§ `verl/verl/trainer/ppo/critic_eval_integration.py` ä¸­çš„ç¤ºä¾‹ã€‚

## ğŸ“ è¾“å‡ºæ–‡ä»¶

### 1. è¯¦ç»†è¯„ä¼°ç»“æœ

**æ–‡ä»¶**: `critic_eval_results/eval_step_XXX_results.json`

```json
{
  "step": 100,
  "metrics": {
    "eval/accuracy": 0.85,
    "eval/teacher_score_mean": 2.34,
    "eval/student_score_mean": 1.89,
    "eval/score_diff": 0.45
  },
  "results": [
    {
      "teacher_score": 2.5,
      "student_scores": [1.8, 2.1, 1.9, 2.0],
      "correct": 4,
      "total": 4
    }
  ]
}
```

### 2. è¯„ä¼°å†å²

**æ–‡ä»¶**: `critic_eval_results/eval_history.csv`

```csv
step,timestamp,eval/accuracy,eval/teacher_score_mean,eval/student_score_mean,eval/score_diff
100,1706789123.45,0.85,2.34,1.89,0.45
200,1706789234.56,0.87,2.41,1.92,0.49
```

## ğŸ§ª æµ‹è¯•

### ç‹¬ç«‹æµ‹è¯•è„šæœ¬

```bash
python tools/test_critic_evaluator.py \
    --critic_path /path/to/critic/model \
    --actor_path /path/to/actor/model \
    --eval_data /path/to/eval.parquet \
    --num_samples 10 \
    --n_resp_per_prompt 4 \
    --batch_size 4
```

### é¢„æœŸè¾“å‡º

```
================================================================================
Critic è¯„ä¼°å™¨æµ‹è¯•
================================================================================
Critic æ¨¡å‹: /path/to/critic/model
Actor æ¨¡å‹: /path/to/actor/model
è¯„ä¼°æ•°æ®: /path/to/eval.parquet
æ ·æœ¬æ•°: 10

Loading critic model from /path/to/critic/model...
âœ… Critic model loaded
Loading actor model from /path/to/actor/model...
âœ… Actor model loaded

Creating evaluator...
CriticEvaluator initialized:
  - Eval frequency: every 1 steps
  - Eval samples: 10
  - Responses per prompt: 4
  - Use Actor model: True
  - Output dir: ./test_eval_results

================================================================================
å¼€å§‹è¯„ä¼°...
================================================================================
Generating Student responses using Actor model...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:15<00:00,  1.5s/it]
Running batch inference...
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:02<00:00,  1.2it/s]

================================================================================
è¯„ä¼°ç»“æœ
================================================================================
eval/accuracy: 0.8500
eval/teacher_score_mean: 2.3456
eval/student_score_mean: 1.8923
eval/score_diff: 0.4533
eval/num_samples: 10
eval/num_comparisons: 40

âœ… è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°: ./test_eval_results
```

## âš™ï¸ é…ç½®å»ºè®®

### æ˜¾å­˜ä¼˜åŒ–

å¦‚æœæ˜¾å­˜ä¸è¶³ï¼š

```yaml
critic_evaluation:
  batch_size: 4              # å‡å°æ‰¹å¤„ç†å¤§å°
  num_eval_samples: 50       # å‡å°‘æ ·æœ¬æ•°
  generation_config:
    max_new_tokens: 256      # å‡å°‘ç”Ÿæˆé•¿åº¦
```

### é€Ÿåº¦ä¼˜åŒ–

å¦‚æœè¯„ä¼°å¤ªæ…¢ï¼š

```yaml
critic_evaluation:
  eval_freq: 200             # é™ä½è¯„ä¼°é¢‘ç‡
  num_eval_samples: 50       # å‡å°‘æ ·æœ¬æ•°
```

### å‡†ç¡®æ€§ä¼˜åŒ–

å¦‚æœéœ€è¦æ›´å‡†ç¡®çš„è¯„ä¼°ï¼š

```yaml
critic_evaluation:
  num_eval_samples: 200      # å¢åŠ æ ·æœ¬æ•°
  n_resp_per_prompt: 8       # å¢åŠ æ¯ä¸ª prompt çš„ responses
```

## ğŸ” ä¸ç°æœ‰æµ‹è¯•è„šæœ¬çš„å¯¹æ¯”

### ç°æœ‰æµ‹è¯•è„šæœ¬ (`tools/test_critic_training_mode.py`)

**ç‰¹ç‚¹**:
- ç‹¬ç«‹è¿è¡Œï¼Œéœ€è¦æ‰‹åŠ¨æŒ‡å®šæ¨¡å‹è·¯å¾„
- éœ€è¦è°ƒç”¨å¤–éƒ¨ Student API
- é€‚åˆè®­ç»ƒåçš„ç¦»çº¿è¯„ä¼°

**ä½¿ç”¨åœºæ™¯**:
- è®­ç»ƒå®Œæˆåçš„æ¨¡å‹è¯„ä¼°
- å¯¹æ¯”ä¸åŒ checkpoint çš„æ€§èƒ½
- è¯¦ç»†çš„è¯Šæ–­å’Œåˆ†æ

### æ–°çš„è¯„ä¼°åŠŸèƒ½ (`CriticEvaluator`)

**ç‰¹ç‚¹**:
- é›†æˆåˆ°è®­ç»ƒæµç¨‹ï¼Œè‡ªåŠ¨è§¦å‘
- ç›´æ¥ä½¿ç”¨è®­ç»ƒä¸­çš„ Actor æ¨¡å‹
- æ”¯æŒ FSDP åˆ‡ç‰‡æ¨¡å‹ï¼Œæ— éœ€åˆå¹¶

**ä½¿ç”¨åœºæ™¯**:
- è®­ç»ƒè¿‡ç¨‹ä¸­çš„å®æ—¶ç›‘æ§
- åŠæ—¶å‘ç°è®­ç»ƒé—®é¢˜
- è‡ªåŠ¨åŒ–è¯„ä¼°æµç¨‹

### äº’è¡¥å…³ç³»

ä¸¤è€…å¯ä»¥é…åˆä½¿ç”¨ï¼š
- è®­ç»ƒä¸­ä½¿ç”¨ `CriticEvaluator` è¿›è¡Œå¿«é€Ÿç›‘æ§
- è®­ç»ƒåä½¿ç”¨ `test_critic_training_mode.py` è¿›è¡Œè¯¦ç»†åˆ†æ

## ğŸ“ˆ ç›‘æ§å»ºè®®

### TensorBoard å¯è§†åŒ–

```bash
tensorboard --logdir ./tensorboard_log
```

å…³é”®æ›²çº¿ï¼š
1. `eval/accuracy` - åº”è¯¥éšè®­ç»ƒé€æ¸ä¸Šå‡
2. `eval/score_diff` - åº”è¯¥ä¿æŒæ­£å€¼ä¸”ç¨³å®š
3. `eval/teacher_score_mean` vs `eval/student_score_mean` - åº”è¯¥æœ‰æ˜æ˜¾åˆ†ç¦»

### å¥åº·æŒ‡æ ‡

âœ… **è‰¯å¥½çŠ¶æ€**:
- Accuracy > 70%
- Score diff > 0.3
- åˆ†æ•°åˆ†å¸ƒç¨³å®š

âš ï¸ **éœ€è¦å…³æ³¨**:
- Accuracy < 60% â†’ å¯èƒ½ Critic è¿˜åœ¨ warmup æˆ–å­¦ä¹ ç‡è¿‡é«˜
- Score diff < 0.1 â†’ Critic æ— æ³•åŒºåˆ†å¥½åå›ç­”
- åˆ†æ•°æ³¢åŠ¨å‰§çƒˆ â†’ è®­ç»ƒä¸ç¨³å®š

## ğŸ› å¸¸è§é—®é¢˜

### Q1: æ˜¾å­˜ä¸è¶³

**ç—‡çŠ¶**: OOM é”™è¯¯

**è§£å†³**:
```yaml
critic_evaluation:
  batch_size: 4
  num_eval_samples: 50
  generation_config:
    max_new_tokens: 256
```

### Q2: è¯„ä¼°å¤ªæ…¢

**ç—‡çŠ¶**: è¯„ä¼°é˜»å¡è®­ç»ƒæ—¶é—´è¿‡é•¿

**è§£å†³**:
```yaml
critic_evaluation:
  eval_freq: 200        # é™ä½é¢‘ç‡
  num_eval_samples: 50  # å‡å°‘æ ·æœ¬
```

### Q3: å‡†ç¡®ç‡ä¸€ç›´å¾ˆä½

**ç—‡çŠ¶**: `eval/accuracy` < 60%

**å¯èƒ½åŸå› **:
1. Critic è¿˜åœ¨ warmup é˜¶æ®µ
2. å­¦ä¹ ç‡è¿‡é«˜
3. æ•°æ®è´¨é‡é—®é¢˜

**è§£å†³**:
- æ£€æŸ¥ `critic/d_loss` æ˜¯å¦ä¸‹é™
- é™ä½ Critic å­¦ä¹ ç‡
- æ£€æŸ¥è¯„ä¼°æ•°æ®è´¨é‡

### Q4: åˆ†æ•°å·®å¼‚è¿‡å°

**ç—‡çŠ¶**: `eval/score_diff` < 0.1

**å¯èƒ½åŸå› **:
1. Critic æ— æ³•åŒºåˆ†å¥½åå›ç­”
2. Student å’Œ Teacher è´¨é‡æ¥è¿‘
3. Temperature å‚æ•°ä¸åˆé€‚

**è§£å†³**:
- è°ƒæ•´ temperature å‚æ•°
- å¢åŠ è®­ç»ƒæ•°æ®å¤šæ ·æ€§
- æ£€æŸ¥ Critic è®­ç»ƒç­–ç•¥

## ğŸ¯ ä¸‹ä¸€æ­¥ä¼˜åŒ–æ–¹å‘

### 1. å¼‚æ­¥è¯„ä¼°

å½“å‰è¯„ä¼°ä¼šé˜»å¡è®­ç»ƒï¼Œå¯ä»¥è€ƒè™‘ï¼š
- ä½¿ç”¨å•ç‹¬çš„è¿›ç¨‹/çº¿ç¨‹æ‰§è¡Œè¯„ä¼°
- è¯„ä¼°æ—¶ä¸å½±å“è®­ç»ƒæµç¨‹

### 2. æ›´ä¸°å¯Œçš„æŒ‡æ ‡

å¯ä»¥æ·»åŠ ï¼š
- åˆ†æ•°åˆ†å¸ƒçš„å¯è§†åŒ–ï¼ˆç›´æ–¹å›¾ï¼‰
- ä¸åŒéš¾åº¦æ ·æœ¬çš„å‡†ç¡®ç‡
- é•¿åº¦ç›¸å…³çš„åˆ†æ

### 3. è‡ªé€‚åº”è¯„ä¼°

æ ¹æ®è®­ç»ƒçŠ¶æ€åŠ¨æ€è°ƒæ•´ï¼š
- Warmup é˜¶æ®µæ›´é¢‘ç¹è¯„ä¼°
- ç¨³å®šé˜¶æ®µé™ä½é¢‘ç‡
- æ ¹æ®å‡†ç¡®ç‡å˜åŒ–è°ƒæ•´æ ·æœ¬æ•°

### 4. å¤šæ¨¡å‹å¯¹æ¯”

æ”¯æŒåŒæ—¶è¯„ä¼°å¤šä¸ª checkpointï¼š
- å¯¹æ¯”ä¸åŒæ­¥æ•°çš„æ¨¡å‹
- é€‰æ‹©æœ€ä½³ checkpoint

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [å®Œæ•´ä½¿ç”¨æŒ‡å—](CRITIC_EVALUATION_GUIDE.md)
- [é…ç½®ç¤ºä¾‹](tools/critic_eval_config_example.yaml)
- [æµ‹è¯•è„šæœ¬](tools/test_critic_evaluator.py)
- [Critic è®­ç»ƒé—®é¢˜åˆ†æ](training_analysis/docs/Criticè®­ç»ƒé—®é¢˜å®Œæ•´åˆ†æä¸è§£å†³æ–¹æ¡ˆ.md)

## ğŸ¤ è´¡çŒ®

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œæ¬¢è¿æäº¤ Issue æˆ– Pull Requestã€‚

---

**å®ç°æ—¥æœŸ**: 2026-02-04  
**ç‰ˆæœ¬**: v1.0  
**ä½œè€…**: Kiro AI Assistant
