# GAD ä¸‰é˜¶æ®µè®­ç»ƒè„šæœ¬æ€»ç»“

## âœ… æ‰€æœ‰è„šæœ¬å·²å®Œæˆä¿®æ”¹

æˆ‘å·²ç»å®Œæˆäº† GAD è®­ç»ƒçš„ä¸‰ä¸ªé˜¶æ®µçš„å¯åŠ¨è„šæœ¬ä¿®æ”¹ï¼Œæ‰€æœ‰è„šæœ¬éƒ½å‚è€ƒäº†å®˜æ–¹ GSPO è„šæœ¬æ ¼å¼ï¼Œç¡®ä¿å‚æ•°æ­£ç¡®ä¸”ä¸ä¼šå¼•å…¥ä¸å­˜åœ¨çš„é…ç½®ã€‚

---

## ä¸€ã€ä¸‰ä¸ªé˜¶æ®µè„šæœ¬æ¦‚è§ˆ

| é˜¶æ®µ | è„šæœ¬åç§° | è®­ç»ƒç›®æ ‡ | critic_warmup |
|------|---------|---------|---------------|
| **Stage 1** | `gpt5-8b-SeqKD-gspo.sh` | SFT åŸºçº¿ | `-1` |
| **Stage 2** | `gpt5-8b-warmup-gspo.sh` | åˆ¤åˆ«å™¨è®­ç»ƒ | `10` |
| **Stage 3** | `gpt5-8b-gad-gspo.sh` | å¯¹æŠ—è®­ç»ƒ | `0` |

---

## äºŒã€å„é˜¶æ®µè¯¦ç»†é…ç½®

### Stage 1: SeqKD (SFT Baseline)

**è„šæœ¬**ï¼š`scripts/train/chengla_8B_gspo/gpt5-8b-SeqKD-gspo.sh`

**æ ¸å¿ƒé…ç½®**ï¼š
```bash
trainer.critic_warmup=-1              # ç¦ç”¨ Critic
algorithm.kl_ctrl.kl_coef=0.0         # ä¸ä½¿ç”¨ KL æƒ©ç½š
actor.use_kl_loss=False               # ä¸ä½¿ç”¨ KL æŸå¤±
actor.entropy_coeff=0.0               # ä¸ä½¿ç”¨ç†µæ­£åˆ™åŒ–
actor.optim.lr=5e-6                   # Actor å­¦ä¹ ç‡
train_batch_size=256                  # æ‰¹æ¬¡å¤§å°
rollout.n=8                           # ç”Ÿæˆ 8 ä¸ªå“åº”ï¼ˆç”¨äºç›‘æ§ï¼‰
total_epochs=4                        # è®­ç»ƒè½®æ•°
```

**ä½¿ç”¨æ–¹æ³•**ï¼š
```bash
bash gpt5-8b-SeqKD-gspo.sh \
  --model /path/to/base/model \
  --exp_name seqkd_exp \
  --nnodes 1
```

**è¾“å‡º**ï¼š
- `/home/jovyan2/opd_rl/models/seqkd_exp/global_step_XXX/actor/`

---

### Stage 2: Warmup (Discriminator Training)

**è„šæœ¬**ï¼š`scripts/train/chengla_8B_gspo/gpt5-8b-warmup-gspo.sh`

**æ ¸å¿ƒé…ç½®**ï¼š
```bash
trainer.critic_warmup=10              # å‰ 10 æ­¥åªè®­ç»ƒ Critic
algorithm.kl_ctrl.kl_coef=0.001       # KL æƒ©ç½šç³»æ•°
actor.policy_loss.loss_mode=gspo      # ä½¿ç”¨ GSPO ç­–ç•¥æŸå¤±
actor.use_kl_loss=True                # å¯ç”¨ KL æŸå¤±
actor.kl_loss_coef=0.001              # KL æŸå¤±ç³»æ•°
actor.optim.lr=1e-6                   # Actor å­¦ä¹ ç‡
critic.optim.lr=1e-6                  # Critic å­¦ä¹ ç‡
train_batch_size=256                  # æ‰¹æ¬¡å¤§å°
rollout.n=8                           # ç”Ÿæˆ 8 ä¸ªå“åº”
total_epochs=2                        # è®­ç»ƒè½®æ•°
```

**ä½¿ç”¨æ–¹æ³•**ï¼š
```bash
bash gpt5-8b-warmup-gspo.sh \
  --model /path/to/seqkd/checkpoint/actor \
  --reward_model /path/to/reward_model \
  --exp_name warmup_exp \
  --nnodes 1
```

**è¾“å‡º**ï¼š
- `/tmp/warmup_exp/global_step_XXX/actor/`
- `/tmp/warmup_exp/global_step_XXX/critic/`

---

### Stage 3: GAD (Adversarial Training)

**è„šæœ¬**ï¼š`scripts/train/chengla_8B_gspo/gpt5-8b-gad-gspo.sh`

**æ ¸å¿ƒé…ç½®**ï¼š
```bash
trainer.critic_warmup=0               # ä»ç¬¬ 0 æ­¥å¼€å§‹åŒæ—¶è®­ç»ƒ
algorithm.kl_ctrl.kl_coef=0.001       # KL æƒ©ç½šç³»æ•°
actor.policy_loss.loss_mode=gspo      # ä½¿ç”¨ GSPO ç­–ç•¥æŸå¤±
actor.use_kl_loss=True                # å¯ç”¨ KL æŸå¤±
actor.kl_loss_coef=0.001              # KL æŸå¤±ç³»æ•°
actor.optim.lr=1e-6                   # Actor å­¦ä¹ ç‡
critic.optim.lr=1e-6                  # Critic å­¦ä¹ ç‡
train_batch_size=256                  # æ‰¹æ¬¡å¤§å°
rollout.n=8                           # ç”Ÿæˆ 8 ä¸ªå“åº”
rollout.temperature=0.8               # é‡‡æ ·æ¸©åº¦
total_epochs=2                        # è®­ç»ƒè½®æ•°
```

**ç‰¹è‰²åŠŸèƒ½**ï¼š
- âœ… è‡ªåŠ¨å¯åŠ¨ TensorBoardï¼ˆç«¯å£ 6006ï¼‰
- âœ… å®æ—¶ç›‘æ§è®­ç»ƒæŒ‡æ ‡
- âœ… æ”¯æŒ WandB æ—¥å¿—

**ä½¿ç”¨æ–¹æ³•**ï¼š
```bash
bash gpt5-8b-gad-gspo.sh \
  --model /path/to/warmup/checkpoint/actor \
  --reward_model /path/to/warmup/checkpoint/critic \
  --exp_name gad_exp \
  --nnodes 1
```

**è¾“å‡º**ï¼š
- `/home/jovyan2/opd_rl/models/gad_exp/global_step_XXX/actor/`
- `/home/jovyan2/opd_rl/models/gad_exp/global_step_XXX/critic/`
- TensorBoard æ—¥å¿—ï¼š`/home/jovyan2/opd_rl/tensorboard/gad_exp/`

---

## ä¸‰ã€å®Œæ•´çš„è®­ç»ƒæµç¨‹

### 3.1 é¡ºåºæ‰§è¡Œ

```bash
# Step 1: SeqKD è®­ç»ƒï¼ˆSFT åŸºçº¿ï¼‰
bash scripts/train/chengla_8B_gspo/gpt5-8b-SeqKD-gspo.sh \
  --model /path/to/Qwen2.5-8B-Instruct \
  --exp_name seqkd_chengla_8b \
  --nnodes 1

# Step 2: Warmup è®­ç»ƒï¼ˆåˆ¤åˆ«å™¨è®­ç»ƒï¼‰
bash scripts/train/chengla_8B_gspo/gpt5-8b-warmup-gspo.sh \
  --model /home/jovyan2/opd_rl/models/seqkd_chengla_8b/global_step_200/actor \
  --reward_model /path/to/reward_model \
  --exp_name warmup_chengla_8b \
  --nnodes 1

# Step 3: GAD è®­ç»ƒï¼ˆå¯¹æŠ—è®­ç»ƒï¼‰
bash scripts/train/chengla_8B_gspo/gpt5-8b-gad-gspo.sh \
  --model /tmp/warmup_chengla_8b/global_step_800/actor \
  --reward_model /tmp/warmup_chengla_8b/global_step_800/critic \
  --exp_name gad_chengla_8b \
  --nnodes 1
```

### 3.2 æ•°æ®æµ

```
åŸºç¡€æ¨¡å‹ (Qwen2.5-8B-Instruct)
    â†“
[Stage 1: SeqKD]
    â”œâ”€ è¾“å…¥: teacher_response
    â”œâ”€ è®­ç»ƒ: SFT æŸå¤±
    â””â”€ è¾“å‡º: Actor checkpoint
        â†“
[Stage 2: Warmup]
    â”œâ”€ è¾“å…¥: Actor (SeqKD) + Reward Model
    â”œâ”€ è®­ç»ƒ: åˆ¤åˆ«å™¨æŸå¤± + GSPO (warmup=10)
    â””â”€ è¾“å‡º: Actor + Critic checkpoints
        â†“
[Stage 3: GAD]
    â”œâ”€ è¾“å…¥: Actor (Warmup) + Critic (Warmup)
    â”œâ”€ è®­ç»ƒ: åˆ¤åˆ«å™¨æŸå¤± + GSPO (warmup=0)
    â””â”€ è¾“å‡º: æœ€ç»ˆæ¨¡å‹
```

---

## å››ã€å…³é”®å‚æ•°å¯¹æ¯”

| å‚æ•° | SeqKD | Warmup | GAD |
|------|-------|--------|-----|
| **critic_warmup** | `-1` | `10` | `0` |
| **policy_loss.loss_mode** | âŒ | `gspo` | `gspo` |
| **use_kl_loss** | `False` | `True` | `True` |
| **kl_ctrl.kl_coef** | `0.0` | `0.001` | `0.001` |
| **actor.lr** | `5e-6` | `1e-6` | `1e-6` |
| **critic.lr** | âŒ | `5e-6` | `1e-6` |
| **train_batch_size** | `256` | `256` | `256` |
| **rollout.n** | `8` | `8` | `8` |
| **rollout.temperature** | `0.8` | `0.8` | `0.8` |
| **total_epochs** | `4` | `2` | `2` |

---

## äº”ã€æ•°æ®è¦æ±‚

### 5.1 æ‰€æœ‰é˜¶æ®µéƒ½éœ€è¦

**æ•°æ®æ ¼å¼**ï¼š
```python
{
    "content": [
        {"role": "user", "content": "ç”¨æˆ·é—®é¢˜"}
    ],
    "teacher_response": "æ•™å¸ˆæ¨¡å‹çš„é«˜è´¨é‡å›å¤"  # å¿…é¡»åŒ…å«
}
```

### 5.2 æ•°æ®è·¯å¾„

**SeqKD**ï¼š
```bash
data.train_files=/home/jovyan2/opd_rl/data/chengla_train.parquet
data.val_files=/home/jovyan2/opd_rl/data/chengla_test.parquet
```

**Warmup**ï¼š
```bash
data.train_files=/tmp/lmsys_gpt5_chat_4k_filtered_train.parquet
data.val_files=/tmp/lmsys_gpt5_chat_4k_filtered_test.parquet
```

**GAD**ï¼š
```bash
data.train_files=/home/jovyan2/opd_rl/data/chengla_train.parquet
data.val_files=/home/jovyan2/opd_rl/data/chengla_test.parquet
```

---

## å…­ã€ç›‘æ§æŒ‡æ ‡

### Stage 1 (SeqKD)

- `actor/sft_loss`ï¼šSFT æŸå¤±
- `actor/teacher_pg_loss`ï¼šåŒä¸Šï¼ˆå…¼å®¹æ€§ï¼‰
- `actor/lr`ï¼šå­¦ä¹ ç‡
- `actor/grad_norm`ï¼šæ¢¯åº¦èŒƒæ•°

### Stage 2 (Warmup)

**å‰ 10 æ­¥**ï¼ˆåªè®­ç»ƒ Criticï¼‰ï¼š
- `critic/d_loss`ï¼šåˆ¤åˆ«å™¨æ€»æŸå¤±
- `critic/d_acc`ï¼šåˆ¤åˆ«å‡†ç¡®ç‡
- `critic/student_value_mean`ï¼šå­¦ç”Ÿå¾—åˆ†ï¼ˆmeanï¼‰
- `critic/teacher_value_mean`ï¼šæ•™å¸ˆå¾—åˆ†ï¼ˆmeanï¼‰
- `critic/hinge_loss`ï¼šHinge æŸå¤±ï¼ˆmargin è¿åç¨‹åº¦ï¼‰
- `critic/ranking_loss`ï¼šæ’åºæŸå¤±ï¼ˆlogsigmoidï¼‰
- `critic/score_diff`ï¼šteacher - student å·®å€¼

**ç¬¬ 10 æ­¥å**ï¼ˆåŒæ—¶è®­ç»ƒï¼‰ï¼š
- ä¸Šè¿° Critic æŒ‡æ ‡
- `actor/pg_loss`ï¼šç­–ç•¥æŸå¤±
- `actor/pg_clipfrac`ï¼šè£å‰ªæ¯”ä¾‹
- `actor/ppo_kl`ï¼šKL æ•£åº¦
- `actor/kl_loss`ï¼šKL æŸå¤±

### Stage 3 (GAD)

**ä»ç¬¬ 0 æ­¥å¼€å§‹**ï¼š
- æ‰€æœ‰ Critic å’Œ Actor æŒ‡æ ‡
- `critic/d_acc` åº”ä¿æŒåœ¨ 0.5-0.8ï¼ˆä¸åº”ä¸º 1.0ï¼‰
- `critic/score_diff` åº”ç¨³å®šåœ¨ margin (1.0) é™„è¿‘
- `actor/pg_clipfrac` åº”ä¿æŒåœ¨ 0.1-0.3

---

## ä¸ƒã€TensorBoard ä½¿ç”¨

### 7.1 GAD é˜¶æ®µè‡ªåŠ¨å¯åŠ¨

GAD è„šæœ¬ä¼šè‡ªåŠ¨å¯åŠ¨ TensorBoardï¼š
```bash
âœ“ TensorBoard å·²å¯åŠ¨ï¼
  è®¿é—®: http://localhost:6006
  è¿›ç¨‹ ID: 12345
```

### 7.2 è®¿é—®æ–¹å¼

**æœ¬åœ°è®¿é—®**ï¼š
```bash
http://localhost:6006
```

**è¿œç¨‹è®¿é—®**ï¼ˆSSH ç«¯å£è½¬å‘ï¼‰ï¼š
```bash
# åœ¨æœ¬åœ°æœºå™¨è¿è¡Œ
ssh -L 6006:localhost:6006 your_server

# ç„¶ååœ¨æµè§ˆå™¨è®¿é—®
http://localhost:6006
```

### 7.3 ç®¡ç†å‘½ä»¤

**åœæ­¢ TensorBoard**ï¼š
```bash
pkill -f 'tensorboard.*--port=6006'
```

**é‡æ–°å¯åŠ¨**ï¼š
```bash
tensorboard --logdir=/home/jovyan2/opd_rl/tensorboard/gad_exp --port=6006 --bind_all

```

---

## å…«ã€å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•åˆ¤æ–­å½“å‰æ˜¯å“ªä¸ªé˜¶æ®µï¼Ÿ

**A**: æŸ¥çœ‹ `critic_warmup` é…ç½®ï¼š
- `-1`ï¼šSeqKD é˜¶æ®µ
- `10`ï¼šWarmup é˜¶æ®µ
- `0`ï¼šGAD é˜¶æ®µ

### Q2: å¯ä»¥è·³è¿‡æŸä¸ªé˜¶æ®µå—ï¼Ÿ

**A**: 
- âœ… å¯ä»¥è·³è¿‡ SeqKDï¼Œç›´æ¥ä»åŸºç¡€æ¨¡å‹å¼€å§‹ Warmup
- âŒ ä¸å»ºè®®è·³è¿‡ Warmupï¼Œåˆ¤åˆ«å™¨éœ€è¦é¢„è®­ç»ƒ
- âœ… å¯ä»¥åªè¿è¡Œ SeqKD + Warmupï¼Œä¸è¿è¡Œ GAD

### Q3: å¦‚ä½•è°ƒæ•´è®­ç»ƒè½®æ•°ï¼Ÿ

**A**: ä¿®æ”¹ `trainer.total_epochs`ï¼š
```bash
# SeqKD: å»ºè®® 4 è½®
trainer.total_epochs=4

# Warmup: å»ºè®® 2 è½®
trainer.total_epochs=2

# GAD: å»ºè®® 2-4 è½®
trainer.total_epochs=2
```

### Q4: å¦‚ä½•è°ƒæ•´å­¦ä¹ ç‡ï¼Ÿ

**A**: 
```bash
# SeqKD
actor.optim.lr=5e-6  # è¾ƒå¤§å­¦ä¹ ç‡ï¼Œå¿«é€Ÿæ‹Ÿåˆ

# Warmup/GAD
actor.optim.lr=1e-6  # è¾ƒå°å­¦ä¹ ç‡ï¼Œç¨³å®šè®­ç»ƒ
critic.optim.lr=1e-6
```

---

## ä¹ã€å‚è€ƒæ–‡æ¡£

1. **ä»£ç ä¿®æ”¹æ€»ç»“**ï¼š`ä»£ç ä¿®æ”¹æ€»ç»“.md`
2. **SeqKD é€‚é…åˆ†æ**ï¼š`Stage1_SeqKDé€‚é…åˆ†æ.md`
3. **SeqKD ä¿®æ”¹å®Œæˆæ€»ç»“**ï¼š`Stage1_SeqKDä¿®æ”¹å®Œæˆæ€»ç»“.md`
4. **GAD è®­ç»ƒéªŒè¯ç»“æœ**ï¼š`GADè®­ç»ƒéªŒè¯ç»“æœ.md`
5. **GSPO é€‚é…æŒ‡å—**ï¼š`GSPOé€‚é…æŒ‡å—.md`

---

## åã€æ€»ç»“

### 10.1 å·²å®Œæˆçš„å·¥ä½œ

âœ… **ä»£ç ä¿®æ”¹**ï¼š
- `core_algos.py`ï¼šæ·»åŠ  `compute_sft_loss` å’Œ `compute_discriminator_loss`
- `dp_critic.py`ï¼šæ”¯æŒåˆ¤åˆ«å™¨è®­ç»ƒå’Œ teacher forcing
- `dp_actor.py`ï¼šæ”¯æŒ SFT æ¨¡å¼å’Œ GSPO æ¨¡å¼
- `rl_dataset.py`ï¼šæ”¯æŒ `teacher_response` åŠ è½½

âœ… **å¯åŠ¨è„šæœ¬**ï¼š
- `gpt5-8b-SeqKD-gspo.sh`ï¼šStage 1 (SFT)
- `gpt5-8b-warmup-gspo.sh`ï¼šStage 2 (Warmup)
- `gpt5-8b-gad-gspo.sh`ï¼šStage 3 (GAD)

âœ… **æ–‡æ¡£**ï¼š
- è¯¦ç»†çš„ä½¿ç”¨è¯´æ˜
- å®Œæ•´çš„é…ç½®å¯¹æ¯”
- ç›‘æ§æŒ‡æ ‡è¯´æ˜

### 10.2 æ ¸å¿ƒç‰¹ç‚¹

1. **å‚æ•°æ­£ç¡®**ï¼šæ‰€æœ‰å‚æ•°éƒ½å‚è€ƒå®˜æ–¹ GSPO è„šæœ¬
2. **å‘åå…¼å®¹**ï¼šä¸å½±å“ç°æœ‰åŠŸèƒ½
3. **æ˜“äºä½¿ç”¨**ï¼šæ¸…æ™°çš„å‘½ä»¤è¡Œå‚æ•°
4. **å®Œæ•´ç›‘æ§**ï¼šTensorBoard + WandB æ”¯æŒ

### 10.3 ä¸‹ä¸€æ­¥

1. âœ… **ç«‹å³å¯ç”¨**ï¼šæ‰€æœ‰è„šæœ¬éƒ½å¯ä»¥ç›´æ¥è¿è¡Œ
2. ğŸŸ¢ **å»ºè®®æµ‹è¯•**ï¼šå…ˆè¿è¡Œå°è§„æ¨¡æµ‹è¯•éªŒè¯
3. ğŸŸ¢ **ç›‘æ§è®­ç»ƒ**ï¼šä½¿ç”¨ TensorBoard å®æ—¶æŸ¥çœ‹æŒ‡æ ‡

---


### 11.1 é—®é¢˜èƒŒæ™¯

åœ¨ Warmup é˜¶æ®µè®­ç»ƒä¸­å‘ç° Critic æ— æ³•æœ‰æ•ˆå­¦ä¹ ï¼Œè¡¨ç°ä¸ºï¼š
- `critic/d_loss` æ¥è¿‘ 0ï¼ˆ1.58e-06ï¼‰
- `critic/d_acc` ä¸º 1.0ï¼ˆè¿‡æ‹Ÿåˆï¼‰
- `critic/grad_norm` æå°ï¼ˆ0.000252ï¼‰
- `student_value_mean` (2.57) ä¸ `teacher_value_mean` (18.125) å·®è·å¤§ä½†æ— æ³•æ”¶æ•›

**æ ¹å› åˆ†æ**ï¼šå½“ teacher - student å·®è·è¾ƒå¤§æ—¶ï¼Œ`logsigmoid` å‡½æ•°é¥±å’Œï¼Œæ¢¯åº¦è¶‹è¿‘äº 0ã€‚

### 11.2 åŸå§‹å®ç°ï¼ˆå®˜æ–¹ç‰ˆæœ¬ï¼‰

**æ–‡ä»¶**ï¼š`verl/trainer/ppo/core_algos.py`

```python
def compute_discriminator_loss(student_vpreds, teacher_vpreds, response_mask, teacher_response_mask):
    # ä½¿ç”¨ sum è®¡ç®— sequence-level score
    teacher_reward = torch.sum(teacher_vpreds * teacher_response_mask, dim=-1)
    student_reward = torch.sum(student_vpreds * response_mask, dim=-1)
    
    # ä»…ä½¿ç”¨ logsigmoid ranking loss
    d_loss = -nn.functional.logsigmoid(teacher_reward - student_reward).mean()
    return d_loss
```

**é—®é¢˜**ï¼š
- ä½¿ç”¨ `sum` å¯¼è‡´é•¿åºåˆ— score è¿‡å¤§
- `logsigmoid(15.5) â‰ˆ 0`ï¼Œæ¢¯åº¦æ¶ˆå¤±
- è¿”å›å•ä¸ª tensorï¼Œæ— æ³•ç›‘æ§å„ç»„ä»¶

### 11.3 ä¿®å¤åå®ç°ï¼ˆå½“å‰ç‰ˆæœ¬ï¼‰

```python
def compute_discriminator_loss(
    student_vpreds, teacher_vpreds, response_mask, teacher_response_mask,
    margin: float = 1.0,
) -> tuple:
    # ä½¿ç”¨ mean è®¡ç®— sequence-level scoreï¼ˆscale invariantï¼‰
    teacher_mask_sum = teacher_response_mask.sum(dim=-1).clamp(min=1)
    student_mask_sum = response_mask.sum(dim=-1).clamp(min=1)
    
    teacher_score = torch.sum(teacher_vpreds * teacher_response_mask, dim=-1) / teacher_mask_sum
    student_score = torch.sum(student_vpreds * response_mask, dim=-1) / student_mask_sum
    
    diff = teacher_score - student_score
    
    # 1. Hinge loss: ç¡®ä¿ teacher > student + margin
    hinge_loss = F.relu(margin - diff).mean()
    
    # 2. Logsigmoid with temperature scalingï¼ˆé˜²æ­¢é¥±å’Œï¼‰
    temperature = 2.0
    scaled_diff = torch.clamp(diff / temperature, min=-10, max=10)
    ranking_loss = -F.logsigmoid(scaled_diff).mean()
    
    # ç»„åˆæŸå¤±
    d_loss = hinge_loss + 0.5 * ranking_loss
    
    loss_info = {
        "hinge_loss": hinge_loss.detach().item(),
        "ranking_loss": ranking_loss.detach().item(),
        "score_diff": diff.mean().detach().item(),
    }
    return d_loss, loss_info
```

### 11.4 ä¿®æ”¹å¯¹æ¯”

| æ–¹é¢ | åŸå§‹å®ç° | ä¿®å¤å | æ”¹è¿›åŸå›  |
|------|----------|--------|----------|
| **Score è®¡ç®—** | `sum` | `mean` | é¿å…é•¿åºåˆ—å¯¼è‡´ score è¿‡å¤§ |
| **Ranking loss** | `logsigmoid(diff)` | `logsigmoid(diff/2.0)` | Temperature scaling é˜²æ­¢é¥±å’Œ |
| **Hinge loss** | âŒ æ—  | `relu(margin - diff)` | ç¡®ä¿æŒç»­æ¢¯åº¦ |
| **è¿”å›å€¼** | å•ä¸ª tensor | `(loss, loss_info)` | æ”¯æŒè¯¦ç»†ç›‘æ§ |

### 11.5 è„šæœ¬é…ç½®ä¿®å¤

**`gpt5-8b-warmup-gspo-2.sh`**ï¼š
- âœ… æ·»åŠ  `actor_rollout_ref.actor.policy_loss.loss_mode=$loss_mode`
- âœ… æé«˜ `critic_lr` ä» `1e-6` åˆ° `5e-6`

**`gpt5-8b-warmup-gspo.sh`**ï¼š
- âœ… æé«˜ `critic.optim.lr` ä» `1e-6` åˆ° `5e-6`

### 11.6 æœŸæœ›è®­ç»ƒæ•ˆæœï¼ˆV1 ç‰ˆæœ¬ï¼‰

| æŒ‡æ ‡ | æœŸæœ›è¶‹åŠ¿ | è¯´æ˜ |
|------|----------|------|
| `critic/hinge_loss` | ä¸‹é™ â†’ 0 | margin æ¡ä»¶æ»¡è¶³ |
| `critic/ranking_loss` | ç¨³å®šåœ¨è¾ƒä½å€¼ | æ’åºæ­£ç¡® |
| `critic/score_diff` | ç¨³å®šåœ¨ 1.0 é™„è¿‘ | è¾¾åˆ° margin ç›®æ ‡ |
| `critic/d_acc` | 0.6-0.8 | ä¸åº”ä¸º 1.0ï¼ˆè¿‡æ‹Ÿåˆï¼‰ |
| `critic/grad_norm` | > 0.01 | æœ‰æ•ˆæ¢¯åº¦æ›´æ–° |

---

## 12. Discriminator Loss V2ï¼šåŒå‘çº¦æŸç‰ˆæœ¬ï¼ˆ2024-12-05ï¼‰

### 12.1 V1 ç‰ˆæœ¬é—®é¢˜

V1 ç‰ˆæœ¬åœ¨å®é™…è®­ç»ƒä¸­å‘ç°ä¸¥é‡é—®é¢˜ï¼š

**è®­ç»ƒæ—¥å¿—ï¼ˆStep 141ï¼‰**ï¼š
```
critic/d_loss: 2.26e-05          âŒ è¶‹è¿‘äº 0
critic/grad_norm: 0.0            âŒ æ¢¯åº¦æ¶ˆå¤±
critic/score_diff: 117.0         âŒ å·®è·çˆ†ç‚¸
critic/student_value_mean: -115.75  âŒ æŒç»­ä¸‹é™
response_length/mean: 1.0        âŒ æ¨¡å‹å´©æºƒ
```

**é—®é¢˜æ ¹å› **ï¼š
- V1 åªæœ‰**ä¸‹ç•Œçº¦æŸ**ï¼ˆ`diff >= margin`ï¼‰
- å½“ `score_diff > margin` æ—¶ï¼Œ`hinge_loss = 0`ï¼Œæ— æ¢¯åº¦
- Critic ä¸æ–­å‹ä½ student scoreï¼Œæ²¡æœ‰ä¸Šç•Œé™åˆ¶
- `score_diff` ä» 1.8 â†’ 117ï¼Œstudent_value ä» -0.998 â†’ -115.75
- æœ€ç»ˆæ¨¡å‹å´©æºƒï¼Œåªç”Ÿæˆ 1 ä¸ª token

### 12.2 V2 ç‰ˆæœ¬è®¾è®¡ï¼šåŒå‘ Hinge Loss

```python
def compute_discriminator_loss(
    student_vpreds, teacher_vpreds, response_mask, teacher_response_mask,
    margin: float = 1.0,
) -> tuple:
    # ä½¿ç”¨ mean è®¡ç®— sequence-level scoreï¼ˆscale invariantï¼‰
    teacher_mask_sum = teacher_response_mask.sum(dim=-1).clamp(min=1)
    student_mask_sum = response_mask.sum(dim=-1).clamp(min=1)
    
    teacher_score = torch.sum(teacher_vpreds * teacher_response_mask, dim=-1) / teacher_mask_sum
    student_score = torch.sum(student_vpreds * response_mask, dim=-1) / student_mask_sum
    
    diff = teacher_score - student_score
    
    # 1. ä¸‹ç•Œçº¦æŸï¼šç¡®ä¿ diff >= margin (teacher è¦æ¯” student é«˜)
    hinge_loss_lower = F.relu(margin - diff).mean()
    
    # 2. ä¸Šç•Œçº¦æŸï¼šç¡®ä¿ diff <= max_margin (é˜²æ­¢ student è·‘å‘è´Ÿæ— ç©·)
    max_margin = 5.0  # æœ€å¤§å…è®¸å·®è·
    hinge_loss_upper = F.relu(diff - max_margin).mean()
    
    # 3. Logsigmoid with temperature scalingï¼ˆé˜²æ­¢é¥±å’Œï¼‰
    temperature = 2.0
    scaled_diff = torch.clamp(diff / temperature, min=-10, max=10)
    ranking_loss = -F.logsigmoid(scaled_diff).mean()
    
    # ç»„åˆæŸå¤±ï¼šåŒå‘çº¦æŸ + ranking
    d_loss = hinge_loss_lower + hinge_loss_upper + 0.5 * ranking_loss
    
    loss_info = {
        "hinge_loss_lower": hinge_loss_lower.detach().item(),
        "hinge_loss_upper": hinge_loss_upper.detach().item(),
        "ranking_loss": ranking_loss.detach().item(),
        "score_diff": diff.mean().detach().item(),
    }
    return d_loss, loss_info
```

### 12.3 V1 vs V2 å¯¹æ¯”

| æ–¹é¢ | V1 | V2 | æ”¹è¿›åŸå›  |
|------|-----|-----|----------|
| **ä¸‹ç•Œçº¦æŸ** | `relu(margin - diff)` | `relu(margin - diff)` | ç›¸åŒ |
| **ä¸Šç•Œçº¦æŸ** | âŒ æ—  | `relu(diff - max_margin)` | **é˜²æ­¢ score_diff çˆ†ç‚¸** |
| **ç›®æ ‡èŒƒå›´** | `diff >= 1.0` | `1.0 <= diff <= 5.0` | åŒå‘çº¦æŸ |
| **ç›‘æ§æŒ‡æ ‡** | `hinge_loss` | `hinge_loss_lower`, `hinge_loss_upper` | æ›´ç»†ç²’åº¦ |

### 12.4 V2 æœŸæœ›è®­ç»ƒæ•ˆæœ

| æŒ‡æ ‡ | æœŸæœ›èŒƒå›´ | è¯´æ˜ |
|------|----------|------|
| `critic/score_diff` | **1.0 ~ 5.0** | åœ¨åŒå‘çº¦æŸèŒƒå›´å†… |
| `critic/hinge_loss_lower` | â†’ 0 | diff > margin |
| `critic/hinge_loss_upper` | â†’ 0 | diff < max_margin |
| `critic/grad_norm` | > 0 | æœ‰æ•ˆæ¢¯åº¦ |
| `critic/student_value_mean` | ç¨³å®š | ä¸ä¼šæŒç»­ä¸‹é™ |

### 12.5 dp_critic.py åŒæ­¥ä¿®æ”¹

```python
micro_batch_metrics.update(
    {
        "critic/d_loss": d_loss.detach().item(),
        "critic/d_acc": d_acc.detach().item(),
        "critic/student_value_mean": student_score.mean().detach().item(),
        "critic/teacher_value_mean": teacher_score.mean().detach().item(),
        "critic/hinge_loss_lower": loss_info["hinge_loss_lower"],  # V2 æ–°å¢
        "critic/hinge_loss_upper": loss_info["hinge_loss_upper"],  # V2 æ–°å¢
        "critic/ranking_loss": loss_info["ranking_loss"],
        "critic/score_diff": loss_info["score_diff"],
    }
)
```

---

## 13. Actor è®­ç»ƒé—®é¢˜åˆ†æï¼ˆ2024-12-05ï¼‰

### 13.1 é—®é¢˜ç°è±¡

å³ä½¿ Critic æŒ‡æ ‡æ”¹å–„åï¼ŒActor ä»ç„¶æ— æ³•å­¦ä¹ ï¼š

```
actor/pg_loss: 0.0           âŒ å§‹ç»ˆä¸º 0
actor/ppo_kl: 0.0            âŒ å§‹ç»ˆä¸º 0
actor/grad_norm: 3.64e-07    âŒ å‡ ä¹ä¸º 0
critic/advantages/mean: 0.0  âŒ ä¼˜åŠ¿å‡½æ•°ä¸º 0
response_length: 130 â†’ 7     âŒ é€æ¸é€€åŒ–
```

### 13.2 æ ¹å› åˆ†æï¼šGRPO + GAD çš„å…¼å®¹æ€§é—®é¢˜

**GRPO Advantage è®¡ç®—**ï¼š
```python
# å¯¹äºåŒä¸€ä¸ª prompt çš„ n ä¸ª response
scores = token_level_rewards.sum(dim=-1)  # æ¥è‡ª Critic values
advantage[i] = (score[i] - mean(scores)) / std(scores)
```

**GAD æ¨¡å¼ä¸‹çš„é—®é¢˜**ï¼š
1. Critic å¯¹åŒä¸€ä¸ª prompt çš„æ‰€æœ‰ student response ç»™å‡º**å‡ ä¹ç›¸åŒçš„ value**
2. å› ä¸ºå®ƒä»¬éƒ½æ˜¯åŒä¸€ä¸ª student æ¨¡å‹ç”Ÿæˆçš„
3. `std(scores) â‰ˆ 0` â†’ `advantages â‰ˆ 0`
4. `pg_loss = -advantages * ratio = 0`

**é—®é¢˜é“¾**ï¼š
```
temperature æœªè®¾ç½®ï¼ˆé»˜è®¤ä½ï¼‰
    â†“
åŒä¸€ prompt çš„ n ä¸ª response å‡ ä¹ç›¸åŒ
    â†“
Critic ç»™å®ƒä»¬çš„ value å‡ ä¹ç›¸åŒ
    â†“
GRPO: score - mean â‰ˆ 0, std â‰ˆ 0
    â†“
advantages = 0
    â†“
pg_loss = 0, Actor æ— æ¢¯åº¦
    â†“
æ¨¡å‹é€æ¸é€€åŒ–
```

### 13.3 è§£å†³æ–¹æ¡ˆï¼šå¢åŠ  Response å¤šæ ·æ€§

**ä¿®æ”¹ `gpt5-8b-warmup-gspo-2.sh`**ï¼š

```bash
# ä¿®æ”¹å‰
n_resp_per_prompt=4
# temperature æœªè®¾ç½®

# ä¿®æ”¹å
n_resp_per_prompt=8      # å¢åŠ åˆ° 8ï¼Œä¸å®˜æ–¹ä¸€è‡´
temperature=0.8          # å¢åŠ å¤šæ ·æ€§

# å‘½ä»¤è¡Œæ·»åŠ 
actor_rollout_ref.rollout.temperature=$temperature \
```

### 13.4 é…ç½®å¯¹æ¯”

| å‚æ•° | ä¿®æ”¹å‰ | ä¿®æ”¹å | å®˜æ–¹å€¼ |
|------|--------|--------|--------|
| `n_resp_per_prompt` | 4 | **8** | 8 |
| `temperature` | æœªè®¾ç½® | **0.8** | 0.8 |

### 13.5 æœŸæœ›æ•ˆæœ

å¢åŠ  temperature åï¼š
- åŒä¸€ prompt çš„ response æ›´å¤šæ ·åŒ–
- Critic ç»™å‡ºçš„ value æœ‰å·®å¼‚
- `std(scores) > 0`
- `advantages â‰  0`
- `pg_loss â‰  0`
- Actor èƒ½å¤Ÿå­¦ä¹ 

---

## 14. å›æ»šæ–¹æ³•

### 14.1 å›æ»šåˆ° V1 Loss

ä¿®æ”¹ `core_algos.py`ï¼Œç§»é™¤ä¸Šç•Œçº¦æŸï¼š

```python
def compute_discriminator_loss(...):
    # ... score è®¡ç®—ç›¸åŒ ...
    
    # åªä¿ç•™ä¸‹ç•Œçº¦æŸ
    hinge_loss = F.relu(margin - diff).mean()
    
    # ç§»é™¤ä¸Šç•Œçº¦æŸ
    # hinge_loss_upper = F.relu(diff - max_margin).mean()
    
    d_loss = hinge_loss + 0.5 * ranking_loss
    
    loss_info = {
        "hinge_loss": hinge_loss.detach().item(),  # æ”¹å›å•ä¸ª hinge_loss
        "ranking_loss": ranking_loss.detach().item(),
        "score_diff": diff.mean().detach().item(),
    }
    return d_loss, loss_info
```

### 14.2 å›æ»šåˆ°åŸå§‹å®ç°

```python
def compute_discriminator_loss(student_vpreds, teacher_vpreds, response_mask, teacher_response_mask):
    teacher_reward = torch.sum(teacher_vpreds * teacher_response_mask, dim=-1)
    student_reward = torch.sum(student_vpreds * response_mask, dim=-1)
    d_loss = -torch.nn.functional.logsigmoid(teacher_reward - student_reward).mean()
    return d_loss  # æ³¨æ„ï¼šè¿”å›å•ä¸ª tensorï¼Œéœ€åŒæ­¥ä¿®æ”¹ dp_critic.py
```

---

## 15. ç‰ˆæœ¬å†å²

| ç‰ˆæœ¬ | æ—¥æœŸ | ä¸»è¦ä¿®æ”¹ | çŠ¶æ€ |
|------|------|----------|------|
| åŸå§‹ | - | `logsigmoid(sum_diff)` | âŒ æ¢¯åº¦æ¶ˆå¤± |
| V1 | 2024-12-05 | æ·»åŠ  hinge loss + temperature | âš ï¸ score_diff çˆ†ç‚¸ |
| V2 | 2024-12-05 | åŒå‘ hinge loss | âœ… å½“å‰ç‰ˆæœ¬ |

---

**æœ€ç»ˆç»“è®º**ï¼šâœ… **Discriminator Loss V2 + Temperature é…ç½®ä¿®å¤åï¼Œå¯ä»¥å¼€å§‹å®Œæ•´çš„ GAD è®­ç»ƒæµç¨‹ï¼**

---

## 16. Discriminator Loss æœ€ç»ˆç‰ˆæœ¬ï¼šç§»é™¤èƒ½åŠ›å¤©èŠ±æ¿ï¼ˆ2024-12-17ï¼‰

### 16.1 é—®é¢˜èƒŒæ™¯

åœ¨å®é™…è®­ç»ƒä¸­å‘ç°ï¼ŒåŸæœ‰çš„ `hinge_loss_lower` æˆä¸ºäº† student æ¨¡å‹è¶…è¶Š teacher çš„**äººä¸ºå¤©èŠ±æ¿**ï¼š

**è®­ç»ƒç›®æ ‡å†²çª**ï¼š
- ç”¨æˆ·æœŸæœ›ï¼šstudent æ¨¡å‹èƒ½å¤Ÿ**æ¨¡ä»¿å¹¶è¶…è¶Š** teacher æ¨¡å‹
- åŸæœ‰è®¾è®¡ï¼š`hinge_loss_lower = relu(margin - diff)` å¼ºåˆ¶è¦æ±‚ `teacher > student + margin`
- ç»“æœï¼šå½“ student æ¥è¿‘ teacher æ—¶ï¼Œåˆ¤åˆ«å™¨è¢«æƒ©ç½šï¼Œé˜»æ­¢äº†è¿›ä¸€æ­¥å­¦ä¹ 

**æ ¸å¿ƒçŸ›ç›¾**ï¼š
```python
# å½“ student æ¥è¿‘æˆ–è¶…è¶Š teacher æ—¶
diff = teacher_score - student_score  # å¯èƒ½ < marginï¼Œç”šè‡³ < 0
hinge_loss_lower = relu(margin - diff)  # > 0ï¼Œæƒ©ç½šåˆ¤åˆ«å™¨
# è¿™å®é™…ä¸Šæ˜¯åœ¨é˜»æ­¢ student å˜å¾—æ›´å¥½ï¼
```

### 16.2 ç†è®ºåˆ†æï¼šGAN æ¡†æ¶ä¸‹çš„æ”¶æ•›

åœ¨æ ‡å‡† GAN æ¡†æ¶ä¸­ï¼š
- **Generator (student)** çš„ç›®æ ‡ï¼šéª—è¿‡åˆ¤åˆ«å™¨
- **Discriminator** çš„ç›®æ ‡ï¼šåŒºåˆ† real (teacher) å’Œ fake (student)

**ç†æƒ³æ”¶æ•›çŠ¶æ€**ï¼š
- å½“ student å®Œç¾æ¨¡ä»¿ teacher æ—¶ï¼Œåˆ¤åˆ«å™¨æ— æ³•åŒºåˆ† â†’ Nash å‡è¡¡
- `score_diff â†’ 0`ï¼Œ`ranking_loss â†’ log(2) â‰ˆ 0.693`
- æ­¤æ—¶ student è¾¾åˆ° teacher æ°´å¹³ï¼Œä½†**ä¸ä¼šè¶…è¶Š**

**è¶…è¶Šçš„æ¡ä»¶**ï¼š
- éœ€è¦**é¢å¤–çš„å¥–åŠ±ä¿¡å·**ï¼ˆå¦‚ä»»åŠ¡å¥–åŠ±ã€äººç±»åå¥½ç­‰ï¼‰
- çº¯åˆ¤åˆ«å™¨æ¡†æ¶çš„å¤©èŠ±æ¿å°±æ˜¯ teacher æ°´å¹³

### 16.3 æœ€ç»ˆç‰ˆæœ¬è®¾è®¡

åŸºäºä¸Šè¿°åˆ†æï¼Œè®¾è®¡äº†**ç§»é™¤èƒ½åŠ›å¤©èŠ±æ¿**çš„ç‰ˆæœ¬ï¼š

```python
def compute_discriminator_loss(
    student_vpreds: torch.Tensor,
    teacher_vpreds: torch.Tensor,
    response_mask: torch.Tensor,
    teacher_response_mask: torch.Tensor,
    margin: float = 0.5,  # ä¿ç•™æ¥å£å…¼å®¹ï¼Œä½†ä¸å†ä½¿ç”¨
) -> tuple:
    """
    æ”¹è¿›ç‰ˆï¼šç§»é™¤äº† hinge_loss_lowerï¼Œå…è®¸ student æ— é™é€¼è¿‘ teacherã€‚
    
    è®¾è®¡ç›®æ ‡ï¼š
    - åˆ¤åˆ«å™¨å­¦ä¹ åŒºåˆ† teacher å’Œ student çš„èƒ½åŠ›
    - ä¸è®¾ç½®äººä¸ºçš„èƒ½åŠ›å¤©èŠ±æ¿ï¼Œå…è®¸ student è¾¾åˆ°ç”šè‡³æ¥è¿‘ teacher æ°´å¹³
    - å½“ student æ¥è¿‘ teacher æ—¶ï¼Œranking_loss è‡ªç„¶å¢å¤§ï¼Œæ¨åŠ¨åˆ¤åˆ«å™¨å¯»æ‰¾æ›´ç»†å¾®å·®åˆ«
    
    æ”¶æ•›çŠ¶æ€ï¼š
    - score_diff â†’ 0: student å’Œ teacher è´¨é‡ç›¸å½“
    - ranking_loss â†’ log(2) â‰ˆ 0.693: åˆ¤åˆ«å™¨æ— æ³•åŒºåˆ†ä¸¤è€…
    """
    
    # 1. å¾—åˆ†è®¡ç®—
    teacher_score_raw = torch.sum(teacher_vpreds * teacher_response_mask, dim=-1)
    student_score_raw = torch.sum(student_vpreds * response_mask, dim=-1)
    
    # å½’ä¸€åŒ–å¾—åˆ†ï¼ˆé˜²æ­¢é•¿åº¦åè§ï¼‰
    eps = 1e-8
    teacher_mask_sum = teacher_response_mask.sum(dim=-1).clamp(min=eps)
    student_mask_sum = response_mask.sum(dim=-1).clamp(min=eps)
    teacher_score = teacher_score_raw / teacher_mask_sum
    student_score = student_score_raw / student_mask_sum
    diff = teacher_score - student_score
    
    # 2. æŸå¤±ç»„ä»¶
    
    # Ranking Lossï¼šè½¯çº¦æŸï¼Œå³ä½¿ student > teacher ä¹Ÿæœ‰æ¢¯åº¦
    temperature = 2.0  # å¹³è¡¡ç‰ˆæœ¬ï¼Œé¿å…è¿‡å¼ºæˆ–è¿‡å¼±
    scaled_diff = diff / temperature
    scaled_diff = torch.clamp(scaled_diff, min=-10, max=10)
    ranking_loss = -torch.nn.functional.logsigmoid(scaled_diff).mean()
    
    # Score Regularizationï¼šé˜²æ­¢æ•°å€¼æ¼‚ç§»
    score_reg = 0.005 * (teacher_score_raw.pow(2).mean() + student_score_raw.pow(2).mean())
    
    # Over-confidence Penaltyï¼šè½¯çº¦æŸï¼Œé˜²æ­¢åˆ¤åˆ«å™¨è¿‡åº¦è‡ªä¿¡
    diff_penalty = torch.nn.functional.relu(diff - 1.5).pow(2).mean()
    
    # 3. æ€»æŸå¤±ï¼ˆç§»é™¤äº† hinge_loss_lowerï¼‰
    d_loss = 1.5 * ranking_loss + score_reg + 0.5 * diff_penalty
    
    return d_loss, loss_info
```

### 16.4 ç‰ˆæœ¬æ¼”è¿›å¯¹æ¯”

| ç‰ˆæœ¬ | æ ¸å¿ƒç‰¹ç‚¹ | é—®é¢˜ | é€‚ç”¨åœºæ™¯ |
|------|----------|------|----------|
| **åŸå§‹** | `logsigmoid(sum_diff)` | æ¢¯åº¦æ¶ˆå¤± | âŒ ä¸å¯ç”¨ |
| **V1** | æ·»åŠ  `hinge_loss_lower` | é˜»æ­¢è¶…è¶Š | ä¸¥æ ¼æ¨¡ä»¿ |
| **V2** | åŒå‘ hinge loss | å¤æ‚ï¼Œä»æœ‰å¤©èŠ±æ¿ | å—æ§è®­ç»ƒ |
| **æœ€ç»ˆ** | ç§»é™¤ `hinge_loss_lower` | æ—  | **æ¨¡ä»¿+è¶…è¶Š** |

### 16.5 è®­ç»ƒåŠ¨æ€åˆ†æ

**è®­ç»ƒåˆæœŸ**ï¼ˆstudent << teacherï¼‰ï¼š
- `diff > 0`ï¼Œ`ranking_loss` è¾ƒå°
- åˆ¤åˆ«å™¨å®¹æ˜“åŒºåˆ†ï¼Œç»™ student æ˜ç¡®å­¦ä¹ ä¿¡å·

**è®­ç»ƒä¸­æœŸ**ï¼ˆstudent â‰ˆ teacherï¼‰ï¼š
- `diff â†’ 0`ï¼Œ`ranking_loss â†’ 0.693`
- åˆ¤åˆ«å™¨è¶Šæ¥è¶Šéš¾åŒºåˆ†ï¼Œæ¨åŠ¨å¯»æ‰¾æ›´ç»†å¾®å·®åˆ«

**ç†æƒ³æ”¶æ•›**ï¼ˆstudent â‰ˆ teacherï¼‰ï¼š
- `score_diff â‰ˆ 0`ï¼Œ`d_acc â‰ˆ 50%`ï¼ˆéšæœºçŒœæµ‹ï¼‰
- åˆ¤åˆ«å™¨æ— æ³•å¯é åŒºåˆ†ï¼Œè®­ç»ƒä¿¡å·è‡ªç„¶å‡å¼±
- student è¾¾åˆ° teacher æ°´å¹³

### 16.6 å®é™…è®­ç»ƒéªŒè¯

**ä¿®æ”¹å‰**ï¼ˆæœ‰ hinge_loss_lowerï¼‰ï¼š
```
step:19 - d_acc:0.998 - score_diff:0.038
response_length/clip_ratio:1.0  # æ¨¡å¼å´©æºƒ
```

**ä¿®æ”¹å**ï¼ˆç§»é™¤ hinge_loss_lowerï¼‰ï¼š
```
step:56 - d_acc:0.59 - score_diff:0.0026
response_length/mean:255 â†’ 246  # é€æ¸æ”¹å–„
```

### 16.7 å‚æ•°è°ƒä¼˜å†ç¨‹

| å‚æ•° | è¿‡å¼ºé…ç½® | è¿‡å¼±é…ç½® | æœ€ç»ˆå¹³è¡¡ |
|------|----------|----------|----------|
| `temperature` | 1.0 (d_acc=99%) | 3.0 (d_acc=52%) | **2.0** |
| `score_reg` | 0.001 | 0.01 | **0.005** |
| `diff_penalty` é˜ˆå€¼ | 2.0 | 0.5 | **1.5** |
| `ranking_loss` æƒé‡ | 2.0 | 1.0 | **1.5** |

---

## 17. è®­ç»ƒè„šæœ¬ä¼˜åŒ–ï¼šè§£å†³ response é•¿åº¦é—®é¢˜ï¼ˆ2024-12-17ï¼‰

### 17.1 æ•°æ®é›†åˆ†æå‘ç°çš„é—®é¢˜

é€šè¿‡ `analyze_tokens.py` åˆ†æè®­ç»ƒæ•°æ®ï¼š

```
Teacher Response ç»Ÿè®¡:
- P95: 159 tokens
- P99: 174 tokens  
- Max: 175 tokens

å»ºè®® max_response_length: 174 (è¦†ç›– 99% æ ·æœ¬)
```

**ä½†è®­ç»ƒè„šæœ¬è®¾ç½®**ï¼š
```bash
max_response_length=512  # è¿‡å¤§ 3xï¼
```

**è®­ç»ƒæ—¥å¿—æ˜¾ç¤ºçš„é—®é¢˜**ï¼š
```
response_length/mean: 477.6
response_length/clip_ratio: 86.1%  # 86% è¢«æˆªæ–­ï¼
```

### 17.2 é—®é¢˜æ ¹å› 

1. **Teacher response æœ€é•¿ 175 tokens**
2. **Student ç”Ÿæˆ 477 tokens å¹³å‡é•¿åº¦**
3. **åˆ¤åˆ«å™¨ä¸»è¦é é•¿åº¦åŒºåˆ†**ï¼Œè€Œéå†…å®¹è´¨é‡
4. **Actor å­¦ä¸åˆ°æœ‰æ•ˆä¿¡å·**ï¼š`pg_loss â‰ˆ 0`

### 17.3 è§£å†³æ–¹æ¡ˆ

**æ ¸å¿ƒä¿®æ”¹**ï¼š
```bash
# æ•°æ®åŒ¹é…çš„é•¿åº¦è®¾ç½®
max_response_length=256   # ä» 512 æ”¹ä¸º 256
max_model_len=2048        # ä» 2550 æ”¹ä¸º 2048

# å‡å°‘æ— æ„ä¹‰ç”Ÿæˆ
temperature=0.8           # ä» 1.0 æ”¹ä¸º 0.8

# å¢åŠ è®­ç»ƒæ­¥æ•°
train_batch_size=128      # ä» 256 æ”¹ä¸º 128
total_epochs=8            # ä» 4 æ”¹ä¸º 8
```

**è®­ç»ƒæ­¥æ•°è®¡ç®—**ï¼š
```
åŸé…ç½®: 1000æ¡ / 256 = 4æ­¥/epoch Ã— 4epochs = 16æ­¥
æ–°é…ç½®: 1000æ¡ / 128 = 8æ­¥/epoch Ã— 8epochs = 64æ­¥
```

### 17.4 GSPO Clip Ratio ä¿®å¤

å‘ç°åŸé…ç½®çš„ `clip_ratio` è¿‡å°ï¼š
```bash
# åŸé…ç½®ï¼ˆå¯¼è‡´ 63% æ›´æ–°è¢«è£å‰ªï¼‰
clip_ratio_low=3e-4
clip_ratio_high=4e-4

# ä¿®å¤å
clip_ratio_low=0.1
clip_ratio_high=0.1
```

**GSPO ä¸­çš„ä½œç”¨**ï¼š
```python
# importance ratio è¢«é™åˆ¶åœ¨ [1-0.1, 1+0.1] = [0.9, 1.1]
seq_importance_ratio = torch.clamp(ratio, 1 - clip_ratio_low, 1 + clip_ratio_high)
```

---

## 18. ä¸ºä»€ä¹ˆéœ€è¦åˆ†é˜¶æ®µè®­ç»ƒï¼Ÿ

### 18.1 è¡¨é¢å·®å¼‚

| é˜¶æ®µ | critic_warmup | æ¨¡å‹æ¥æº | è®­ç»ƒè½®æ•° |
|------|---------------|----------|----------|
| Warmup | 10 | SeqKD â†’ Warmup | 2 epochs |
| GAD | 0 | Warmup â†’ GAD | 4 epochs |

### 18.2 æ ¸å¿ƒåŸå› ï¼švLLM æ¨ç†å¼•æ“é™åˆ¶

**å…³é”®é—®é¢˜**ï¼švLLM åœ¨è®­ç»ƒå¼€å§‹æ—¶åŠ è½½æ¨¡å‹æƒé‡ï¼Œ**è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä¼šåŒæ­¥æ›´æ–°**ã€‚

```
Warmup é˜¶æ®µ:
Step 0-10:  vLLM ç”¨ SeqKD æƒé‡ç”Ÿæˆ response
           Actor æƒé‡ä» SeqKD å¼€å§‹æ›´æ–°
           
Step 11+:   vLLM è¿˜æ˜¯ç”¨ SeqKD æƒé‡ç”Ÿæˆ response  âŒ
           Actor æƒé‡å·²ç»å˜åŒ–å¾ˆå¤§
           ç”Ÿæˆçš„ response ä¸å½“å‰ Actor ä¸åŒ¹é…

[é‡æ–°å¯åŠ¨è®­ç»ƒï¼Œé‡æ–°åŠ è½½ checkpoint]

GAD é˜¶æ®µ:
Step 0+:    vLLM ç”¨ Warmup åçš„æƒé‡ç”Ÿæˆ response  âœ…
           Actor ç»§ç»­ä» Warmup æƒé‡ä¼˜åŒ–
           ç”Ÿæˆä¸è®­ç»ƒåŒæ­¥
```

### 18.3 å¦‚æœä¸åˆ†é˜¶æ®µçš„åæœ

```
è¿ç»­è®­ç»ƒ 6 epochs:
Epoch 1-2: æ­£å¸¸ï¼ˆvLLM æƒé‡ = Actor æƒé‡ï¼‰
Epoch 3-6: å¼‚å¸¸ï¼ˆvLLM æƒé‡ â‰  Actor æƒé‡ï¼‰
          è®­ç»ƒæ•ˆæœå˜å·®ï¼Œå¯èƒ½ä¸ç¨³å®š
```

### 18.4 å·¥ç¨‹ vs ç®—æ³•

| æ–¹é¢ | è¯´æ˜ |
|------|------|
| **ç®—æ³•å±‚é¢** | Warmup å’Œ GAD æ²¡æœ‰æœ¬è´¨åŒºåˆ« |
| **å·¥ç¨‹å±‚é¢** | vLLM å¼•æ“éœ€è¦é‡æ–°åŠ è½½æƒé‡ |
| **è§£å†³æ–¹æ¡ˆ** | åˆ†é˜¶æ®µè®­ç»ƒï¼Œä¿è¯ç”Ÿæˆä¸è®­ç»ƒçš„ä¸€è‡´æ€§ |

---

## 19. å¥åº·è®­ç»ƒæŒ‡æ ‡æ€»ç»“

åŸºäºå¤šæ¬¡è°ƒè¯•ç»éªŒï¼Œæ€»ç»“å¥åº·çš„è®­ç»ƒæŒ‡æ ‡ï¼š

### 19.1 åˆ¤åˆ«å™¨æŒ‡æ ‡

| æŒ‡æ ‡ | å¥åº·èŒƒå›´ | å¼‚å¸¸ä¿¡å· |
|------|----------|----------|
| `d_acc` | 55% ~ 80% | >90% è¿‡å¼ºï¼Œ<50% è¿‡å¼± |
| `score_diff` | 0.01 ~ 0.5 | >1.0 è¿‡å¤§ï¼Œ<0.001 æ— åŒºåˆ† |
| `ranking_loss` | 0.65 ~ 0.69 | 0.693 æ— æ³•åŒºåˆ† |
| `diff_penalty` | â‰ˆ 0 | >0.1 åˆ¤åˆ«å™¨è¿‡åº¦è‡ªä¿¡ |

### 19.2 Actor æŒ‡æ ‡

| æŒ‡æ ‡ | å¥åº·èŒƒå›´ | å¼‚å¸¸ä¿¡å· |
|------|----------|----------|
| `pg_loss` | |å€¼| > 0.001 | â‰ˆ0 æ— å­¦ä¹ ä¿¡å· |
| `pg_clipfrac` | 10% ~ 30% | >50% æ›´æ–°è¿‡æ¿€ |
| `ppo_kl` | > 0 | =0 ç­–ç•¥æœªæ›´æ–° |

### 19.3 Response è´¨é‡

| æŒ‡æ ‡ | å¥åº·èŒƒå›´ | å¼‚å¸¸ä¿¡å· |
|------|----------|----------|
| `response_length/mean` | æ¥è¿‘ teacher é•¿åº¦ | æŒç»­å¢é•¿åˆ° max |
| `response_length/clip_ratio` | < 30% | >80% å¤§é‡æˆªæ–­ |

---

## 20. æœ€ç»ˆé…ç½®æ¨è

åŸºäºæ‰€æœ‰è°ƒè¯•ç»éªŒï¼Œæ¨èçš„æœ€ç»ˆé…ç½®ï¼š

### 20.1 Loss å‚æ•°ï¼ˆcore_algos.pyï¼‰

```python
# å¹³è¡¡çš„åˆ¤åˆ«å™¨é…ç½®
temperature = 2.0           # åˆ¤åˆ«å™¨å¼ºåº¦é€‚ä¸­
score_reg = 0.005          # é€‚åº¦æ­£åˆ™åŒ–
diff_penalty_threshold = 1.5  # è¿‡åº¦è‡ªä¿¡é˜ˆå€¼
ranking_loss_weight = 1.5   # ä¸»è¦æŸå¤±æƒé‡
```

### 20.2 è®­ç»ƒå‚æ•°ï¼ˆè„šæœ¬ï¼‰

```bash
# æ•°æ®åŒ¹é…çš„é•¿åº¦
max_response_length=256
max_model_len=2048

# åˆç†çš„ batch size å’Œè®­ç»ƒæ­¥æ•°
train_batch_size=128
total_epochs=8

# GSPO clip ratio
clip_ratio_low=0.1
clip_ratio_high=0.1

# ç”Ÿæˆå¤šæ ·æ€§
temperature=0.8
n_resp_per_prompt=8

# å­¦ä¹ ç‡
actor_lr=3e-6
critic_lr=1e-6  # é™ä½ï¼Œè®©åˆ¤åˆ«å™¨æ…¢æ…¢å­¦ä¹ 
critic_warmup=10  # åˆ¤åˆ«å™¨é¢„è®­ç»ƒ
```

---

**æ€»ç»“**ï¼šç»è¿‡å¤šè½®è°ƒè¯•å’Œç†è®ºåˆ†æï¼Œæœ€ç»ˆå½¢æˆäº†**ç§»é™¤èƒ½åŠ›å¤©èŠ±æ¿ + å¹³è¡¡å‚æ•°é…ç½®**çš„ GAD è®­ç»ƒæ–¹æ¡ˆï¼Œèƒ½å¤Ÿè®© student æ¨¡å‹åœ¨åˆ¤åˆ«å™¨æ¡†æ¶ä¸‹æ— é™é€¼è¿‘ teacher æ°´å¹³ã€‚
---

## 21. GSPO è®­ç»ƒ Bug ä¿®å¤è®°å½•ï¼ˆå†å²é—®é¢˜ï¼‰

### 21.1 é—®é¢˜ 1: Actor ä¸å­¦ä¹  (`ppo_kl = 0`)

**ç—‡çŠ¶**ï¼š
```
actor/ppo_kl: 0.0
actor/pg_clipfrac: 0.0
```

**æ ¹å› **ï¼šåœ¨ `verl/workers/actor/dp_actor.py` ä¸­ï¼š
```python
on_policy = len(mini_batches) == 1 and self.config.ppo_epochs == 1

if on_policy:
    old_log_prob = log_prob.detach()  # â† å¯¼è‡´ ppo_kl = 0
```

**è§£å†³**ï¼šè®­ç»ƒè„šæœ¬æ·»åŠ  `ppo_epochs=2`

### 21.2 é—®é¢˜ 2: Discriminator Score æ¼‚ç§»

**ç—‡çŠ¶**ï¼š
```
student_value_mean: -167.0
teacher_value_mean: 167.75
raw_score_diff: 335.5
```

**æ ¹å› **ï¼š
1. Score æ­£åˆ™åŒ–å¤ªå¼±
2. `hinge_loss_upper` ä½¿ç”¨é”™è¯¯çš„ diffï¼ˆå½’ä¸€åŒ– vs rawï¼‰

**è§£å†³**ï¼š
- ä½¿ç”¨ raw diff è®¡ç®—ä¸Šç•Œçº¦æŸ
- Score æ­£åˆ™åŒ–ä½¿ç”¨ raw scores

### 21.3 é—®é¢˜ 3: `d_acc` è®¡ç®—é”™è¯¯

**ç—‡çŠ¶**ï¼š`d_acc: 1.0`ï¼ˆå§‹ç»ˆä¸º 1.0ï¼‰

**æ ¹å› **ï¼šå¯¹å·² masked çš„ vpreds é‡å¤å½’ä¸€åŒ–

**è§£å†³**ï¼šç›´æ¥ä½¿ç”¨ `sum()`ï¼Œä¸å†é™¤ä»¥ `mask_sum`

---

## 22. è®­ç»ƒè„šæœ¬åˆå¹¶å‘½ä»¤

```bash
# åˆå¹¶ FSDP checkpoint ä¸º HuggingFace æ ¼å¼
python verl/scripts/legacy_model_merger.py merge \
    --backend fsdp \
    --local_dir /path/to/fsdp/checkpoint/actor \
    --target_dir /path/to/merged/actor \
    --hf_model_path /path/to/base/model
```