"""
æµ‹è¯•è®­ç»ƒåçš„Criticå’ŒStudentæ¨¡å‹ - ç®€åŒ–ç‰ˆï¼ˆä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´ï¼‰

ä¸è®­ç»ƒä»£ç çš„ä¸€è‡´æ€§ï¼š
- ä½¿ç”¨å¹³å‡åˆ†æ•°ï¼ˆæ’é™¤EOS tokenï¼‰ï¼Œä¸è®­ç»ƒä»£ç ä¸€è‡´
- ä¸ä½¿ç”¨æ··åˆåˆ†æ•°å’Œbatch normalization
- æ¯ä¸ªpromptç”Ÿæˆ8ä¸ªstudent responsesè¿›è¡Œè¯„ä¼°
"""
import requests
import pandas as pd
import argparse
from typing import Dict, Optional, List
from datetime import datetime
from tqdm import tqdm
import time
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


# ==================== é…ç½®åŒºåŸŸ ====================

# æµ‹è¯•å‚æ•°é…ç½®
TEST_CONFIG = {
    "data_path": "/home/jovyan/JQ/gad_gspo_B300/data/trainning_dataset/subject_1-29/merged/merge-1-29.parquet",
    "num_samples": 100,
    "random_sample": True,
    "random_seed": 42,
    "output_dir": "/home/jovyan/JQ/gad_gspo_B300/outputs",
    "output_filename": None,
    
    # å¤šæ ·æœ¬ç”Ÿæˆé…ç½®
    "num_student_samples": 8,  # æ¯ä¸ªpromptç”Ÿæˆ8ä¸ªstudent response
    "student_temperature": 0.6,  # æé«˜temperatureå¢åŠ å¤šæ ·æ€§
}

# æ¨¡å‹é…ç½®
API_CONFIGS = {
    "critic_model": {
        "name": "critic-model",
        "type": "local",
        "model_path": "/home/jovyan/JQ/gad_gspo_B300/models/opd-v9-1-29-fsdp2/global_step_500/critic_merged",
        "device": "cuda:4",
        "force_trl": True,
    },
    "student_model": {
        "name": "student-model",
        "type": "api",
        "url": "http://10.72.1.39:8008/v1/chat/completions",
        "api_key": "sk-xxxx",
        "model_name": "opd-v9-500",
        "temperature": 0.6,
        "repetition_penalty": 1.2
    }
}
# ================================================



class CriticTester:
    """ç®€åŒ–ç‰ˆCriticæµ‹è¯•å™¨ - ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´"""
    
    def __init__(self, critic_config: Dict, student_config: Dict, test_config: Dict):
        self.critic_config = critic_config
        self.student_config = student_config
        self.test_config = test_config
        
        # åŠ è½½Criticæ¨¡å‹
        self.critic_model = None
        self.critic_tokenizer = None
        if critic_config.get("type") == "local":
            print(f"ğŸ”„ åŠ è½½æœ¬åœ°Criticæ¨¡å‹: {critic_config['model_path']}")
            self.load_critic_model(critic_config)
            print(f"âœ… Criticæ¨¡å‹åŠ è½½å®Œæˆ")
    
    def load_critic_model(self, config: Dict):
        """åŠ è½½Criticæ¨¡å‹"""
        from trl import AutoModelForCausalLMWithValueHead
        from transformers import AutoModelForCausalLM
        
        device = config.get("device", "cuda:0")
        model_path = config["model_path"]
        use_train_mode = config.get("use_train_mode", False)
        
        print(f"ğŸ”„ åŠ è½½tokenizer...")
        self.critic_tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        print(f"ğŸ”„ åŠ è½½æ¨¡å‹...")
        base_model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            device_map=device
        )
        
        self.critic_model = AutoModelForCausalLMWithValueHead.from_pretrained(base_model)
        
        # æ ¹æ®é…ç½®é€‰æ‹©trainæˆ–evalæ¨¡å¼
        if use_train_mode:
            self.critic_model.train()
            print(f"âš ï¸  ä½¿ç”¨trainæ¨¡å¼ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰")
        else:
            self.critic_model.eval()
            print(f"âœ… ä½¿ç”¨evalæ¨¡å¼")
    
    def call_student_api(self, prompt: str, debug: bool = False) -> str:
        """è°ƒç”¨Student APIç”Ÿæˆresponse"""
        try:
            config = self.student_config
            messages = [{"role": "user", "content": prompt}]
            
            payload = {
                "model": config["model_name"],
                "messages": messages,
                "max_tokens": 512,
                "temperature": config.get("temperature", 0.8),
                "top_p": 0.9,
                "repetition_penalty": config.get("repetition_penalty", 1.2),
            }
            
            headers = {
                "Authorization": f"Bearer {config['api_key']}",
                "Content-Type": "application/json"
            }
            
            response = requests.post(config["url"], json=payload, headers=headers, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            raw_content = result['choices'][0]['message']['content']
            
            # åªç§»é™¤é¦–å°¾ç©ºç™½ï¼Œä¿æŒåŸå§‹è¾“å‡º
            cleaned_content = raw_content.strip()
            
            # è°ƒè¯•æ¨¡å¼ï¼šæ˜¾ç¤ºåŸå§‹è¾“å‡º
            if debug and raw_content != cleaned_content:
                print(f"  âš ï¸  æ£€æµ‹åˆ°é¦–å°¾ç©ºç™½å­—ç¬¦:")
                print(f"    åŸå§‹: {repr(raw_content)}")
                print(f"    æ¸…ç†å: {repr(cleaned_content)}")
            
            return cleaned_content
        except Exception as e:
            print(f"âŒ Student APIè°ƒç”¨å¤±è´¥: {e}")
            return ""
    
    def get_critic_score(self, prompt: str, response: str) -> tuple:
        """
        è·å–Criticåˆ†æ•°ï¼ˆå•æ ·æœ¬ï¼Œä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´ï¼‰
        
        Returns:
            (score_avg, length): å¹³å‡åˆ†æ•°å’Œresponseé•¿åº¦ï¼ˆæ’é™¤EOSï¼‰
        """
        try:
            messages = [
                {"role": "user", "content": prompt},
                {"role": "assistant", "content": response}
            ]
            
            input_text = self.critic_tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=False
            )
            
            inputs = self.critic_tokenizer(
                input_text, return_tensors="pt", truncation=True, max_length=2048
            )
            
            device = next(self.critic_model.pretrained_model.parameters()).device
            inputs = inputs.to(device)
            
            # å‰å‘ä¼ æ’­
            with torch.no_grad():
                outputs = self.critic_model(**inputs, use_cache=False)
                
                # è·å–responseé•¿åº¦
                response_tokens = self.critic_tokenizer(
                    response, add_special_tokens=False, return_tensors="pt"
                )
                response_length = response_tokens['input_ids'].size(1)
                
                # æå–responseéƒ¨åˆ†çš„values
                values = outputs[2][:, -response_length:]
                if values.dim() == 3:
                    values = values.squeeze(-1)
                
                # è·å–maskå¹¶æ’é™¤EOS token
                attention_mask = inputs['attention_mask']
                response_mask = attention_mask[:, -response_length:]
                response_ids = inputs['input_ids'][:, -response_length:]
                
                eos_token_id = self.critic_tokenizer.eos_token_id
                is_eos = (response_ids == eos_token_id)
                response_mask_no_eos = response_mask & (~is_eos)
                
                # è®¡ç®—å¹³å‡åˆ†æ•°ï¼ˆä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰
                values_sum = torch.sum(values * response_mask_no_eos, dim=-1)
                length = response_mask_no_eos.sum(dim=-1)
                score_avg = (values_sum / length.clamp(min=1)).item()
                length = length.item()
                
                return score_avg, length
        except Exception as e:
            print(f"âŒ Criticè¯„åˆ†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0.0, 0
    
    def get_critic_scores_batch(self, prompts: list, responses: list) -> tuple:
        """
        è·å–Criticåˆ†æ•°ï¼ˆæ‰¹é‡æ¨ç†ï¼Œä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰
        
        Args:
            prompts: promptåˆ—è¡¨
            responses: responseåˆ—è¡¨
        
        Returns:
            (scores, lengths): åˆ†æ•°åˆ—è¡¨å’Œé•¿åº¦åˆ—è¡¨
        """
        try:
            device = next(self.critic_model.pretrained_model.parameters()).device
            batch_size = len(prompts)
            
            # å‡†å¤‡batchæ•°æ®
            all_input_texts = []
            all_response_lengths = []
            
            for prompt, response in zip(prompts, responses):
                messages = [
                    {"role": "user", "content": prompt},
                    {"role": "assistant", "content": response}
                ]
                
                input_text = self.critic_tokenizer.apply_chat_template(
                    messages, tokenize=False, add_generation_prompt=False
                )
                all_input_texts.append(input_text)
                
                # è®¡ç®—responseé•¿åº¦
                response_tokens = self.critic_tokenizer(
                    response, add_special_tokens=False, return_tensors="pt"
                )
                all_response_lengths.append(response_tokens['input_ids'].size(1))
            
            # Batch tokenizationï¼ˆä½¿ç”¨paddingï¼‰
            inputs = self.critic_tokenizer(
                all_input_texts, 
                return_tensors="pt", 
                truncation=True, 
                max_length=2048,
                padding=True  # å…³é”®ï¼šæ·»åŠ padding
            )
            inputs = inputs.to(device)
            
            # Batchå‰å‘ä¼ æ’­
            with torch.no_grad():
                outputs = self.critic_model(**inputs, use_cache=False)
                
                # æå–æ¯ä¸ªæ ·æœ¬çš„values
                scores = []
                lengths = []
                
                for i in range(batch_size):
                    response_length = all_response_lengths[i]
                    
                    # æå–responseéƒ¨åˆ†çš„values
                    values = outputs[2][i:i+1, -response_length:]
                    if values.dim() == 3:
                        values = values.squeeze(-1)
                    
                    # è·å–maskå¹¶æ’é™¤EOS token
                    attention_mask = inputs['attention_mask'][i:i+1]
                    response_mask = attention_mask[:, -response_length:]
                    response_ids = inputs['input_ids'][i:i+1, -response_length:]
                    
                    eos_token_id = self.critic_tokenizer.eos_token_id
                    is_eos = (response_ids == eos_token_id)
                    response_mask_no_eos = response_mask & (~is_eos)
                    
                    # è®¡ç®—å¹³å‡åˆ†æ•°
                    values_sum = torch.sum(values * response_mask_no_eos, dim=-1)
                    length = response_mask_no_eos.sum(dim=-1)
                    score_avg = (values_sum / length.clamp(min=1)).item()
                    
                    scores.append(score_avg)
                    lengths.append(length.item())
                
                return scores, lengths
                
        except Exception as e:
            print(f"âŒ Batch Criticè¯„åˆ†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return [0.0] * len(prompts), [0] * len(prompts)

    
    def test_single_prompt(self, prompt: str, teacher_response: str, debug: bool = False) -> Dict:
        """æµ‹è¯•å•ä¸ªpromptï¼ˆç”Ÿæˆå¤šä¸ªstudent responsesï¼‰
        
        æ³¨æ„ï¼šæ­£ç¡®ç‡åˆ¤æ–­ä½¿ç”¨ student_score <= teacher_scoreï¼ˆåŒ…æ‹¬ç›¸ç­‰æƒ…å†µï¼‰
        """
        print("ğŸ“ Prompt:")
        print(f"  {prompt}")
        print()
        
        num_students = self.test_config.get("num_student_samples", 8)
        
        # ç”Ÿæˆå¤šä¸ªStudentå“åº”
        print(f"ğŸ”„ ç”Ÿæˆ {num_students} ä¸ªStudentå“åº”...")
        student_responses = []
        for i in range(num_students):
            response = self.call_student_api(prompt, debug=debug)
            if response:
                student_responses.append(response)
                print(f"  âœ“ Student #{i+1}: {len(response)} chars")
        print()
        
        if len(student_responses) == 0:
            print("âŒ æ²¡æœ‰æˆåŠŸç”Ÿæˆstudent responses")
            return None
        
        # è·å–Teacheråˆ†æ•°
        teacher_score, teacher_length = self.get_critic_score(prompt, teacher_response)
        
        # è·å–æ‰€æœ‰Studentåˆ†æ•°
        student_scores = []
        student_lengths = []
        for response in student_responses:
            score, length = self.get_critic_score(prompt, response)
            student_scores.append(score)
            student_lengths.append(length)
        
        # æ‰“å°ç»“æœ
        print("ğŸ‘¨â€ğŸ« Teacher Response:")
        print("-" * 100)
        print(f"  åˆ†æ•°(avg): {teacher_score:7.4f}")
        print(f"  Length: {int(teacher_length):3d}")
        print(f"  Text: {teacher_response}")
        if debug:
            print(f"  Repr: {repr(teacher_response)}")
        print()
        
        print(f"ğŸ“ Student Responses ({len(student_responses)} ä¸ª):")
        print("-" * 100)
        for i, (response, score, length) in enumerate(zip(student_responses, student_scores, student_lengths)):
            diff = score - teacher_score
            # ä¿®æ”¹åˆ¤æ–­é€»è¾‘ï¼šstudent_score <= teacher_score ä¸ºæ­£ç¡®ï¼ˆåŒ…æ‹¬ç›¸ç­‰æƒ…å†µï¼‰
            is_correct = score <= teacher_score
            status = 'âœ…æ­£ç¡®' if is_correct else 'âŒé”™è¯¯'
            
            # ç‰¹æ®Šæ ‡è®°ç›¸ç­‰æƒ…å†µ
            if abs(diff) < 1e-6:  # æµ®ç‚¹æ•°ç›¸ç­‰åˆ¤æ–­
                status += ' (ç›¸ç­‰)'
            
            print(f"\n  Student #{i+1}:")
            print(f"    åˆ†æ•°(avg): {score:7.4f}")
            print(f"    Length: {int(length):3d}")
            print(f"    ä¸Teacheråˆ†å·®: {diff:7.4f} ({status})")
            print(f"    Text: {response}")
            
            # è°ƒè¯•æ¨¡å¼ï¼šæ˜¾ç¤ºè¯¦ç»†çš„å­—ç¬¦å·®å¼‚
            if debug and response != teacher_response:
                print(f"    Repr: {repr(response)}")
                # æ£€æŸ¥æ˜¯å¦åªæ˜¯ç©ºç™½å­—ç¬¦å·®å¼‚
                if response.replace(' ', '').replace('\n', '').replace('\t', '') == \
                   teacher_response.replace(' ', '').replace('\n', '').replace('\t', ''):
                    print(f"    âš ï¸  ä¸Teacherä»…ç©ºç™½å­—ç¬¦ä¸åŒ")
        
        # ç»Ÿè®¡ï¼ˆä¿®æ”¹åˆ¤æ–­é€»è¾‘ï¼šåŒ…æ‹¬ç›¸ç­‰æƒ…å†µï¼‰
        correct_count = sum(1 for score in student_scores if score <= teacher_score)
        accuracy = correct_count / len(student_scores)
        
        print()
        print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  Teacheråˆ†æ•°: {teacher_score:7.4f}")
        print(f"  Studentå¹³å‡åˆ†æ•°: {sum(student_scores)/len(student_scores):7.4f}")
        print(f"  å‡†ç¡®ç‡: {correct_count}/{len(student_scores)} ({accuracy*100:.1f}%)")
        print(f"  æ³¨ï¼šæ­£ç¡®ç‡åˆ¤æ–­ä½¿ç”¨ student_score <= teacher_scoreï¼ˆåŒ…æ‹¬ç›¸ç­‰æƒ…å†µï¼‰")
        print()
        print("=" * 100)
        print()
        
        return {
            'prompt': prompt,
            'teacher_response': teacher_response,
            'teacher_score': teacher_score,
            'teacher_length': teacher_length,
            'student_responses': student_responses,
            'student_scores': student_scores,
            'student_lengths': student_lengths,
            'correct_count': correct_count,
            'accuracy': accuracy,
            'num_students': len(student_responses),
        }
    
    def test_dataset(self, data_path: str, num_samples: Optional[int] = None,
                    output_path: Optional[str] = None, use_batch_inference: bool = True,
                    batch_size: int = 32) -> pd.DataFrame:
        """æµ‹è¯•æ•°æ®é›†
        
        Args:
            data_path: æ•°æ®é›†è·¯å¾„
            num_samples: æµ‹è¯•æ ·æœ¬æ•°é‡
            output_path: è¾“å‡ºè·¯å¾„
            use_batch_inference: æ˜¯å¦ä½¿ç”¨batchæ¨ç†ï¼ˆæ¨èTrueï¼Œä¸è®­ç»ƒä¸€è‡´ï¼‰
            batch_size: batchæ¨ç†çš„æ‰¹æ¬¡å¤§å°
        """
        # è¯»å–æ•°æ®
        if data_path.endswith('.parquet'):
            df = pd.read_parquet(data_path)
        else:
            raise ValueError("ä»…æ”¯æŒ.parquetæ ¼å¼")
        
        # é‡‡æ ·
        if num_samples:
            if self.test_config.get("random_sample"):
                df = df.sample(n=min(num_samples, len(df)), 
                             random_state=self.test_config.get("random_seed", 42))
            else:
                df = df.head(num_samples)
        
        print(f"ğŸ“Š æµ‹è¯• {len(df)} ä¸ªæ ·æœ¬...")
        print(f"   æ¨ç†æ¨¡å¼: {'Batchæ¨ç†' if use_batch_inference else 'å•æ ·æœ¬æ¨ç†'}")
        if use_batch_inference:
            print(f"   Batchå¤§å°: {batch_size}")
        print()
        
        if use_batch_inference:
            results = self._test_dataset_batch(df, batch_size)
        else:
            results = self._test_dataset_single(df)
        
        # è½¬æ¢ä¸ºDataFrame
        results_df = pd.DataFrame(results)
        
        # æ‰“å°ç»Ÿè®¡
        self.print_statistics(results_df, use_batch_inference)
        
        # ä¿å­˜
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            mode = "batch" if use_batch_inference else "single"
            output_path = f"{self.test_config['output_dir']}/test_results_{mode}_{timestamp}.xlsx"
        
        if not output_path.endswith('.xlsx'):
            output_path += '.xlsx'
        
        import os
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        results_df.to_excel(output_path, index=False)
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
        return results_df
    
    def _test_dataset_single(self, df: pd.DataFrame) -> list:
        """å•æ ·æœ¬æ¨ç†æ¨¡å¼"""
        results = []
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="æµ‹è¯•è¿›åº¦ï¼ˆå•æ ·æœ¬ï¼‰"):
            try:
                # è§£ææ•°æ®
                content = row['content']
                if isinstance(content, (list, tuple)) and len(content) > 0:
                    prompt = content[0].get('content', '') if isinstance(content[0], dict) else str(content[0])
                else:
                    prompt = str(content)
                
                teacher_response = row.get('teacher_response', '')
                
                if not prompt or not teacher_response:
                    continue
                
                result = self.test_single_prompt(prompt, teacher_response, 
                                                debug=self.test_config.get('debug', False))
                if result:
                    result['sample_id'] = idx
                    results.append(result)
                
                time.sleep(0.5)
            except Exception as e:
                print(f"âŒ æµ‹è¯•æ ·æœ¬ {idx} å¤±è´¥: {e}")
                continue
        
        return results
    
    def _test_dataset_batch(self, df: pd.DataFrame, batch_size: int = 32) -> list:
        """Batchæ¨ç†æ¨¡å¼ï¼ˆä¸è®­ç»ƒä¸€è‡´ï¼‰"""
        results = []
        
        # å‡†å¤‡æ‰€æœ‰æ•°æ®
        all_prompts = []
        all_teacher_responses = []
        all_sample_ids = []
        
        for idx, row in df.iterrows():
            try:
                content = row['content']
                if isinstance(content, (list, tuple)) and len(content) > 0:
                    prompt = content[0].get('content', '') if isinstance(content[0], dict) else str(content[0])
                else:
                    prompt = str(content)
                
                teacher_response = row.get('teacher_response', '')
                
                if not prompt or not teacher_response:
                    continue
                
                all_prompts.append(prompt)
                all_teacher_responses.append(teacher_response)
                all_sample_ids.append(idx)
            except Exception as e:
                print(f"âŒ è§£ææ ·æœ¬ {idx} å¤±è´¥: {e}")
                continue
        
        print(f"ğŸ“ å‡†å¤‡å®Œæˆï¼Œå…± {len(all_prompts)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
        print(f"ğŸ”„ å¼€å§‹ç”ŸæˆStudent responses...")
        
        # ç”Ÿæˆæ‰€æœ‰Student responses
        num_students = self.test_config.get("num_student_samples", 8)
        all_student_responses = []
        
        for i, prompt in enumerate(tqdm(all_prompts, desc="ç”Ÿæˆresponses")):
            student_responses_for_prompt = []
            for _ in range(num_students):
                response = self.call_student_api(prompt, debug=False)
                if response:
                    student_responses_for_prompt.append(response)
                time.sleep(0.1)
            all_student_responses.append(student_responses_for_prompt)
        
        print(f"âœ… Student responsesç”Ÿæˆå®Œæˆ")
        print(f"ğŸ”„ å¼€å§‹Batchæ¨ç†...")
        
        # Batchæ¨ç†
        total_samples = len(all_prompts)
        for batch_start in tqdm(range(0, total_samples, batch_size), desc="Batchæ¨ç†"):
            batch_end = min(batch_start + batch_size, total_samples)
            
            batch_prompts = all_prompts[batch_start:batch_end]
            batch_teacher_responses = all_teacher_responses[batch_start:batch_end]
            batch_student_responses = all_student_responses[batch_start:batch_end]
            batch_sample_ids = all_sample_ids[batch_start:batch_end]
            
            # è·å–Teacheråˆ†æ•°ï¼ˆbatchï¼‰
            teacher_scores, teacher_lengths = self.get_critic_scores_batch(
                batch_prompts, batch_teacher_responses
            )
            
            # å¯¹æ¯ä¸ªpromptçš„å¤šä¸ªstudent responsesè¿›è¡Œè¯„åˆ†
            for i in range(len(batch_prompts)):
                prompt = batch_prompts[i]
                teacher_response = batch_teacher_responses[i]
                teacher_score = teacher_scores[i]
                teacher_length = teacher_lengths[i]
                student_responses = batch_student_responses[i]
                
                if len(student_responses) == 0:
                    continue
                
                # è·å–Studentåˆ†æ•°ï¼ˆbatchï¼‰
                student_scores, student_lengths = self.get_critic_scores_batch(
                    [prompt] * len(student_responses),
                    student_responses
                )
                
                # è®¡ç®—å‡†ç¡®ç‡
                correct_count = sum(1 for score in student_scores if score <= teacher_score)
                accuracy = correct_count / len(student_scores)
                
                # ä¿å­˜ç»“æœ
                results.append({
                    'sample_id': batch_sample_ids[i],
                    'prompt': prompt,
                    'teacher_response': teacher_response,
                    'teacher_score': teacher_score,
                    'teacher_length': teacher_length,
                    'student_responses': student_responses,
                    'student_scores': student_scores,
                    'student_lengths': student_lengths,
                    'correct_count': correct_count,
                    'accuracy': accuracy,
                    'num_students': len(student_responses),
                })
        
        return results
    
    def print_statistics(self, results_df: pd.DataFrame, use_batch_inference: bool = False):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print()
        print("=" * 100)
        print(f"ğŸ“ˆ æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯ï¼ˆ{'Batchæ¨ç†' if use_batch_inference else 'å•æ ·æœ¬æ¨ç†'} - ä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰")
        print("=" * 100)
        
        total = len(results_df)
        if total == 0:
            print("âš ï¸  æ²¡æœ‰æˆåŠŸæµ‹è¯•çš„æ ·æœ¬")
            return
        
        total_students = results_df['num_students'].sum()
        total_correct = results_df['correct_count'].sum()
        avg_accuracy = results_df['accuracy'].mean() * 100
        overall_accuracy = total_correct / total_students * 100
        
        print(f"æ€»æ ·æœ¬æ•°: {total}")
        print(f"æ€»Studentå“åº”æ•°: {total_students}")
        print(f"å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.2f}%")
        print(f"æ€»ä½“å‡†ç¡®ç‡: {total_correct}/{total_students} ({overall_accuracy:.2f}%)")
        print(f"æ³¨ï¼šæ­£ç¡®ç‡åˆ¤æ–­ä½¿ç”¨ student_score <= teacher_scoreï¼ˆåŒ…æ‹¬ç›¸ç­‰æƒ…å†µï¼‰")
        print()
        
        print("Teacheræ¨¡å‹ç»Ÿè®¡:")
        print(f"  å¹³å‡åˆ†æ•°: {results_df['teacher_score'].mean():.4f}")
        print(f"  åˆ†æ•°æ ‡å‡†å·®: {results_df['teacher_score'].std():.4f}")
        print(f"  åˆ†æ•°èŒƒå›´: [{results_df['teacher_score'].min():.4f}, {results_df['teacher_score'].max():.4f}]")
        print(f"  å¹³å‡é•¿åº¦: {results_df['teacher_length'].mean():.2f}")
        print()
        
        # Studentç»Ÿè®¡ï¼ˆéœ€è¦å±•å¼€åˆ—è¡¨ï¼‰
        all_student_scores = []
        all_student_lengths = []
        for _, row in results_df.iterrows():
            all_student_scores.extend(row['student_scores'])
            all_student_lengths.extend(row['student_lengths'])
        
        print("Studentæ¨¡å‹ç»Ÿè®¡:")
        print(f"  å¹³å‡åˆ†æ•°: {sum(all_student_scores)/len(all_student_scores):.4f}")
        print(f"  åˆ†æ•°æ ‡å‡†å·®: {pd.Series(all_student_scores).std():.4f}")
        print(f"  åˆ†æ•°èŒƒå›´: [{min(all_student_scores):.4f}, {max(all_student_scores):.4f}]")
        print(f"  å¹³å‡é•¿åº¦: {sum(all_student_lengths)/len(all_student_lengths):.2f}")
        print()
        
        # åˆ†æ•°å·®å¼‚ç»Ÿè®¡
        score_diff = results_df['teacher_score'].mean() - sum(all_student_scores)/len(all_student_scores)
        print("åˆ†æ•°å·®å¼‚:")
        print(f"  Teacher - Student: {score_diff:.4f}")
        
        if use_batch_inference:
            print()
            print("âœ… ä½¿ç”¨Batchæ¨ç†ï¼Œä¸è®­ç»ƒæ—¶çš„æ¨ç†æ–¹å¼ä¸€è‡´")
        else:
            print()
            print("âš ï¸  ä½¿ç”¨å•æ ·æœ¬æ¨ç†ï¼Œä¸è®­ç»ƒæ—¶çš„æ¨ç†æ–¹å¼ä¸åŒ")
            print("   å»ºè®®ä½¿ç”¨ --use_batch_inference å‚æ•°è¿›è¡Œæµ‹è¯•")
        
        print()
        print("=" * 100)


def main():
    parser = argparse.ArgumentParser(description="æµ‹è¯•Criticæ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰")
    parser.add_argument("--data_path", type=str, default=None)
    parser.add_argument("--num_samples", type=int, default=None)
    parser.add_argument("--output_path", type=str, default=None)
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œæ˜¾ç¤ºè¯¦ç»†çš„å­—ç¬¦å·®å¼‚")
    parser.add_argument("--use_batch_inference", action="store_true", default=True,
                       help="ä½¿ç”¨batchæ¨ç†ï¼ˆæ¨èï¼Œä¸è®­ç»ƒä¸€è‡´ï¼‰")
    parser.add_argument("--batch_size", type=int, default=32,
                       help="Batchæ¨ç†çš„æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--no_batch", action="store_true",
                       help="ç¦ç”¨batchæ¨ç†ï¼Œä½¿ç”¨å•æ ·æœ¬æ¨ç†")
    
    args = parser.parse_args()
    
    data_path = args.data_path or TEST_CONFIG["data_path"]
    num_samples = args.num_samples if args.num_samples is not None else TEST_CONFIG["num_samples"]
    output_path = args.output_path
    debug = args.debug
    use_batch_inference = not args.no_batch  # é»˜è®¤ä½¿ç”¨batchæ¨ç†
    batch_size = args.batch_size
    
    print("=" * 100)
    print("ğŸ“‹ æµ‹è¯•é…ç½®ä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆ - ä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰")
    print("=" * 100)
    print(f"æ•°æ®é›†è·¯å¾„: {data_path}")
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {num_samples if num_samples else 'å…¨éƒ¨'}")
    print(f"æ¯ä¸ªpromptçš„Studentæ ·æœ¬æ•°: {TEST_CONFIG['num_student_samples']}")
    print(f"Criticæ¨¡å‹: {API_CONFIGS['critic_model']['model_path']}")
    print(f"Studentæ¨¡å‹: {API_CONFIGS['student_model']['url']}")
    print(f"è°ƒè¯•æ¨¡å¼: {'å¼€å¯' if debug else 'å…³é—­'}")
    print(f"æ¨ç†æ¨¡å¼: {'Batchæ¨ç†' if use_batch_inference else 'å•æ ·æœ¬æ¨ç†'}")
    if use_batch_inference:
        print(f"Batchå¤§å°: {batch_size}")
    print()
    print("åˆ†æ•°è®¡ç®—æ–¹å¼: å¹³å‡åˆ†æ•°ï¼ˆæ’é™¤EOS tokenï¼‰ï¼Œä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´")
    if use_batch_inference:
        print("âœ… ä½¿ç”¨Batchæ¨ç†ï¼Œä¸è®­ç»ƒæ—¶çš„æ¨ç†æ–¹å¼ä¸€è‡´ï¼ˆæ¨èï¼‰")
    else:
        print("âš ï¸  ä½¿ç”¨å•æ ·æœ¬æ¨ç†ï¼Œä¸è®­ç»ƒæ—¶çš„æ¨ç†æ–¹å¼ä¸åŒ")
    print("=" * 100)
    print()
    
    tester = CriticTester(
        critic_config=API_CONFIGS["critic_model"],
        student_config=API_CONFIGS["student_model"],
        test_config=TEST_CONFIG
    )
    
    # ä¼ é€’debugå‚æ•°
    TEST_CONFIG['debug'] = debug
    
    tester.test_dataset(
        data_path, 
        num_samples, 
        output_path,
        use_batch_inference=use_batch_inference,
        batch_size=batch_size
    )
    print("âœ… æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()
