"""
åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜å®é™…ä½¿ç”¨çš„responses

ç”¨é€”ï¼š
1. ä¿å­˜è®­ç»ƒæ—¶å®é™…ä½¿ç”¨çš„studentå’Œteacher responses
2. ç”¨äºåç»­éªŒè¯Criticçš„è¯„åˆ†ä¸€è‡´æ€§
3. åˆ†æè®­ç»ƒæ—¶å’Œæµ‹è¯•æ—¶responsesçš„å·®å¼‚

ä½¿ç”¨æ–¹æ³•ï¼š
åœ¨ dp_critic.py çš„ update_critic æ–¹æ³•ä¸­è°ƒç”¨æ­¤å‡½æ•°
"""

import torch
import os
from datetime import datetime


def save_training_responses(
    step: int,
    model_inputs: dict,
    teacher_score: torch.Tensor,
    student_score: torch.Tensor,
    save_dir: str = "./saved_responses",
    save_interval: int = 50,
    num_samples: int = 10
):
    """
    ä¿å­˜è®­ç»ƒæ—¶çš„responseså’Œåˆ†æ•°
    
    Args:
        step: å½“å‰è®­ç»ƒæ­¥æ•°
        model_inputs: åŒ…å«input_ids, responses, teacher_responseç­‰çš„å­—å…¸
        teacher_score: Teacherçš„åˆ†æ•°
        student_score: Studentçš„åˆ†æ•°
        save_dir: ä¿å­˜ç›®å½•
        save_interval: ä¿å­˜é—´éš”ï¼ˆæ¯Næ­¥ä¿å­˜ä¸€æ¬¡ï¼‰
        num_samples: æ¯æ¬¡ä¿å­˜çš„æ ·æœ¬æ•°é‡
    """
    
    # åªåœ¨æŒ‡å®šçš„æ­¥æ•°ä¿å­˜
    if step % save_interval != 0:
        return
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(save_dir, exist_ok=True)
    
    # å‡†å¤‡ä¿å­˜çš„æ•°æ®
    batch_size = min(num_samples, teacher_score.size(0))
    
    save_data = {
        'step': step,
        'timestamp': datetime.now().isoformat(),
        
        # è¾“å…¥æ•°æ®
        'input_ids': model_inputs['input_ids'][:batch_size].cpu(),
        'attention_mask': model_inputs['attention_mask'][:batch_size].cpu(),
        'position_ids': model_inputs.get('position_ids', None),
        
        # Responses
        'student_responses': model_inputs['responses'][:batch_size].cpu(),
        'teacher_responses': model_inputs['teacher_response'][:batch_size].cpu(),
        
        # Masks
        'student_response_mask': model_inputs['attention_mask'][:batch_size, -model_inputs['responses'].size(1):].cpu(),
        'teacher_response_mask': model_inputs['teacher_attention_mask'][:batch_size, -model_inputs['teacher_response'].size(1):].cpu(),
        
        # åˆ†æ•°
        'teacher_scores': teacher_score[:batch_size].cpu(),
        'student_scores': student_score[:batch_size].cpu(),
        
        # ç»Ÿè®¡ä¿¡æ¯
        'batch_d_acc': (teacher_score > student_score).float().mean().item(),
        'batch_score_diff': (teacher_score - student_score).mean().item(),
    }
    
    # ä¿å­˜
    save_path = os.path.join(save_dir, f"responses_step_{step}.pt")
    torch.save(save_data, save_path)
    
    print(f"âœ… Saved training responses to {save_path}")
    print(f"   Samples: {batch_size}, d_acc: {save_data['batch_d_acc']:.2%}, score_diff: {save_data['batch_score_diff']:.4f}")


# ============================================================================
# åœ¨ dp_critic.py ä¸­çš„ä½¿ç”¨ç¤ºä¾‹
# ============================================================================

"""
åœ¨ verl/verl/workers/critic/dp_critic.py çš„ update_critic æ–¹æ³•ä¸­æ·»åŠ ï¼š

# åœ¨è®¡ç®—å®Œteacher_scoreå’Œstudent_scoreä¹‹å
if use_discriminator:
    # ... ç°æœ‰çš„ä»£ç  ...
    
    # è®¡ç®—d_acc
    d_acc = (teacher_score > student_score).float().mean()
    
    # ğŸ”§ æ·»åŠ ï¼šä¿å­˜è®­ç»ƒæ—¶çš„responses
    from tools.save_training_responses import save_training_responses
    
    # æ¯50æ­¥ä¿å­˜ä¸€æ¬¡ï¼Œä¿å­˜10ä¸ªæ ·æœ¬
    save_training_responses(
        step=self._update_step,
        model_inputs=model_inputs,
        teacher_score=teacher_score,
        student_score=student_score,
        save_dir="/home/jovyan/JQ/gad_gspo_B300/saved_responses",
        save_interval=50,
        num_samples=10
    )
    
    # ... ç»§ç»­ç°æœ‰çš„ä»£ç  ...
"""


# ============================================================================
# éªŒè¯è„šæœ¬ï¼šä½¿ç”¨ä¿å­˜çš„responsesæµ‹è¯•Critic
# ============================================================================

def test_critic_with_saved_responses(
    critic_model,
    tokenizer,
    saved_responses_path: str
):
    """
    ä½¿ç”¨ä¿å­˜çš„responsesæµ‹è¯•Critic
    
    Args:
        critic_model: Criticæ¨¡å‹
        tokenizer: Tokenizer
        saved_responses_path: ä¿å­˜çš„responsesæ–‡ä»¶è·¯å¾„
    
    Returns:
        dict: åŒ…å«æµ‹è¯•ç»“æœçš„å­—å…¸
    """
    import torch
    
    print("="*80)
    print(f"ä½¿ç”¨ä¿å­˜çš„responsesæµ‹è¯•Critic")
    print(f"æ–‡ä»¶: {saved_responses_path}")
    print("="*80)
    
    # åŠ è½½ä¿å­˜çš„æ•°æ®
    data = torch.load(saved_responses_path)
    
    print(f"\nåŠ è½½çš„æ•°æ®:")
    print(f"  Step: {data['step']}")
    print(f"  Timestamp: {data['timestamp']}")
    print(f"  Samples: {data['teacher_scores'].size(0)}")
    print(f"  è®­ç»ƒæ—¶d_acc: {data['batch_d_acc']:.2%}")
    print(f"  è®­ç»ƒæ—¶score_diff: {data['batch_score_diff']:.4f}")
    
    # å°†æ•°æ®ç§»åˆ°GPU
    device = next(critic_model.pretrained_model.parameters()).device
    input_ids = data['input_ids'].to(device)
    attention_mask = data['attention_mask'].to(device)
    student_responses = data['student_responses'].to(device)
    teacher_responses = data['teacher_responses'].to(device)
    
    # é‡æ–°è®¡ç®—åˆ†æ•°
    print(f"\né‡æ–°è®¡ç®—åˆ†æ•°...")
    
    teacher_scores_new = []
    student_scores_new = []
    
    critic_model.eval()
    with torch.no_grad():
        for i in range(input_ids.size(0)):
            # è®¡ç®—teacheråˆ†æ•°
            teacher_input_ids = torch.cat([
                input_ids[i:i+1, :-teacher_responses.size(1)],
                teacher_responses[i:i+1]
            ], dim=1)
            
            teacher_outputs = critic_model(
                input_ids=teacher_input_ids,
                attention_mask=attention_mask[i:i+1],
                use_cache=False
            )
            teacher_values = teacher_outputs[2][:, -teacher_responses.size(1):]
            if teacher_values.dim() == 3:
                teacher_values = teacher_values.squeeze(-1)
            
            # æ’é™¤EOS token
            teacher_response_mask = data['teacher_response_mask'][i:i+1].to(device)
            teacher_response_ids = teacher_responses[i:i+1]
            eos_token_id = tokenizer.eos_token_id
            is_eos = (teacher_response_ids == eos_token_id)
            teacher_mask_no_eos = teacher_response_mask & (~is_eos)
            
            teacher_score = (teacher_values * teacher_mask_no_eos).sum() / teacher_mask_no_eos.sum().clamp(min=1)
            teacher_scores_new.append(teacher_score.item())
            
            # è®¡ç®—studentåˆ†æ•°ï¼ˆç±»ä¼¼çš„è¿‡ç¨‹ï¼‰
            student_input_ids = torch.cat([
                input_ids[i:i+1, :-student_responses.size(1)],
                student_responses[i:i+1]
            ], dim=1)
            
            student_outputs = critic_model(
                input_ids=student_input_ids,
                attention_mask=attention_mask[i:i+1],
                use_cache=False
            )
            student_values = student_outputs[2][:, -student_responses.size(1):]
            if student_values.dim() == 3:
                student_values = student_values.squeeze(-1)
            
            student_response_mask = data['student_response_mask'][i:i+1].to(device)
            student_response_ids = student_responses[i:i+1]
            is_eos = (student_response_ids == eos_token_id)
            student_mask_no_eos = student_response_mask & (~is_eos)
            
            student_score = (student_values * student_mask_no_eos).sum() / student_mask_no_eos.sum().clamp(min=1)
            student_scores_new.append(student_score.item())
    
    # è®¡ç®—æ–°çš„d_acc
    teacher_scores_new = torch.tensor(teacher_scores_new)
    student_scores_new = torch.tensor(student_scores_new)
    
    d_acc_new = (teacher_scores_new > student_scores_new).float().mean().item()
    score_diff_new = (teacher_scores_new - student_scores_new).mean().item()
    
    # å¯¹æ¯”ç»“æœ
    print(f"\n" + "="*80)
    print("å¯¹æ¯”ç»“æœ")
    print("="*80)
    
    print(f"\nè®­ç»ƒæ—¶è®°å½•çš„åˆ†æ•°:")
    print(f"  Teacherå¹³å‡åˆ†: {data['teacher_scores'].mean().item():.4f}")
    print(f"  Studentå¹³å‡åˆ†: {data['student_scores'].mean().item():.4f}")
    print(f"  d_acc: {data['batch_d_acc']:.2%}")
    print(f"  score_diff: {data['batch_score_diff']:.4f}")
    
    print(f"\né‡æ–°è®¡ç®—çš„åˆ†æ•°:")
    print(f"  Teacherå¹³å‡åˆ†: {teacher_scores_new.mean().item():.4f}")
    print(f"  Studentå¹³å‡åˆ†: {student_scores_new.mean().item():.4f}")
    print(f"  d_acc: {d_acc_new:.2%}")
    print(f"  score_diff: {score_diff_new:.4f}")
    
    print(f"\nå·®å¼‚:")
    teacher_diff = abs(data['teacher_scores'].mean().item() - teacher_scores_new.mean().item())
    student_diff = abs(data['student_scores'].mean().item() - student_scores_new.mean().item())
    d_acc_diff = abs(data['batch_d_acc'] - d_acc_new)
    
    print(f"  Teacheråˆ†æ•°å·®å¼‚: {teacher_diff:.4f}")
    print(f"  Studentåˆ†æ•°å·®å¼‚: {student_diff:.4f}")
    print(f"  d_accå·®å¼‚: {d_acc_diff:.2%}")
    
    # è¯Šæ–­
    print(f"\n" + "="*80)
    print("è¯Šæ–­")
    print("="*80)
    
    if teacher_diff < 0.1 and student_diff < 0.1 and d_acc_diff < 0.05:
        print("âœ… åˆ†æ•°ä¸€è‡´æ€§è‰¯å¥½ï¼")
        print("   Criticçš„è¯„åˆ†æ˜¯ç¨³å®šçš„")
        print("   é—®é¢˜å¯èƒ½åœ¨äºæµ‹è¯•æ—¶ä½¿ç”¨äº†ä¸åŒçš„responses")
    else:
        print("âš ï¸  åˆ†æ•°ä¸ä¸€è‡´ï¼")
        print("   å¯èƒ½çš„åŸå› :")
        print("   1. Criticæ¨¡å‹åŠ è½½æœ‰é—®é¢˜")
        print("   2. æ¨ç†ä»£ç ä¸è®­ç»ƒä»£ç ä¸ä¸€è‡´")
        print("   3. æ•°å€¼ç²¾åº¦é—®é¢˜")
    
    return {
        'training_d_acc': data['batch_d_acc'],
        'test_d_acc': d_acc_new,
        'training_teacher_mean': data['teacher_scores'].mean().item(),
        'test_teacher_mean': teacher_scores_new.mean().item(),
        'training_student_mean': data['student_scores'].mean().item(),
        'test_student_mean': student_scores_new.mean().item(),
    }


if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šæµ‹è¯•ä¿å­˜çš„responses
    from transformers import AutoTokenizer, AutoModelForCausalLM
    from trl import AutoModelForCausalLMWithValueHead
    
    critic_path = "/home/jovyan/JQ/gad_gspo_B300/models/opd-v9-1-29-fsdp2/global_step_500/critic_merged"
    saved_responses_path = "/home/jovyan/JQ/gad_gspo_B300/saved_responses/responses_step_300.pt"
    
    # åŠ è½½æ¨¡å‹
    print("åŠ è½½Criticæ¨¡å‹...")
    tokenizer = AutoTokenizer.from_pretrained(critic_path, trust_remote_code=True)
    base_model = AutoModelForCausalLM.from_pretrained(
        critic_path,
        trust_remote_code=True,
        torch_dtype=torch.float16,
        device_map="cuda:0"
    )
    critic_model = AutoModelForCausalLMWithValueHead.from_pretrained(base_model)
    
    # æµ‹è¯•
    results = test_critic_with_saved_responses(
        critic_model,
        tokenizer,
        saved_responses_path
    )
