"""
æµ‹è¯•è®­ç»ƒåçš„Criticå’ŒStudentæ¨¡å‹ - ç®€åŒ–ç‰ˆï¼ˆä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´ï¼‰

ä¸è®­ç»ƒä»£ç çš„ä¸€è‡´æ€§ï¼š
- ä½¿ç”¨å¹³å‡åˆ†æ•°ï¼ˆæ’é™¤EOS tokenï¼‰ï¼Œä¸è®­ç»ƒä»£ç ä¸€è‡´
- ä¸ä½¿ç”¨æ··åˆåˆ†æ•°å’Œbatch normalization
- æ¯ä¸ªpromptç”Ÿæˆ8ä¸ªstudent responsesè¿›è¡Œè¯„ä¼°
"""
import requests
import pandas as pd
import argparse
from typing import Dict, Optional, List
from datetime import datetime
from tqdm import tqdm
import time
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


# ==================== é…ç½®åŒºåŸŸ ====================

# æµ‹è¯•å‚æ•°é…ç½®
TEST_CONFIG = {
    "data_path": "/home/jovyan/JQ/gad_gspo_B300/data/trainning_dataset/subject_1-29/merged/merge-1-29.parquet",
    "num_samples": 100,
    "random_sample": True,
    "random_seed": 42,
    "output_dir": "/home/jovyan/JQ/gad_gspo_B300/outputs",
    "output_filename": None,
    
    # å¤šæ ·æœ¬ç”Ÿæˆé…ç½®
    "num_student_samples": 8,  # æ¯ä¸ªpromptç”Ÿæˆ8ä¸ªstudent response
    "student_temperature": 0.6,  # æé«˜temperatureå¢åŠ å¤šæ ·æ€§
}

# æ¨¡å‹é…ç½®
API_CONFIGS = {
    "critic_model": {
        "name": "critic-model",
        "type": "local",
        "model_path": "/home/jovyan/JQ/gad_gspo_B300/models/opd-v9-1-29-fsdp2/global_step_500/critic_merged",
        "device": "cuda:4",
        "force_trl": True,
    },
    "student_model": {
        "name": "student-model",
        "type": "api",
        "url": "http://10.72.1.39:8008/v1/chat/completions",
        "api_key": "sk-xxxx",
        "model_name": "opd-v9-500",
        "temperature": 0.6,
        "repetition_penalty": 1.2
    }
}
# ================================================



class CriticTester:
    """ç®€åŒ–ç‰ˆCriticæµ‹è¯•å™¨ - ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´"""
    
    def __init__(self, critic_config: Dict, student_config: Dict, test_config: Dict):
        self.critic_config = critic_config
        self.student_config = student_config
        self.test_config = test_config
        
        # åŠ è½½Criticæ¨¡å‹
        self.critic_model = None
        self.critic_tokenizer = None
        if critic_config.get("type") == "local":
            print(f"ğŸ”„ åŠ è½½æœ¬åœ°Criticæ¨¡å‹: {critic_config['model_path']}")
            self.load_critic_model(critic_config)
            print(f"âœ… Criticæ¨¡å‹åŠ è½½å®Œæˆ")
    
    def load_critic_model(self, config: Dict):
        """åŠ è½½Criticæ¨¡å‹"""
        from trl import AutoModelForCausalLMWithValueHead
        from transformers import AutoModelForCausalLM
        
        device = config.get("device", "cuda:0")
        model_path = config["model_path"]
        
        print(f"ğŸ”„ åŠ è½½tokenizer...")
        self.critic_tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        print(f"ğŸ”„ åŠ è½½æ¨¡å‹...")
        base_model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            device_map=device
        )
        
        self.critic_model = AutoModelForCausalLMWithValueHead.from_pretrained(base_model)
        self.critic_model.eval()
    
    def call_student_api(self, prompt: str, debug: bool = False) -> str:
        """è°ƒç”¨Student APIç”Ÿæˆresponse"""
        try:
            config = self.student_config
            messages = [{"role": "user", "content": prompt}]
            
            payload = {
                "model": config["model_name"],
                "messages": messages,
                "max_tokens": 512,
                "temperature": config.get("temperature", 0.8),
                "top_p": 0.9,
                "repetition_penalty": config.get("repetition_penalty", 1.2),
            }
            
            headers = {
                "Authorization": f"Bearer {config['api_key']}",
                "Content-Type": "application/json"
            }
            
            response = requests.post(config["url"], json=payload, headers=headers, timeout=60)
            response.raise_for_status()
            
            result = response.json()
            raw_content = result['choices'][0]['message']['content']
            
            # åªç§»é™¤é¦–å°¾ç©ºç™½ï¼Œä¿æŒåŸå§‹è¾“å‡º
            cleaned_content = raw_content.strip()
            
            # è°ƒè¯•æ¨¡å¼ï¼šæ˜¾ç¤ºåŸå§‹è¾“å‡º
            if debug and raw_content != cleaned_content:
                print(f"  âš ï¸  æ£€æµ‹åˆ°é¦–å°¾ç©ºç™½å­—ç¬¦:")
                print(f"    åŸå§‹: {repr(raw_content)}")
                print(f"    æ¸…ç†å: {repr(cleaned_content)}")
            
            return cleaned_content
        except Exception as e:
            print(f"âŒ Student APIè°ƒç”¨å¤±è´¥: {e}")
            return ""
    
    def get_critic_score(self, prompt: str, response: str) -> tuple:
        """
        è·å–Criticåˆ†æ•°ï¼ˆä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´ï¼‰
        
        Returns:
            (score_avg, length): å¹³å‡åˆ†æ•°å’Œresponseé•¿åº¦ï¼ˆæ’é™¤EOSï¼‰
        """
        try:
            messages = [
                {"role": "user", "content": prompt},
                {"role": "assistant", "content": response}
            ]
            
            input_text = self.critic_tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=False
            )
            
            inputs = self.critic_tokenizer(
                input_text, return_tensors="pt", truncation=True, max_length=2048
            )
            
            device = next(self.critic_model.pretrained_model.parameters()).device
            inputs = inputs.to(device)
            
            # å‰å‘ä¼ æ’­
            with torch.no_grad():
                outputs = self.critic_model(**inputs, use_cache=False)
                
                # è·å–responseé•¿åº¦
                response_tokens = self.critic_tokenizer(
                    response, add_special_tokens=False, return_tensors="pt"
                )
                response_length = response_tokens['input_ids'].size(1)
                
                # æå–responseéƒ¨åˆ†çš„values
                values = outputs[2][:, -response_length:]
                if values.dim() == 3:
                    values = values.squeeze(-1)
                
                # è·å–maskå¹¶æ’é™¤EOS token
                attention_mask = inputs['attention_mask']
                response_mask = attention_mask[:, -response_length:]
                response_ids = inputs['input_ids'][:, -response_length:]
                
                eos_token_id = self.critic_tokenizer.eos_token_id
                is_eos = (response_ids == eos_token_id)
                response_mask_no_eos = response_mask & (~is_eos)
                
                # è®¡ç®—å¹³å‡åˆ†æ•°ï¼ˆä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰
                values_sum = torch.sum(values * response_mask_no_eos, dim=-1)
                length = response_mask_no_eos.sum(dim=-1)
                score_avg = (values_sum / length.clamp(min=1)).item()
                length = length.item()
                
                return score_avg, length
        except Exception as e:
            print(f"âŒ Criticè¯„åˆ†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0.0, 0

    
    def test_single_prompt(self, prompt: str, teacher_response: str, debug: bool = False) -> Dict:
        """æµ‹è¯•å•ä¸ªpromptï¼ˆç”Ÿæˆå¤šä¸ªstudent responsesï¼‰
        
        æ³¨æ„ï¼šæ­£ç¡®ç‡åˆ¤æ–­ä½¿ç”¨ student_score <= teacher_scoreï¼ˆåŒ…æ‹¬ç›¸ç­‰æƒ…å†µï¼‰
        """
        print("ğŸ“ Prompt:")
        print(f"  {prompt}")
        print()
        
        num_students = self.test_config.get("num_student_samples", 8)
        
        # ç”Ÿæˆå¤šä¸ªStudentå“åº”
        print(f"ğŸ”„ ç”Ÿæˆ {num_students} ä¸ªStudentå“åº”...")
        student_responses = []
        for i in range(num_students):
            response = self.call_student_api(prompt, debug=debug)
            if response:
                student_responses.append(response)
                print(f"  âœ“ Student #{i+1}: {len(response)} chars")
        print()
        
        if len(student_responses) == 0:
            print("âŒ æ²¡æœ‰æˆåŠŸç”Ÿæˆstudent responses")
            return None
        
        # è·å–Teacheråˆ†æ•°
        teacher_score, teacher_length = self.get_critic_score(prompt, teacher_response)
        
        # è·å–æ‰€æœ‰Studentåˆ†æ•°
        student_scores = []
        student_lengths = []
        for response in student_responses:
            score, length = self.get_critic_score(prompt, response)
            student_scores.append(score)
            student_lengths.append(length)
        
        # æ‰“å°ç»“æœ
        print("ğŸ‘¨â€ğŸ« Teacher Response:")
        print("-" * 100)
        print(f"  åˆ†æ•°(avg): {teacher_score:7.4f}")
        print(f"  Length: {int(teacher_length):3d}")
        print(f"  Text: {teacher_response}")
        if debug:
            print(f"  Repr: {repr(teacher_response)}")
        print()
        
        print(f"ğŸ“ Student Responses ({len(student_responses)} ä¸ª):")
        print("-" * 100)
        for i, (response, score, length) in enumerate(zip(student_responses, student_scores, student_lengths)):
            diff = score - teacher_score
            # ä¿®æ”¹åˆ¤æ–­é€»è¾‘ï¼šstudent_score <= teacher_score ä¸ºæ­£ç¡®ï¼ˆåŒ…æ‹¬ç›¸ç­‰æƒ…å†µï¼‰
            is_correct = score <= teacher_score
            status = 'âœ…æ­£ç¡®' if is_correct else 'âŒé”™è¯¯'
            
            # ç‰¹æ®Šæ ‡è®°ç›¸ç­‰æƒ…å†µ
            if abs(diff) < 1e-6:  # æµ®ç‚¹æ•°ç›¸ç­‰åˆ¤æ–­
                status += ' (ç›¸ç­‰)'
            
            print(f"\n  Student #{i+1}:")
            print(f"    åˆ†æ•°(avg): {score:7.4f}")
            print(f"    Length: {int(length):3d}")
            print(f"    ä¸Teacheråˆ†å·®: {diff:7.4f} ({status})")
            print(f"    Text: {response}")
            
            # è°ƒè¯•æ¨¡å¼ï¼šæ˜¾ç¤ºè¯¦ç»†çš„å­—ç¬¦å·®å¼‚
            if debug and response != teacher_response:
                print(f"    Repr: {repr(response)}")
                # æ£€æŸ¥æ˜¯å¦åªæ˜¯ç©ºç™½å­—ç¬¦å·®å¼‚
                if response.replace(' ', '').replace('\n', '').replace('\t', '') == \
                   teacher_response.replace(' ', '').replace('\n', '').replace('\t', ''):
                    print(f"    âš ï¸  ä¸Teacherä»…ç©ºç™½å­—ç¬¦ä¸åŒ")
        
        # ç»Ÿè®¡ï¼ˆä¿®æ”¹åˆ¤æ–­é€»è¾‘ï¼šåŒ…æ‹¬ç›¸ç­‰æƒ…å†µï¼‰
        correct_count = sum(1 for score in student_scores if score <= teacher_score)
        accuracy = correct_count / len(student_scores)
        
        print()
        print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  Teacheråˆ†æ•°: {teacher_score:7.4f}")
        print(f"  Studentå¹³å‡åˆ†æ•°: {sum(student_scores)/len(student_scores):7.4f}")
        print(f"  å‡†ç¡®ç‡: {correct_count}/{len(student_scores)} ({accuracy*100:.1f}%)")
        print(f"  æ³¨ï¼šæ­£ç¡®ç‡åˆ¤æ–­ä½¿ç”¨ student_score <= teacher_scoreï¼ˆåŒ…æ‹¬ç›¸ç­‰æƒ…å†µï¼‰")
        print()
        print("=" * 100)
        print()
        
        return {
            'prompt': prompt,
            'teacher_response': teacher_response,
            'teacher_score': teacher_score,
            'teacher_length': teacher_length,
            'student_responses': student_responses,
            'student_scores': student_scores,
            'student_lengths': student_lengths,
            'correct_count': correct_count,
            'accuracy': accuracy,
            'num_students': len(student_responses),
        }
    
    def test_dataset(self, data_path: str, num_samples: Optional[int] = None,
                    output_path: Optional[str] = None) -> pd.DataFrame:
        """æµ‹è¯•æ•°æ®é›†"""
        # è¯»å–æ•°æ®
        if data_path.endswith('.parquet'):
            df = pd.read_parquet(data_path)
        else:
            raise ValueError("ä»…æ”¯æŒ.parquetæ ¼å¼")
        
        # é‡‡æ ·
        if num_samples:
            if self.test_config.get("random_sample"):
                df = df.sample(n=min(num_samples, len(df)), 
                             random_state=self.test_config.get("random_seed", 42))
            else:
                df = df.head(num_samples)
        
        print(f"ğŸ“Š æµ‹è¯• {len(df)} ä¸ªæ ·æœ¬...")
        print()
        
        results = []
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="æµ‹è¯•è¿›åº¦"):
            try:
                # è§£ææ•°æ®
                content = row['content']
                if isinstance(content, (list, tuple)) and len(content) > 0:
                    prompt = content[0].get('content', '') if isinstance(content[0], dict) else str(content[0])
                else:
                    prompt = str(content)
                
                teacher_response = row.get('teacher_response', '')
                
                if not prompt or not teacher_response:
                    continue
                
                result = self.test_single_prompt(prompt, teacher_response, 
                                                debug=self.test_config.get('debug', False))
                if result:
                    result['sample_id'] = idx
                    results.append(result)
                
                time.sleep(0.5)
            except Exception as e:
                print(f"âŒ æµ‹è¯•æ ·æœ¬ {idx} å¤±è´¥: {e}")
                continue
        
        # è½¬æ¢ä¸ºDataFrame
        results_df = pd.DataFrame(results)
        
        # æ‰“å°ç»Ÿè®¡
        self.print_statistics(results_df)
        
        # ä¿å­˜
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"{self.test_config['output_dir']}/test_results_simplified_{timestamp}.xlsx"
        
        if not output_path.endswith('.xlsx'):
            output_path += '.xlsx'
        
        import os
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        results_df.to_excel(output_path, index=False)
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
        return results_df
    
    def print_statistics(self, results_df: pd.DataFrame):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print()
        print("=" * 100)
        print("ğŸ“ˆ æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆ - ä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰")
        print("=" * 100)
        
        total = len(results_df)
        if total == 0:
            print("âš ï¸  æ²¡æœ‰æˆåŠŸæµ‹è¯•çš„æ ·æœ¬")
            return
        
        total_students = results_df['num_students'].sum()
        total_correct = results_df['correct_count'].sum()
        avg_accuracy = results_df['accuracy'].mean() * 100
        overall_accuracy = total_correct / total_students * 100
        
        print(f"æ€»æ ·æœ¬æ•°: {total}")
        print(f"æ€»Studentå“åº”æ•°: {total_students}")
        print(f"å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.2f}%")
        print(f"æ€»ä½“å‡†ç¡®ç‡: {total_correct}/{total_students} ({overall_accuracy:.2f}%)")
        print(f"æ³¨ï¼šæ­£ç¡®ç‡åˆ¤æ–­ä½¿ç”¨ student_score <= teacher_scoreï¼ˆåŒ…æ‹¬ç›¸ç­‰æƒ…å†µï¼‰")
        print()
        
        print("Teacheræ¨¡å‹ç»Ÿè®¡:")
        print(f"  å¹³å‡åˆ†æ•°: {results_df['teacher_score'].mean():.4f}")
        print(f"  åˆ†æ•°æ ‡å‡†å·®: {results_df['teacher_score'].std():.4f}")
        print(f"  å¹³å‡é•¿åº¦: {results_df['teacher_length'].mean():.2f}")
        print()
        
        # Studentç»Ÿè®¡ï¼ˆéœ€è¦å±•å¼€åˆ—è¡¨ï¼‰
        all_student_scores = []
        all_student_lengths = []
        for _, row in results_df.iterrows():
            all_student_scores.extend(row['student_scores'])
            all_student_lengths.extend(row['student_lengths'])
        
        print("Studentæ¨¡å‹ç»Ÿè®¡:")
        print(f"  å¹³å‡åˆ†æ•°: {sum(all_student_scores)/len(all_student_scores):.4f}")
        print(f"  åˆ†æ•°æ ‡å‡†å·®: {pd.Series(all_student_scores).std():.4f}")
        print(f"  å¹³å‡é•¿åº¦: {sum(all_student_lengths)/len(all_student_lengths):.2f}")
        print()
        
        print("=" * 100)


def main():
    parser = argparse.ArgumentParser(description="æµ‹è¯•Criticæ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰")
    parser.add_argument("--data_path", type=str, default=None)
    parser.add_argument("--num_samples", type=int, default=None)
    parser.add_argument("--output_path", type=str, default=None)
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼Œæ˜¾ç¤ºè¯¦ç»†çš„å­—ç¬¦å·®å¼‚")
    
    args = parser.parse_args()
    
    data_path = args.data_path or TEST_CONFIG["data_path"]
    num_samples = args.num_samples if args.num_samples is not None else TEST_CONFIG["num_samples"]
    output_path = args.output_path
    debug = args.debug
    
    print("=" * 100)
    print("ğŸ“‹ æµ‹è¯•é…ç½®ä¿¡æ¯ï¼ˆç®€åŒ–ç‰ˆ - ä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰")
    print("=" * 100)
    print(f"æ•°æ®é›†è·¯å¾„: {data_path}")
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {num_samples if num_samples else 'å…¨éƒ¨'}")
    print(f"æ¯ä¸ªpromptçš„Studentæ ·æœ¬æ•°: {TEST_CONFIG['num_student_samples']}")
    print(f"Criticæ¨¡å‹: {API_CONFIGS['critic_model']['model_path']}")
    print(f"Studentæ¨¡å‹: {API_CONFIGS['student_model']['url']}")
    print(f"è°ƒè¯•æ¨¡å¼: {'å¼€å¯' if debug else 'å…³é—­'}")
    print()
    print("åˆ†æ•°è®¡ç®—æ–¹å¼: å¹³å‡åˆ†æ•°ï¼ˆæ’é™¤EOS tokenï¼‰ï¼Œä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´")
    print("=" * 100)
    print()
    
    tester = CriticTester(
        critic_config=API_CONFIGS["critic_model"],
        student_config=API_CONFIGS["student_model"],
        test_config=TEST_CONFIG
    )
    
    # ä¼ é€’debugå‚æ•°
    TEST_CONFIG['debug'] = debug
    
    tester.test_dataset(data_path, num_samples, output_path)
    print("âœ… æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()
