"""
æµ‹è¯•Criticæ¨¡å‹ - å®Œå…¨æ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„batchç»“æ„ï¼ˆç‹¬ç«‹è„šæœ¬ï¼‰

å…³é”®æ”¹è¿›ï¼š
1. ä½¿ç”¨trainæ¨¡å¼ï¼ˆè€Œéevalæ¨¡å¼ï¼‰
2. ä½¿ç”¨ä¸è®­ç»ƒå®Œå…¨ç›¸åŒçš„batchç»“æ„ï¼ˆteacherså’Œstudentsåœ¨åŒä¸€ä¸ªbatchä¸­ï¼‰
3. æ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„n_resp_per_prompt=4é…ç½®
"""

import os
import sys

# ğŸ”§ å…³é”®ä¿®å¤ï¼šåœ¨å¯¼å…¥torchä¹‹å‰è®¾ç½®CUDA_VISIBLE_DEVICES
# è¿™å¿…é¡»åœ¨ä»»ä½•å¯¼å…¥torchçš„ä»£ç ä¹‹å‰æ‰§è¡Œ
if '--gpu_ids' in sys.argv:
    gpu_ids_idx = sys.argv.index('--gpu_ids') + 1
    if gpu_ids_idx < len(sys.argv):
        gpu_ids = sys.argv[gpu_ids_idx]
        os.environ["CUDA_VISIBLE_DEVICES"] = gpu_ids
        print(f"âš¡ è®¾ç½®CUDA_VISIBLE_DEVICES={gpu_ids}")
elif '--use_multi_gpu' in sys.argv or len(sys.argv) == 1:
    # é»˜è®¤ä½¿ç”¨GPU 4,5
    os.environ["CUDA_VISIBLE_DEVICES"] = "4,5"
    print(f"âš¡ é»˜è®¤è®¾ç½®CUDA_VISIBLE_DEVICES=4,5")

import requests
import pandas as pd
import argparse
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from trl import AutoModelForCausalLMWithValueHead
from tqdm import tqdm
import time


# ==================== é…ç½®åŒºåŸŸ ====================

# æ¨¡å‹é…ç½®
CRITIC_CONFIG = {
    "model_path": "/home/jovyan/JQ/gad_gspo_B300/models/2-3-warmup-v10-fsdp2/global_step_310/critic_merged",
    "use_multi_gpu": True,  # å¯ç”¨å¤šGPU
}

STUDENT_CONFIG = {
    "url": "http://10.72.1.39:8009/v1/chat/completions",
    "api_key": "sk-xxxx",
    "model_name": "SFT56",
    "temperature": 0.6,  # ä¸è®­ç»ƒæ—¶ä¸€è‡´
    "repetition_penalty": 1.2
}

# æ•°æ®é…ç½®
DATA_PATH = "/home/jovyan/JQ/gad_gspo_B300/data/trainning_dataset/subject_1-29/merged/merge-1-29.parquet"

# ================================================


def call_student_api(prompt: str, config: dict) -> str:
    """è°ƒç”¨Student APIç”Ÿæˆresponse"""
    try:
        messages = [{"role": "user", "content": prompt}]
        
        payload = {
            "model": config["model_name"],
            "messages": messages,
            "max_tokens": 512,
            "temperature": config.get("temperature", 0.6),
            "top_p": 0.9,
            "repetition_penalty": config.get("repetition_penalty", 1.2),
        }
        
        headers = {
            "Authorization": f"Bearer {config['api_key']}",
            "Content-Type": "application/json"
        }
        
        response = requests.post(config["url"], json=payload, headers=headers, timeout=60)
        response.raise_for_status()
        
        result = response.json()
        raw_content = result['choices'][0]['message']['content']
        cleaned_content = raw_content.strip()
        
        return cleaned_content
    except Exception as e:
        print(f"âŒ Student APIè°ƒç”¨å¤±è´¥: {e}")
        return ""


def load_critic_model(config: dict):
    """åŠ è½½Criticæ¨¡å‹ï¼ˆä½¿ç”¨trainæ¨¡å¼ï¼Œæ”¯æŒå¤šGPUï¼‰"""
    model_path = config["model_path"]
    use_multi_gpu = config.get("use_multi_gpu", False)
    
    # æ³¨æ„ï¼šCUDA_VISIBLE_DEVICESå·²ç»åœ¨è„šæœ¬å¼€å¤´è®¾ç½®
    # ç°åœ¨GPU 4,5ä¼šè¢«æ˜ å°„ä¸ºcuda:0å’Œcuda:1
    device = "cuda:0"
    
    print(f"ğŸ”„ åŠ è½½tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    
    print(f"ğŸ”„ åŠ è½½æ¨¡å‹...")
    
    if use_multi_gpu:
        # ä½¿ç”¨å¤šGPU - device_mapä¼šè‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰å¯è§çš„GPU
        print(f"âš¡ ä½¿ç”¨å¤šGPUåŠ è½½...")
        base_model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            device_map="auto"  # autoä¼šè‡ªåŠ¨ä½¿ç”¨CUDA_VISIBLE_DEVICESä¸­çš„æ‰€æœ‰GPU
        )
    else:
        # å•GPU
        base_model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            device_map=device
        )
    
    critic_model = AutoModelForCausalLMWithValueHead.from_pretrained(base_model)
    
    # ğŸ”§ å…³é”®ï¼šä½¿ç”¨trainæ¨¡å¼ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
    critic_model.train()
    print(f"âš ï¸  ä½¿ç”¨trainæ¨¡å¼ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰")
    
    if use_multi_gpu:
        # æ‰“å°æ¨¡å‹åˆ†é…æƒ…å†µ
        if hasattr(critic_model.pretrained_model, 'hf_device_map'):
            print(f"âœ… æ¨¡å‹åˆ†é…æƒ…å†µ:")
            device_map = critic_model.pretrained_model.hf_device_map
            # ç»Ÿè®¡æ¯ä¸ªè®¾å¤‡ä¸Šçš„å±‚æ•°
            device_counts = {}
            for layer, dev in device_map.items():
                dev_str = str(dev)
                device_counts[dev_str] = device_counts.get(dev_str, 0) + 1
            for dev, count in sorted(device_counts.items()):
                print(f"   {dev}: {count} å±‚")
        else:
            print(f"âœ… æ¨¡å‹å·²åˆ†é…åˆ°å¤šä¸ªGPU")
    else:
        print(f"âœ… æ¨¡å‹åŠ è½½åˆ° {device}")
    
    return critic_model, tokenizer


def get_critic_scores_batch(critic_model, tokenizer, prompts: list, responses: list, max_length: int = 2048) -> tuple:
    """
    è·å–Criticåˆ†æ•°ï¼ˆæ‰¹é‡æ¨ç†ï¼Œä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰
    
    Args:
        critic_model: Criticæ¨¡å‹
        tokenizer: Tokenizer
        prompts: promptåˆ—è¡¨
        responses: responseåˆ—è¡¨
        max_length: æœ€å¤§åºåˆ—é•¿åº¦
    
    Returns:
        (scores, lengths): åˆ†æ•°åˆ—è¡¨å’Œé•¿åº¦åˆ—è¡¨
    """
    try:
        # è·å–è®¾å¤‡ï¼ˆæ”¯æŒå¤šGPUï¼‰
        if hasattr(critic_model.pretrained_model, 'hf_device_map'):
            # å¤šGPUæ¨¡å¼ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªè®¾å¤‡
            device = list(critic_model.pretrained_model.hf_device_map.values())[0]
        else:
            device = next(critic_model.pretrained_model.parameters()).device
        
        batch_size = len(prompts)
        
        # å‡†å¤‡batchæ•°æ®
        all_input_texts = []
        
        for prompt, response in zip(prompts, responses):
            messages = [
                {"role": "user", "content": prompt},
                {"role": "assistant", "content": response}
            ]
            
            input_text = tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=False
            )
            all_input_texts.append(input_text)
        
        # Batch tokenizationï¼ˆä½¿ç”¨paddingï¼‰
        inputs = tokenizer(
            all_input_texts, 
            return_tensors="pt", 
            truncation=True, 
            max_length=max_length,
            padding=True
        )
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šè®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å®é™…responseé•¿åº¦
        # å¿…é¡»åœ¨tokenizationä¹‹åè®¡ç®—ï¼Œå› ä¸ºchat_templateå¯èƒ½æ·»åŠ ç‰¹æ®Štoken
        all_response_lengths = []
        for prompt, response in zip(prompts, responses):
            # åˆ†åˆ«tokenize promptå’Œå®Œæ•´å¯¹è¯
            prompt_messages = [{"role": "user", "content": prompt}]
            prompt_text = tokenizer.apply_chat_template(
                prompt_messages, tokenize=False, add_generation_prompt=True
            )
            prompt_tokens = tokenizer(prompt_text, add_special_tokens=False)
            prompt_length = len(prompt_tokens['input_ids'])
            
            # å®Œæ•´å¯¹è¯çš„é•¿åº¦
            full_messages = [
                {"role": "user", "content": prompt},
                {"role": "assistant", "content": response}
            ]
            full_text = tokenizer.apply_chat_template(
                full_messages, tokenize=False, add_generation_prompt=False
            )
            full_tokens = tokenizer(full_text, add_special_tokens=False)
            full_length = len(full_tokens['input_ids'])
            
            # responseé•¿åº¦ = å®Œæ•´é•¿åº¦ - prompté•¿åº¦
            response_length = full_length - prompt_length
            all_response_lengths.append(response_length)
        
        # åªå°†inputsç§»åˆ°ç¬¬ä¸€ä¸ªè®¾å¤‡ï¼ˆæ¨¡å‹ä¼šè‡ªåŠ¨å¤„ç†å¤šGPUï¼‰
        if not hasattr(critic_model.pretrained_model, 'hf_device_map'):
            inputs = inputs.to(device)
        else:
            # å¤šGPUæ¨¡å¼ï¼Œç§»åˆ°ç¬¬ä¸€ä¸ªè®¾å¤‡
            inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                     for k, v in inputs.items()}
        
        # Batchå‰å‘ä¼ æ’­
        with torch.no_grad():
            outputs = critic_model(**inputs, use_cache=False)
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ­£ç¡®æå–values
            # å¯¹äº AutoModelForCausalLMWithValueHeadï¼Œvaluesåœ¨ output[2]
            if hasattr(critic_model, "v_head"):
                # output[2] shape: (batch, seq_len) æˆ– (batch, seq_len, 1)
                all_values = outputs[2]
                if all_values.dim() == 3:
                    all_values = all_values.squeeze(-1)  # (batch, seq_len)
            else:
                # å¯¹äºå…¶ä»–æ¨¡å‹ç±»å‹
                all_values = outputs.logits
                if all_values.dim() == 3:
                    all_values = all_values.squeeze(-1)
            
            # æå–æ¯ä¸ªæ ·æœ¬çš„values
            scores = []
            lengths = []
            
            # ğŸ”§ è°ƒè¯•ï¼šæ‰“å°åŸå§‹valuesçš„ç»Ÿè®¡ä¿¡æ¯
            if batch_size <= 10:  # åªåœ¨å°batchæ—¶æ‰“å°
                print(f"\nğŸ” è°ƒè¯•ä¿¡æ¯ - åŸå§‹valuesç»Ÿè®¡:")
                print(f"   all_values shape: {all_values.shape}")
                print(f"   all_values mean: {all_values.mean().item():.4f}")
                print(f"   all_values std: {all_values.std().item():.4f}")
                print(f"   all_values range: [{all_values.min().item():.4f}, {all_values.max().item():.4f}]")
                
                # æ‰“å°æ¯ä¸ªæ ·æœ¬çš„responseé•¿åº¦å’Œå®é™…åºåˆ—é•¿åº¦
                for i in range(min(3, batch_size)):
                    actual_length = inputs['attention_mask'][i].sum().item()
                    response_len = all_response_lengths[i]
                    print(f"   æ ·æœ¬{i}: å®é™…é•¿åº¦={actual_length}, responseé•¿åº¦={response_len}")
                    # æ‰“å°æœ€åå‡ ä¸ªtokençš„values
                    last_values = all_values[i, -response_len:]
                    print(f"           responseéƒ¨åˆ†values: mean={last_values.mean().item():.4f}, "
                          f"min={last_values.min().item():.4f}, max={last_values.max().item():.4f}")
            
            for i in range(batch_size):
                response_length = all_response_lengths[i]
                
                # æå–responseéƒ¨åˆ†çš„values
                values = all_values[i:i+1, -response_length:]  # (1, response_length)
                
                # è·å–maskå¹¶æ’é™¤EOS token
                attention_mask = inputs['attention_mask'][i:i+1]
                response_mask = attention_mask[:, -response_length:]
                response_ids = inputs['input_ids'][i:i+1, -response_length:]
                
                eos_token_id = tokenizer.eos_token_id
                is_eos = (response_ids == eos_token_id)
                response_mask_no_eos = response_mask & (~is_eos)
                
                # ğŸ”§ å…³é”®ä¿®å¤ï¼šç†è§£è®­ç»ƒæ—¶çš„åˆ†æ•°è®¡ç®—æµç¨‹
                # 1. _forward_micro_batchè¿”å›values_outputï¼Œshape (batch, response_length)
                #    å…¶ä¸­åªæœ‰æœ€åä¸€ä¸ªæœ‰æ•ˆä½ç½®æœ‰å¹³å‡å€¼ï¼Œå…¶ä»–ä½ç½®éƒ½æ˜¯0
                # 2. update_criticä¸­ï¼šteacher_score = teacher_vpreds.sum(dim=-1)
                #    å› ä¸ºåªæœ‰ä¸€ä¸ªä½ç½®æœ‰å€¼ï¼Œsumå°±ç­‰äºé‚£ä¸ªå¹³å‡å€¼
                # 3. compute_discriminator_lossä¸­ï¼š
                #    teacher_score_raw = torch.sum(teacher_vpreds * teacher_response_mask, dim=-1)
                #    å› ä¸ºåªæœ‰æœ€åä¸€ä¸ªä½ç½®æœ‰å€¼ä¸”mask=1ï¼Œæ‰€ä»¥è¿˜æ˜¯å¹³å‡å€¼
                # 
                # ç»“è®ºï¼šè®­ç»ƒæ—¶å®é™…ä½¿ç”¨çš„æ˜¯å¹³å‡å€¼ï¼Œä¸æ˜¯sumï¼
                values_sum = (values * response_mask_no_eos).sum(dim=-1)  # (1,)
                length = response_mask_no_eos.sum(dim=-1).clamp(min=1)  # (1,)
                score_avg = (values_sum / length).item()  # å¹³å‡å€¼
                
                scores.append(score_avg)
                lengths.append(length.item())
            
            return scores, lengths
            
    except Exception as e:
        print(f"âŒ Batch Criticè¯„åˆ†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return [0.0] * len(prompts), [0] * len(prompts)


def test_with_training_batch_structure(
    critic_model,
    tokenizer,
    student_config: dict,
    data_path: str, 
    num_samples: int = 100,
    n_resp_per_prompt: int = 4,
    batch_size: int = 32
):
    """
    ä½¿ç”¨ä¸è®­ç»ƒå®Œå…¨ç›¸åŒçš„batchç»“æ„è¿›è¡Œæµ‹è¯•
    
    è®­ç»ƒæ—¶çš„batchç»“æ„ï¼š
    - æ¯ä¸ªpromptç”Ÿæˆn_resp_per_promptä¸ªstudent responses
    - ä¸€ä¸ªbatchåŒ…å«batch_sizeä¸ªprompts
    - teacherså’Œstudentsåœ¨åŒä¸€ä¸ªbatchä¸­è¿›è¡Œæ¨ç†
    
    Args:
        critic_model: Criticæ¨¡å‹
        tokenizer: Tokenizer
        student_config: Studentæ¨¡å‹é…ç½®
        data_path: æ•°æ®é›†è·¯å¾„
        num_samples: æµ‹è¯•æ ·æœ¬æ•°é‡
        n_resp_per_prompt: æ¯ä¸ªpromptç”Ÿæˆçš„student responsesæ•°é‡
        batch_size: æ¯ä¸ªbatchåŒ…å«çš„promptsæ•°é‡
    """
    # è¯»å–æ•°æ®
    df = pd.read_parquet(data_path)
    
    # é‡‡æ ·
    if num_samples:
        df = df.sample(n=min(num_samples, len(df)), random_state=42)
    
    print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
    print(f"   æ ·æœ¬æ•°: {len(df)}")
    print(f"   æ¯ä¸ªpromptçš„studentæ•°: {n_resp_per_prompt}")
    print(f"   Batchå¤§å°: {batch_size} prompts")
    print(f"   æ¯ä¸ªbatchæ€»æ ·æœ¬æ•°: {batch_size * (1 + n_resp_per_prompt)}")
    print()
    
    # å‡†å¤‡æ‰€æœ‰æ•°æ®
    all_prompts = []
    all_teacher_responses = []
    
    for idx, row in df.iterrows():
        try:
            content = row['content']
            if isinstance(content, (list, tuple)) and len(content) > 0:
                prompt = content[0].get('content', '') if isinstance(content[0], dict) else str(content[0])
            else:
                prompt = str(content)
            
            teacher_response = row.get('teacher_response', '')
            
            if not prompt or not teacher_response:
                continue
            
            all_prompts.append(prompt)
            all_teacher_responses.append(teacher_response)
        except Exception as e:
            print(f"âŒ è§£ææ ·æœ¬å¤±è´¥: {e}")
            continue
    
    print(f"ğŸ“ å‡†å¤‡å®Œæˆï¼Œå…± {len(all_prompts)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
    print(f"ğŸ”„ å¼€å§‹ç”ŸæˆStudent responses...")
    
    # ç”Ÿæˆæ‰€æœ‰Student responses
    all_student_responses = []
    for prompt in tqdm(all_prompts, desc="ç”Ÿæˆresponses"):
        student_responses_for_prompt = []
        for _ in range(n_resp_per_prompt):
            response = call_student_api(prompt, student_config)
            if response:
                student_responses_for_prompt.append(response)
            time.sleep(0.1)
        all_student_responses.append(student_responses_for_prompt)
    
    print(f"âœ… Student responsesç”Ÿæˆå®Œæˆ")
    print(f"ğŸ”„ å¼€å§‹Batchæ¨ç†ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„batchç»“æ„ï¼‰...")
    
    # ä½¿ç”¨è®­ç»ƒæ—¶çš„batchç»“æ„è¿›è¡Œæ¨ç†
    results = []
    total_correct = 0
    total_comparisons = 0
    
    all_teacher_scores = []
    all_student_scores = []
    
    for batch_start in tqdm(range(0, len(all_prompts), batch_size), desc="Batchæ¨ç†"):
        batch_end = min(batch_start + batch_size, len(all_prompts))
        
        batch_prompts = all_prompts[batch_start:batch_end]
        batch_teacher_responses = all_teacher_responses[batch_start:batch_end]
        batch_student_responses = all_student_responses[batch_start:batch_end]
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šè®­ç»ƒæ—¶teacherå’Œstudentæ˜¯åˆ†åˆ«forwardçš„ï¼Œä¸æ˜¯æ··åœ¨ä¸€èµ·ï¼
        # æ‰€ä»¥æµ‹è¯•æ—¶ä¹Ÿåº”è¯¥åˆ†åˆ«forward
        
        # 1. Forwardæ‰€æœ‰teachers
        teacher_scores, teacher_lengths = get_critic_scores_batch(
            critic_model, tokenizer, batch_prompts, batch_teacher_responses
        )
        
        # 2. Forwardæ‰€æœ‰studentsï¼ˆæ¯ä¸ªpromptæœ‰n_resp_per_promptä¸ªstudentsï¼‰
        all_student_scores_for_batch = []
        for i, (prompt, student_resps) in enumerate(zip(batch_prompts, batch_student_responses)):
            # ä¸ºæ¯ä¸ªpromptçš„æ‰€æœ‰student responsesåšbatchæ¨ç†
            if len(student_resps) > 0:
                student_scores_for_prompt, _ = get_critic_scores_batch(
                    critic_model, tokenizer, 
                    [prompt] * len(student_resps),  # é‡å¤prompt
                    student_resps
                )
                all_student_scores_for_batch.append(student_scores_for_prompt)
            else:
                all_student_scores_for_batch.append([])
        
        # 3. è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        for i in range(len(batch_prompts)):
            teacher_score = teacher_scores[i]
            student_scores = all_student_scores_for_batch[i]
            
            # ç»Ÿè®¡
            all_teacher_scores.append(teacher_score)
            all_student_scores.extend(student_scores)
            
            # è®¡ç®—å‡†ç¡®ç‡
            correct = sum(1 for s in student_scores if s <= teacher_score)
            total_correct += correct
            total_comparisons += len(student_scores)
            
            results.append({
                'teacher_score': teacher_score,
                'student_scores': student_scores,
                'correct': correct,
                'total': len(student_scores),
            })
    
    # æ‰“å°ç»Ÿè®¡
    print()
    print("=" * 100)
    print("ğŸ“ˆ æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„batchç»“æ„ï¼‰")
    print("=" * 100)
    
    print(f"\næ€»æ ·æœ¬æ•°: {len(results)}")
    print(f"æ€»Studentå“åº”æ•°: {total_comparisons}")
    print(f"æ€»ä½“å‡†ç¡®ç‡: {total_correct}/{total_comparisons} ({total_correct/total_comparisons*100:.2f}%)")
    print()
    
    print("Teacheræ¨¡å‹ç»Ÿè®¡:")
    teacher_mean = sum(all_teacher_scores) / len(all_teacher_scores)
    teacher_std = pd.Series(all_teacher_scores).std()
    print(f"  å¹³å‡åˆ†æ•°: {teacher_mean:.4f}")
    print(f"  åˆ†æ•°æ ‡å‡†å·®: {teacher_std:.4f}")
    print(f"  åˆ†æ•°èŒƒå›´: [{min(all_teacher_scores):.4f}, {max(all_teacher_scores):.4f}]")
    print()
    
    print("Studentæ¨¡å‹ç»Ÿè®¡:")
    student_mean = sum(all_student_scores) / len(all_student_scores)
    student_std = pd.Series(all_student_scores).std()
    print(f"  å¹³å‡åˆ†æ•°: {student_mean:.4f}")
    print(f"  åˆ†æ•°æ ‡å‡†å·®: {student_std:.4f}")
    print(f"  åˆ†æ•°èŒƒå›´: [{min(all_student_scores):.4f}, {max(all_student_scores):.4f}]")
    print()
    
    print("åˆ†æ•°å·®å¼‚:")
    score_diff = teacher_mean - student_mean
    print(f"  Teacher - Student: {score_diff:.4f}")
    print()
    
    print("âœ… ä½¿ç”¨trainæ¨¡å¼ + è®­ç»ƒæ—¶çš„batchç»“æ„")
    print("=" * 100)
    
    return results


def main():
    parser = argparse.ArgumentParser(description="æµ‹è¯•Criticæ¨¡å‹ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„batchç»“æ„ï¼‰")
    parser.add_argument("--data_path", type=str, default=DATA_PATH)
    parser.add_argument("--num_samples", type=int, default=100)
    parser.add_argument("--n_resp_per_prompt", type=int, default=4,
                       help="æ¯ä¸ªpromptç”Ÿæˆçš„student responsesæ•°é‡ï¼ˆä¸è®­ç»ƒé…ç½®ä¸€è‡´ï¼‰")
    parser.add_argument("--batch_size", type=int, default=8,
                       help="æ¯ä¸ªbatchåŒ…å«çš„promptsæ•°é‡ï¼ˆå‡å°ä»¥èŠ‚çœæ˜¾å­˜ï¼‰")
    parser.add_argument("--critic_path", type=str, default=None,
                       help="Criticæ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼‰")
    parser.add_argument("--use_multi_gpu", action="store_true", default=True,
                       help="ä½¿ç”¨å¤šGPUï¼ˆé»˜è®¤å¼€å¯ï¼‰")
    parser.add_argument("--gpu_ids", type=str, default="4,5",
                       help="æŒ‡å®šä½¿ç”¨çš„GPU IDsï¼Œç”¨é€—å·åˆ†éš”ï¼Œä¾‹å¦‚ï¼š'4,5' æˆ– '0,1,2,3'")
    parser.add_argument("--max_length", type=int, default=2048,
                       help="æœ€å¤§åºåˆ—é•¿åº¦")
    
    args = parser.parse_args()
    
    # æ›´æ–°é…ç½®
    if args.critic_path:
        CRITIC_CONFIG["model_path"] = args.critic_path
    CRITIC_CONFIG["use_multi_gpu"] = args.use_multi_gpu
    
    print("=" * 100)
    print("ğŸ“‹ æµ‹è¯•é…ç½®ä¿¡æ¯ï¼ˆå®Œå…¨æ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„batchç»“æ„ï¼‰")
    print("=" * 100)
    print(f"æ•°æ®é›†è·¯å¾„: {args.data_path}")
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {args.num_samples}")
    print(f"æ¯ä¸ªpromptçš„Studentæ ·æœ¬æ•°: {args.n_resp_per_prompt}")
    print(f"Batchå¤§å°: {args.batch_size} prompts")
    print(f"Criticæ¨¡å‹: {CRITIC_CONFIG['model_path']}")
    print(f"Studentæ¨¡å‹: {STUDENT_CONFIG['url']}")
    print(f"å¤šGPUæ¨¡å¼: {'å¼€å¯' if args.use_multi_gpu else 'å…³é—­'}")
    if args.use_multi_gpu:
        print(f"æŒ‡å®šGPU: {args.gpu_ids} (å·²é€šè¿‡CUDA_VISIBLE_DEVICESè®¾ç½®)")
    print(f"æœ€å¤§åºåˆ—é•¿åº¦: {args.max_length}")
    print()
    print("å…³é”®æ”¹è¿›:")
    print("  1. âœ… ä½¿ç”¨trainæ¨¡å¼ï¼ˆè€Œéevalæ¨¡å¼ï¼‰")
    print("  2. âœ… Teacherså’ŒStudentsåœ¨åŒä¸€ä¸ªbatchä¸­æ¨ç†")
    print("  3. âœ… æ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„n_resp_per_prompt=4é…ç½®")
    if args.use_multi_gpu:
        print(f"  4. âš¡ ä½¿ç”¨å¤šGPUåŠ é€Ÿï¼ˆGPU: {args.gpu_ids}ï¼‰")
    print("=" * 100)
    print()
    
    # åŠ è½½æ¨¡å‹
    critic_model, tokenizer = load_critic_model(CRITIC_CONFIG)
    
    # è¿è¡Œæµ‹è¯•
    test_with_training_batch_structure(
        critic_model=critic_model,
        tokenizer=tokenizer,
        student_config=STUDENT_CONFIG,
        data_path=args.data_path,
        num_samples=args.num_samples,
        n_resp_per_prompt=args.n_resp_per_prompt,
        batch_size=args.batch_size
    )
    
    print("âœ… æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()
