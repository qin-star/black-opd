"""
æµ‹è¯•è®­ç»ƒåçš„Criticå’ŒStudentæ¨¡å‹
ç”¨äºè¯„ä¼°æ¨¡å‹æ‰“åˆ†èƒ½åŠ›å’Œè®­ç»ƒæ•ˆæœ
"""
import requests
import pandas as pd
import argparse
from typing import Dict, Optional
from datetime import datetime
from tqdm import tqdm
import time
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


# ==================== é…ç½®åŒºåŸŸ ====================
# åœ¨è¿™é‡Œä¿®æ”¹ä½ çš„æ¨¡å‹é…ç½®å’Œæµ‹è¯•å‚æ•°

# æµ‹è¯•å‚æ•°é…ç½®
TEST_CONFIG = {
    "data_path": "/home/jovyan/JQ/gad_gspo_B300/data/trainning_dataset/subject_1-29/merged/merge-1-29.parquet",  # æµ‹è¯•æ•°æ®é›†è·¯å¾„
    "num_samples": 10,  # æµ‹è¯•æ ·æœ¬æ•°é‡ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
    "random_sample": True,  # æ˜¯å¦éšæœºæŠ½æ ·ï¼ˆTrue=éšæœºï¼ŒFalse=å–å‰Nä¸ªï¼‰
    "random_seed": 42,  # éšæœºç§å­ï¼ˆç”¨äºå¯å¤ç°çš„éšæœºæŠ½æ ·ï¼‰
    "output_dir": "/home/jovyan/JQ/gad_gspo_B300/outputs",  # è¾“å‡ºç›®å½•
    "output_filename": None  # è¾“å‡ºæ–‡ä»¶åï¼ˆNoneè¡¨ç¤ºè‡ªåŠ¨ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶åï¼‰
}

# æ¨¡å‹é…ç½®
API_CONFIGS = {
    "critic_model": {
        "name": "critic-model",
        "type": "local",  # "local" è¡¨ç¤ºæœ¬åœ°åŠ è½½ï¼Œ"api" è¡¨ç¤ºAPIè°ƒç”¨
        "model_path": "/home/jovyan/JQ/gad_gspo_B300/models/opd-v9-1-29-fsdp2/global_step_500/critic_merged",  # æœ¬åœ°æ¨¡å‹è·¯å¾„
        "device": "cuda:4",  # ä½¿ç”¨çš„è®¾å¤‡
        "temperature": 0.0,
        "repetition_penalty": 1.0,
        "force_trl": True,  # å¼ºåˆ¶ä½¿ç”¨trlåŠ è½½ï¼ˆè·³è¿‡TokenClassificationå°è¯•ï¼‰
        "use_simple_length": False  # ä½¿ç”¨ç®€å•çš„é•¿åº¦è®¡ç®—ï¼ˆsplit()è€Œä¸æ˜¯tokenizerï¼‰
    },
    "student_model": {
        "name": "student-model",
        "type": "api",  # APIè°ƒç”¨
        "url": "http://10.72.1.39:8008/v1/chat/completions",
        "api_key": "sk-xxxx",
        "model_name": "opd-v9-500",
        "temperature": 0.6,
        "repetition_penalty": 1.2
    }
}
# ================================================


class ModelTester:
    def __init__(self, critic_config: Dict, student_config: Dict):
        """
        åˆå§‹åŒ–æ¨¡å‹æµ‹è¯•å™¨
        
        Args:
            critic_config: Criticæ¨¡å‹é…ç½®å­—å…¸
            student_config: Studentæ¨¡å‹é…ç½®å­—å…¸
        """
        self.critic_config = critic_config
        self.student_config = student_config
        
        # å¦‚æœCriticæ˜¯æœ¬åœ°æ¨¡å‹ï¼ŒåŠ è½½å®ƒ
        self.critic_model = None
        self.critic_tokenizer = None
        if critic_config.get("type") == "local":
            print(f"ğŸ”„ åŠ è½½æœ¬åœ°Criticæ¨¡å‹: {critic_config['model_path']}")
            self.load_local_critic(critic_config)
            print(f"âœ… Criticæ¨¡å‹åŠ è½½å®Œæˆ")
    
    def load_local_critic(self, config: Dict):
        """
        åŠ è½½æœ¬åœ°Criticæ¨¡å‹
        
        Args:
            config: Criticæ¨¡å‹é…ç½®
        """
        device = config.get("device", "cuda:0")
        model_path = config["model_path"]
        force_trl = config.get("force_trl", False)
        
        print(f"ğŸ”„ åŠ è½½tokenizer...")
        # åŠ è½½tokenizer
        self.critic_tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        print(f"âœ… TokenizeråŠ è½½å®Œæˆ")
        
        # å¦‚æœå¼ºåˆ¶ä½¿ç”¨trlï¼Œç›´æ¥è·³è¿‡TokenClassificationå°è¯•
        if force_trl:
            print(f"âš¡ é…ç½®å¼ºåˆ¶ä½¿ç”¨trlï¼Œè·³è¿‡TokenClassificationå°è¯•")
            self._load_with_trl(model_path, device)
            return
        
        # ğŸ”§ å…³é”®ï¼šæŒ‰ç…§è®­ç»ƒä»£ç çš„é€»è¾‘åŠ è½½æ¨¡å‹
        # é¦–å…ˆå°è¯• AutoModelForTokenClassificationï¼Œå¤±è´¥åˆ™ä½¿ç”¨ trl
        from transformers import AutoModelForTokenClassification, AutoConfig
        
        # å…ˆæ£€æŸ¥æ¨¡å‹é…ç½®
        print(f"ğŸ” æ£€æŸ¥æ¨¡å‹é…ç½®...")
        model_config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        print(f"  æ¶æ„: {model_config.architectures}")
        print(f"  num_labels: {getattr(model_config, 'num_labels', 'N/A')}")
        
        # åˆ¤æ–­æ˜¯å¦æ˜¯TokenClassificationæ¨¡å‹
        is_token_classification = any('TokenClassification' in arch for arch in model_config.architectures)
        
        if is_token_classification:
            print(f"ğŸ”„ ä½¿ç”¨ AutoModelForTokenClassification åŠ è½½...")
            try:
                self.critic_model = AutoModelForTokenClassification.from_pretrained(
                    model_path,
                    trust_remote_code=True,
                    torch_dtype=torch.float16,
                    device_map=device
                )
                print(f"âœ… æˆåŠŸä½¿ç”¨ AutoModelForTokenClassification åŠ è½½")
                self.critic_model.eval()
                return
            except Exception as e:
                print(f"âŒ AutoModelForTokenClassification åŠ è½½å¤±è´¥: {e}")
                raise
        
        # å¦‚æœä¸æ˜¯TokenClassificationï¼Œä½¿ç”¨trl
        print(f"ğŸ”„ æ¨¡å‹ä¸æ˜¯TokenClassificationç±»å‹ï¼Œä½¿ç”¨trlåŠ è½½...")
        self._load_with_trl(model_path, device)
    
    def _load_with_trl(self, model_path: str, device: str):
        """ä½¿ç”¨trlåŠ è½½æ¨¡å‹"""
        try:
            from trl import AutoModelForCausalLMWithValueHead
            from transformers import AutoModelForCausalLM
            
            print(f"  åŠ è½½åŸºç¡€CausalLMæ¨¡å‹...")
            # å…ˆåŠ è½½åŸºç¡€æ¨¡å‹
            base_model = AutoModelForCausalLM.from_pretrained(
                model_path,
                trust_remote_code=True,
                torch_dtype=torch.float16,
                device_map=device
            )
            
            print(f"  åŒ…è£…ä¸ºValueHeadæ¨¡å‹...")
            # åŒ…è£…ä¸ºValueHeadæ¨¡å‹
            self.critic_model = AutoModelForCausalLMWithValueHead.from_pretrained(base_model)
            print(f"âœ… æˆåŠŸä½¿ç”¨ trl.AutoModelForCausalLMWithValueHead åŠ è½½")
            self.critic_model.eval()
        except Exception as e:
            print(f"âŒ trlåŠ è½½å¤±è´¥: {e}")
            raise RuntimeError(f"æ— æ³•åŠ è½½Criticæ¨¡å‹: {e}")
    
    def call_generation_model(self, config: Dict, prompt: str, max_tokens: int = 512) -> Dict:
        """
        è°ƒç”¨ç”Ÿæˆæ¨¡å‹API
        
        Args:
            config: æ¨¡å‹é…ç½®å­—å…¸
            prompt: è¾“å…¥æç¤º
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            
        Returns:
            åŒ…å«ç”Ÿæˆæ–‡æœ¬å’Œtokenæ•°çš„å­—å…¸
        """
        try:
            messages = [{"role": "user", "content": prompt}]
            
            payload = {
                "model": config["model_name"],
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": config.get("temperature", 0.7),
                "top_p": 0.9,
                "repetition_penalty": config.get("repetition_penalty", 1.0),
                "n": 1
            }
            
            headers = {
                "Authorization": f"Bearer {config['api_key']}",
                "Content-Type": "application/json"
            }
            
            response = requests.post(
                config["url"],
                json=payload,
                headers=headers,
                timeout=60
            )
            response.raise_for_status()
            
            result = response.json()
            generated_text = result['choices'][0]['message']['content']
            usage = result.get('usage', {})
            tokens = usage.get('completion_tokens', len(generated_text.split()))
            
            return {
                'text': generated_text.strip(),
                'length': tokens
            }
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ¨¡å‹è°ƒç”¨å¤±è´¥ [{config['name']}]: {e}")
            return {'text': '', 'length': 0}
    
    def call_critic_model(self, config: Dict, prompt: str, response: str) -> float:
        """
        è°ƒç”¨Criticæ¨¡å‹è¿›è¡Œæ‰“åˆ†
        
        Args:
            config: Criticæ¨¡å‹é…ç½®å­—å…¸
            prompt: åŸå§‹æç¤º
            response: æ¨¡å‹å“åº”
            
        Returns:
            åˆ†æ•°
        """
        # åˆ¤æ–­æ˜¯æœ¬åœ°æ¨¡å‹è¿˜æ˜¯API
        if config.get("type") == "local":
            return self.call_local_critic(prompt, response)
        else:
            return self.call_api_critic(config, prompt, response)
    
    def call_local_critic(self, prompt: str, response: str) -> float:
        """
        è°ƒç”¨æœ¬åœ°Criticæ¨¡å‹è¿›è¡Œæ‰“åˆ†
        
        âš ï¸ é‡è¦ï¼šæ­¤æ–¹æ³•å¿…é¡»ä¸è®­ç»ƒæ—¶çš„åˆ†æ•°è®¡ç®—é€»è¾‘å®Œå…¨ä¸€è‡´ï¼
        
        å…³é”®ä¿®å¤ï¼ˆ2026-02-03ï¼‰ï¼š
        1. å€¼æå–é¡ºåºï¼šå…ˆæå–responseéƒ¨åˆ†ï¼Œå†squeezeï¼ˆä¸è®­ç»ƒä»£ç dp_critic.pyä¸€è‡´ï¼‰
           - è®­ç»ƒä»£ç ï¼švalues[:, -response_length:].squeeze(-1)
           - ä¹‹å‰é”™è¯¯ï¼šå…ˆsqueezeæ•´ä¸ªåºåˆ—ï¼Œå†æå–responseï¼ˆå¯¼è‡´ç»´åº¦é”™ä½ï¼‰
        2. EOS tokenæ’é™¤ï¼šæ˜¾å¼æ’é™¤EOS tokenåè®¡ç®—å¹³å‡å€¼
        3. å¹³å‡å€¼æœºåˆ¶ï¼šå¯¹responseæ‰€æœ‰tokenï¼ˆæ’é™¤EOSï¼‰çš„valueså–å¹³å‡
        
        Args:
            prompt: åŸå§‹æç¤º
            response: æ¨¡å‹å“åº”
            
        Returns:
            åˆ†æ•°ï¼ˆä¸è®­ç»ƒæ—¶è®¡ç®—æ–¹å¼ä¸€è‡´ï¼‰
        """
        try:
            # æ„å»ºè¾“å…¥ - ä½¿ç”¨ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ ¼å¼
            messages = [
                {"role": "user", "content": prompt},
                {"role": "assistant", "content": response}
            ]
            
            # ä½¿ç”¨tokenizerçš„chat template
            if hasattr(self.critic_tokenizer, 'apply_chat_template'):
                input_text = self.critic_tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=False
                )
            else:
                # ç®€å•æ‹¼æ¥ï¼ˆå¦‚æœæ²¡æœ‰chat templateï¼‰
                input_text = f"User: {prompt}\nAssistant: {response}"
            
            # Tokenize
            inputs = self.critic_tokenizer(
                input_text,
                return_tensors="pt",
                truncation=True,
                max_length=2048
            )
            
            # ğŸ”§ è·å–æ¨¡å‹è®¾å¤‡ï¼ˆå…¼å®¹ä¸åŒæ¨¡å‹ç±»å‹ï¼‰
            if hasattr(self.critic_model, 'device'):
                device = self.critic_model.device
            elif hasattr(self.critic_model, 'pretrained_model'):
                # trlæ¨¡å‹çš„è®¾å¤‡åœ¨pretrained_modelä¸­
                device = next(self.critic_model.pretrained_model.parameters()).device
            else:
                # ä»ç¬¬ä¸€ä¸ªå‚æ•°è·å–è®¾å¤‡
                device = next(self.critic_model.parameters()).device
            
            inputs = inputs.to(device)
            input_ids = inputs['input_ids']
            attention_mask = inputs['attention_mask']
            
            # å‰å‘ä¼ æ’­è·å–æ‰€æœ‰tokençš„values
            with torch.no_grad():
                outputs = self.critic_model(**inputs, use_cache=False)
                
                # ğŸ”§ æ ¹æ®æ¨¡å‹ç±»å‹è·å–values - ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´
                # æ–¹å¼1: AutoModelForTokenClassification (æœ‰logitså±æ€§)
                if hasattr(outputs, 'logits'):
                    values = outputs.logits
                    # ğŸ”§ å…³é”®ï¼šå…ˆæå–responseéƒ¨åˆ†ï¼Œå†squeezeï¼ˆä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰
                    # é‡æ–°tokenize responseä»¥è·å–å…¶é•¿åº¦
                    response_tokens = self.critic_tokenizer(
                        response,
                        add_special_tokens=False,
                        return_tensors="pt"
                    )
                    response_length = response_tokens['input_ids'].size(1)
                    
                    # å…ˆæå–responseéƒ¨åˆ†
                    values = values[:, -response_length:]  # (batch, response_length, 1) or (batch, response_length)
                    # å†squeeze
                    if values.dim() == 3:
                        values = values.squeeze(-1)  # (batch, response_length)
                
                # æ–¹å¼2: trl.AutoModelForCausalLMWithValueHead (è¿”å›tuple)
                elif hasattr(self.critic_model, 'v_head') or isinstance(outputs, tuple):
                    if isinstance(outputs, tuple) and len(outputs) > 2:
                        values = outputs[2]  # value headçš„è¾“å‡º (batch, seq_len, 1) or (batch, seq_len)
                        
                        # ğŸ”§ å…³é”®ï¼šä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´çš„å¤„ç†é¡ºåº
                        # 1. å…ˆæå–responseéƒ¨åˆ†
                        # 2. å†squeezeæœ€åä¸€ç»´
                        
                        # é‡æ–°tokenize responseä»¥è·å–å…¶é•¿åº¦
                        response_tokens = self.critic_tokenizer(
                            response,
                            add_special_tokens=False,
                            return_tensors="pt"
                        )
                        response_length = response_tokens['input_ids'].size(1)
                        
                        # å…ˆæå–responseéƒ¨åˆ†
                        values = values[:, -response_length:]  # (batch, response_length, 1) or (batch, response_length)
                        
                        # ğŸ”§ è°ƒè¯•ï¼šæ‰“å°valuesçš„å½¢çŠ¶ï¼ˆåœ¨squeezeä¹‹å‰ï¼‰
                        if not hasattr(self, '_values_shape_printed'):
                            print(f"\nğŸ” Valuesè°ƒè¯•ä¿¡æ¯:")
                            print(f"  æå–responseå values shape: {values.shape}")
                            print(f"  Response length: {response_length}")
                            print(f"  åŸå§‹values dtype: {values.dtype}")
                            print(f"  åŸå§‹valuesèŒƒå›´: [{values.min().item():.4f}, {values.max().item():.4f}]")
                            print(f"  åŸå§‹valueså‡å€¼: {values.mean().item():.4f}")
                            self._values_shape_printed = True
                        
                        # å†squeezeæœ€åä¸€ç»´
                        if values.dim() == 3:
                            values = values.squeeze(-1)  # (batch, response_length)
                    else:
                        raise ValueError("æ¨¡å‹è¿”å›tupleä½†æ ¼å¼ä¸æ­£ç¡®")
                else:
                    raise ValueError(f"æ— æ³•è¯†åˆ«çš„æ¨¡å‹è¾“å‡ºæ ¼å¼: {type(outputs)}")
            
            # ğŸ”§ å…³é”®ï¼šä¸è®­ç»ƒä»£ç ä¿æŒä¸€è‡´çš„åˆ†æ•°è®¡ç®—
            # 1. valueså·²ç»æ˜¯responseéƒ¨åˆ†äº†ï¼ˆåœ¨ä¸Šé¢æå–è¿‡ï¼‰
            # 2. æ’é™¤EOS token
            # 3. è®¡ç®—å¹³å‡å€¼
            
            # valueså·²ç»æ˜¯responseéƒ¨åˆ†ï¼Œç›´æ¥ä½¿ç”¨
            response_values = values  # (1, response_length)
            response_mask = attention_mask[:, -response_length:]  # (1, response_length)
            
            # è·å–responseçš„token IDsï¼ˆç”¨äºè¯†åˆ«EOSï¼‰
            response_ids = input_ids[:, -response_length:]
            
            # è·å–EOS token ID
            eos_token_id = self.critic_tokenizer.eos_token_id
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ’é™¤EOS token
            is_eos = (response_ids == eos_token_id)
            response_mask_no_eos = response_mask & (~is_eos)
            
            # è®¡ç®—å¹³å‡å€¼ï¼ˆä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´ï¼‰
            values_sum = (response_values * response_mask_no_eos).sum(dim=-1)  # (1,)
            values_count = response_mask_no_eos.sum(dim=-1).clamp(min=1)  # (1,)
            score = (values_sum / values_count).item()  # scalar
            
            return float(score)
            
        except Exception as e:
            print(f"âŒ æœ¬åœ°Criticæ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0.0
    
    def call_api_critic(self, config: Dict, prompt: str, response: str) -> float:
        """
        é€šè¿‡APIè°ƒç”¨Criticæ¨¡å‹è¿›è¡Œæ‰“åˆ†
        
        Args:
            config: Criticæ¨¡å‹é…ç½®å­—å…¸
            prompt: åŸå§‹æç¤º
            response: æ¨¡å‹å“åº”
            
        Returns:
            åˆ†æ•°
        """
        try:
            messages = [
                {"role": "user", "content": prompt},
                {"role": "assistant", "content": response}
            ]
            
            payload = {
                "model": config["model_name"],
                "messages": messages,
                "max_tokens": 1,
                "temperature": config.get("temperature", 0.0),
                "logprobs": True,
                "top_logprobs": 1
            }
            
            headers = {
                "Authorization": f"Bearer {config['api_key']}",
                "Content-Type": "application/json"
            }
            
            response_obj = requests.post(
                config["url"],
                json=payload,
                headers=headers,
                timeout=30
            )
            response_obj.raise_for_status()
            
            result = response_obj.json()
            
            # å°è¯•æå–åˆ†æ•°
            if 'score' in result:
                score = float(result['score'])
            elif 'choices' in result and len(result['choices']) > 0:
                choice = result['choices'][0]
                if 'logprobs' in choice and choice['logprobs']:
                    logprobs = choice['logprobs']
                    if 'content' in logprobs and len(logprobs['content']) > 0:
                        score = logprobs['content'][0].get('logprob', 0.0)
                    else:
                        score = 0.0
                elif 'message' in choice:
                    content = choice['message'].get('content', '0.0')
                    try:
                        score = float(content.strip())
                    except:
                        score = 0.0
                else:
                    score = 0.0
            else:
                score = 0.0
            
            return score
        except Exception as e:
            print(f"âŒ Critic APIè°ƒç”¨å¤±è´¥ [{config['name']}]: {e}")
            return 0.0
    
    def format_prompt(self, instruction: str, input_text: str = "") -> str:
        """
        æ ¼å¼åŒ–æç¤ºè¯
        
        Args:
            instruction: æŒ‡ä»¤
            input_text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            æ ¼å¼åŒ–åçš„æç¤º
        """
        if input_text:
            return f"{instruction}\n\n{input_text}"
        return instruction
    
    def test_single_sample(self, prompt: str, teacher_response: Optional[str] = None) -> Dict:
        """
        æµ‹è¯•å•ä¸ªæ ·æœ¬
        
        Args:
            prompt: è¾“å…¥æç¤º
            teacher_response: æ•™å¸ˆå“åº”ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æµ‹è¯•ç»“æœå­—å…¸
        """
        print("ğŸ“ Prompt:")
        print(f"  {prompt}")
        print()
        
        # ç”ŸæˆStudentå“åº”
        student_result = self.call_generation_model(self.student_config, prompt)
        student_text = student_result['text']
        student_length = student_result['length']
        
        # è·å–Criticå¯¹Studentçš„æ‰“åˆ†
        student_score = self.call_critic_model(self.critic_config, prompt, student_text)
        
        print("ğŸ“ Student Response:")
        print("-" * 100)
        print(f"  Score: {student_score:7.4f} | Length: {student_length:3d}")
        print(f"  Text: {student_text}")
        print()
        
        result = {
            'prompt': prompt,
            'student_response': student_text,
            'student_score': student_score,
            'student_length': student_length
        }
        
        # ä½¿ç”¨æ•°æ®é›†ä¸­çš„teacherå“åº”
        teacher_text = teacher_response
        
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨tokenizerè®¡ç®—å‡†ç¡®çš„tokené•¿åº¦ï¼ˆæˆ–ä½¿ç”¨ç®€å•è®¡ç®—ï¼‰
        if self.critic_config.get("use_simple_length", False):
            # ç®€å•è®¡ç®—ï¼šæŒ‰å­—ç¬¦æ•°ï¼ˆä¸­æ–‡ï¼‰æˆ–å•è¯æ•°ï¼ˆè‹±æ–‡ï¼‰
            teacher_length = len(teacher_text) if any('\u4e00' <= c <= '\u9fff' for c in teacher_text[:100]) else len(teacher_text.split())
        else:
            # ç²¾ç¡®è®¡ç®—ï¼šä½¿ç”¨tokenizer
            teacher_tokens = self.critic_tokenizer(
                teacher_text,
                add_special_tokens=False,
                return_tensors="pt"
            )
            teacher_length = teacher_tokens['input_ids'].size(1)
        
        # è·å–Criticå¯¹Teacherçš„æ‰“åˆ†
        teacher_score = self.call_critic_model(self.critic_config, prompt, teacher_text)
        
        print("ğŸ‘¨â€ğŸ« Teacher Response:")
        print("-" * 100)
        print(f"  Score: {teacher_score:7.4f} | Length: {teacher_length:3d}")
        print(f"  Text: {teacher_text}")
        print()
        
        # è®¡ç®—åˆ†æ•°å·®å¼‚
        score_diff = teacher_score - student_score
        is_correct = teacher_score > student_score
        
        print("ğŸ“Š åˆ†æ•°å¯¹æ¯”:")
        print(f"  Teacher - Student = {score_diff:7.4f}")
        print(f"  Teacher > Student: {'âœ… æ­£ç¡®' if is_correct else 'âŒ é”™è¯¯'}")
        print()
        print("=" * 100)
        print()
        
        result.update({
            'teacher_response': teacher_text,
            'teacher_score': teacher_score,
            'teacher_length': teacher_length,
            'score_diff': score_diff,
            'is_correct': is_correct
        })
        
        return result
    
    def test_dataset(self, data_path: str, num_samples: Optional[int] = None, 
                    output_path: Optional[str] = None, random_sample: bool = False,
                    random_seed: int = 42) -> pd.DataFrame:
        """
        æµ‹è¯•æ•°æ®é›†
        
        Args:
            data_path: æ•°æ®é›†è·¯å¾„ï¼ˆæ”¯æŒ.parquetæˆ–.xlsxï¼‰
            num_samples: æµ‹è¯•æ ·æœ¬æ•°é‡ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
            output_path: è¾“å‡ºExcelè·¯å¾„
            random_sample: æ˜¯å¦éšæœºæŠ½æ ·
            random_seed: éšæœºç§å­
            
        Returns:
            æµ‹è¯•ç»“æœDataFrame
        """
        # è¯»å–æ•°æ®
        if data_path.endswith('.parquet'):
            df = pd.read_parquet(data_path)
        elif data_path.endswith('.xlsx'):
            df = pd.read_excel(data_path)
        else:
            raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œä»…æ”¯æŒ.parquetå’Œ.xlsx")
        
        # é™åˆ¶æµ‹è¯•æ•°é‡
        if num_samples is not None:
            if random_sample:
                # éšæœºæŠ½æ ·
                df = df.sample(n=min(num_samples, len(df)), random_state=random_seed)
                print(f"ğŸ“Š éšæœºæŠ½æ · {len(df)} ä¸ªæ ·æœ¬ï¼ˆç§å­={random_seed}ï¼‰...")
            else:
                # å–å‰Nä¸ª
                df = df.head(num_samples)
                print(f"ğŸ“Š å–å‰ {len(df)} ä¸ªæ ·æœ¬...")
        else:
            print(f"ğŸ“Š æµ‹è¯•å…¨éƒ¨ {len(df)} ä¸ªæ ·æœ¬...")
        print()
        
        results = []
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="æµ‹è¯•è¿›åº¦"):
            try:
                # è§£æcontentå­—æ®µ - contentæ˜¯ä¸€ä¸ªæ•°ç»„ï¼ŒåŒ…å«å¯¹è¯å†…å®¹
                if 'content' in row:
                    content = row['content']
                    if isinstance(content, (list, tuple)) and len(content) > 0:
                        # è·å–ç¬¬ä¸€ä¸ªå…ƒç´ çš„contentå­—æ®µä½œä¸ºprompt
                        prompt = content[0].get('content', '') if isinstance(content[0], dict) else str(content[0])
                    else:
                        prompt = str(content)
                elif 'instruction' in row and 'input' in row:
                    prompt = self.format_prompt(row['instruction'], row.get('input', ''))
                elif 'prompt' in row:
                    prompt = row['prompt']
                elif 'question' in row:
                    prompt = row['question']
                else:
                    print(f"âš ï¸  è·³è¿‡ç¬¬ {idx} è¡Œï¼šæ— æ³•è¯†åˆ«çš„æ•°æ®æ ¼å¼")
                    continue
                
                # è·å–teacher_response
                teacher_response = row.get('teacher_response', '')
                
                if not prompt or not teacher_response:
                    print(f"âš ï¸  è·³è¿‡ç¬¬ {idx} è¡Œï¼šç¼ºå°‘promptæˆ–teacher_response")
                    continue
                
                result = self.test_single_sample(prompt, teacher_response)
                result['sample_id'] = idx
                result['data_id'] = row.get('id', idx)
                results.append(result)
                
                # é¿å…è¯·æ±‚è¿‡å¿«
                time.sleep(0.5)
            except Exception as e:
                print(f"âŒ æµ‹è¯•ç¬¬ {idx} ä¸ªæ ·æœ¬æ—¶å‡ºé”™: {e}")
                continue
        
        # è½¬æ¢ä¸ºDataFrame
        results_df = pd.DataFrame(results)
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        self.print_statistics(results_df)
        
        # ä¿å­˜ç»“æœ
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"test_results_{timestamp}.xlsx"
        
        # ç¡®ä¿è¾“å‡ºè·¯å¾„æœ‰.xlsxæ‰©å±•å
        if not output_path.endswith('.xlsx'):
            output_path = output_path + '.xlsx'
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        import os
        output_dir = os.path.dirname(output_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
        
        results_df.to_excel(output_path, index=False)
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
        return results_df
    
    def print_statistics(self, results_df: pd.DataFrame):
        """
        æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            results_df: æµ‹è¯•ç»“æœDataFrame
        """
        print()
        print("=" * 100)
        print("ğŸ“ˆ æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯")
        print("=" * 100)
        
        total = len(results_df)
        
        if total == 0:
            print("âš ï¸  æ²¡æœ‰æˆåŠŸæµ‹è¯•çš„æ ·æœ¬")
            print("=" * 100)
            return
        
        correct = results_df['is_correct'].sum()
        accuracy = correct / total * 100 if total > 0 else 0
        
        print(f"æ€»æ ·æœ¬æ•°: {total}")
        print(f"æ­£ç¡®åˆ¤æ–­æ•°: {correct}")
        print(f"å‡†ç¡®ç‡: {accuracy:.2f}%")
        print()
        
        print("Studentæ¨¡å‹ç»Ÿè®¡:")
        print(f"  å¹³å‡åˆ†æ•°: {results_df['student_score'].mean():.4f}")
        print(f"  åˆ†æ•°æ ‡å‡†å·®: {results_df['student_score'].std():.4f}")
        print(f"  å¹³å‡é•¿åº¦: {results_df['student_length'].mean():.2f}")
        print()
        
        print("Teacheræ¨¡å‹ç»Ÿè®¡:")
        print(f"  å¹³å‡åˆ†æ•°: {results_df['teacher_score'].mean():.4f}")
        print(f"  åˆ†æ•°æ ‡å‡†å·®: {results_df['teacher_score'].std():.4f}")
        print(f"  å¹³å‡é•¿åº¦: {results_df['teacher_length'].mean():.2f}")
        print()
        
        print("åˆ†æ•°å·®å¼‚ç»Ÿè®¡:")
        print(f"  å¹³å‡å·®å¼‚: {results_df['score_diff'].mean():.4f}")
        print(f"  å·®å¼‚æ ‡å‡†å·®: {results_df['score_diff'].std():.4f}")
        print(f"  æœ€å¤§å·®å¼‚: {results_df['score_diff'].max():.4f}")
        print(f"  æœ€å°å·®å¼‚: {results_df['score_diff'].min():.4f}")
        print()
        print("=" * 100)


def main():
    parser = argparse.ArgumentParser(description="æµ‹è¯•è®­ç»ƒåçš„Criticå’ŒStudentæ¨¡å‹")
    parser.add_argument("--data_path", type=str, default=None,
                       help="æµ‹è¯•æ•°æ®é›†è·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„ï¼‰")
    parser.add_argument("--num_samples", type=int, default=None,
                       help="æµ‹è¯•æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„æ•°é‡ï¼‰")
    parser.add_argument("--output_path", type=str, default=None,
                       help="è¾“å‡ºExcelè·¯å¾„ï¼ˆé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„ï¼‰")
    
    args = parser.parse_args()
    
    # ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°æˆ–é…ç½®æ–‡ä»¶ä¸­çš„å€¼
    data_path = args.data_path or TEST_CONFIG["data_path"]
    num_samples = args.num_samples if args.num_samples is not None else TEST_CONFIG["num_samples"]
    
    # å¤„ç†è¾“å‡ºè·¯å¾„
    if args.output_path:
        output_path = args.output_path
    else:
        # ä½¿ç”¨é…ç½®ä¸­çš„è¾“å‡ºç›®å½•å’Œæ–‡ä»¶å
        output_dir = TEST_CONFIG["output_dir"]
        output_filename = TEST_CONFIG.get("output_filename")
        
        if output_filename:
            output_path = f"{output_dir}/{output_filename}"
        else:
            # è‡ªåŠ¨ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"{output_dir}/test_results_{timestamp}.xlsx"
    
    # ä½¿ç”¨è„šæœ¬é¡¶éƒ¨çš„é…ç½®
    critic_config = API_CONFIGS["critic_model"]
    student_config = API_CONFIGS["student_model"]
    
    print("=" * 100)
    print("ğŸ“‹ æµ‹è¯•é…ç½®ä¿¡æ¯")
    print("=" * 100)
    print(f"æ•°æ®é›†è·¯å¾„: {data_path}")
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {num_samples if num_samples else 'å…¨éƒ¨'}")
    print(f"è¾“å‡ºè·¯å¾„: {output_path}")
    print()
    if critic_config.get("type") == "local":
        print(f"Criticæ¨¡å‹: {critic_config['name']} (æœ¬åœ°) @ {critic_config['model_path']}")
    else:
        print(f"Criticæ¨¡å‹: {critic_config['name']} @ {critic_config['url']}")
    print(f"Studentæ¨¡å‹: {student_config['name']} @ {student_config['url']}")
    print(f"Teacherå“åº”: ä»æ•°æ®é›†è¯»å–")
    print("=" * 100)
    print()
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = ModelTester(
        critic_config=critic_config,
        student_config=student_config
    )
    
    # è¿è¡Œæµ‹è¯•
    results_df = tester.test_dataset(
        data_path=data_path,
        num_samples=num_samples,
        output_path=output_path,
        random_sample=TEST_CONFIG.get("random_sample", False),
        random_seed=TEST_CONFIG.get("random_seed", 42)
    )
    
    print("âœ… æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()
