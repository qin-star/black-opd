"""
æµ‹è¯•Criticæ¨¡å‹ - å®Œå…¨æ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„batchç»“æ„

å…³é”®æ”¹è¿›ï¼š
1. ä½¿ç”¨trainæ¨¡å¼ï¼ˆè€Œéevalæ¨¡å¼ï¼‰
2. ä½¿ç”¨ä¸è®­ç»ƒå®Œå…¨ç›¸åŒçš„batchç»“æ„ï¼ˆteacherså’Œstudentsåœ¨åŒä¸€ä¸ªbatchä¸­ï¼‰
3. æ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„n_resp_per_prompt=4é…ç½®
"""

import sys
sys.path.append('/home/jovyan/JQ/gad_gspo_B300/tools')

from test_critic_simplified import CriticTester, API_CONFIGS, TEST_CONFIG
import pandas as pd
import torch
from tqdm import tqdm
import time


class CriticTesterWithTrainingStructure(CriticTester):
    """ä½¿ç”¨è®­ç»ƒæ—¶batchç»“æ„çš„Criticæµ‹è¯•å™¨"""
    
    def load_critic_model(self, config):
        """åŠ è½½Criticæ¨¡å‹ï¼ˆä½¿ç”¨trainæ¨¡å¼ï¼‰"""
        from trl import AutoModelForCausalLMWithValueHead
        from transformers import AutoModelForCausalLM, AutoTokenizer
        
        device = config.get("device", "cuda:0")
        model_path = config["model_path"]
        
        print(f"ğŸ”„ åŠ è½½tokenizer...")
        self.critic_tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        
        print(f"ğŸ”„ åŠ è½½æ¨¡å‹...")
        base_model = AutoModelForCausalLM.from_pretrained(
            model_path,
            trust_remote_code=True,
            torch_dtype=torch.float16,
            device_map=device
        )
        
        self.critic_model = AutoModelForCausalLMWithValueHead.from_pretrained(base_model)
        
        # ğŸ”§ å…³é”®ï¼šä½¿ç”¨trainæ¨¡å¼ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        self.critic_model.train()
        print(f"âš ï¸  ä½¿ç”¨trainæ¨¡å¼ï¼ˆä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰")
    
    def test_with_training_batch_structure(
        self, 
        data_path: str, 
        num_samples: int = 100,
        n_resp_per_prompt: int = 4,  # ä¸è®­ç»ƒé…ç½®ä¸€è‡´
        batch_size: int = 32  # æ¯ä¸ªbatchåŒ…å«å¤šå°‘ä¸ªprompts
    ):
        """
        ä½¿ç”¨ä¸è®­ç»ƒå®Œå…¨ç›¸åŒçš„batchç»“æ„è¿›è¡Œæµ‹è¯•
        
        è®­ç»ƒæ—¶çš„batchç»“æ„ï¼š
        - æ¯ä¸ªpromptç”Ÿæˆn_resp_per_promptä¸ªstudent responses
        - ä¸€ä¸ªbatchåŒ…å«batch_sizeä¸ªprompts
        - teacherså’Œstudentsåœ¨åŒä¸€ä¸ªbatchä¸­è¿›è¡Œæ¨ç†
        
        Args:
            data_path: æ•°æ®é›†è·¯å¾„
            num_samples: æµ‹è¯•æ ·æœ¬æ•°é‡
            n_resp_per_prompt: æ¯ä¸ªpromptç”Ÿæˆçš„student responsesæ•°é‡ï¼ˆä¸è®­ç»ƒä¸€è‡´ï¼‰
            batch_size: æ¯ä¸ªbatchåŒ…å«çš„promptsæ•°é‡
        """
        # è¯»å–æ•°æ®
        df = pd.read_parquet(data_path)
        
        # é‡‡æ ·
        if num_samples:
            df = df.sample(n=min(num_samples, len(df)), random_state=42)
        
        print(f"ğŸ“Š æµ‹è¯•é…ç½®:")
        print(f"   æ ·æœ¬æ•°: {len(df)}")
        print(f"   æ¯ä¸ªpromptçš„studentæ•°: {n_resp_per_prompt}")
        print(f"   Batchå¤§å°: {batch_size} prompts")
        print(f"   æ¯ä¸ªbatchæ€»æ ·æœ¬æ•°: {batch_size * (1 + n_resp_per_prompt)}")
        print()
        
        # å‡†å¤‡æ‰€æœ‰æ•°æ®
        all_prompts = []
        all_teacher_responses = []
        
        for idx, row in df.iterrows():
            try:
                content = row['content']
                if isinstance(content, (list, tuple)) and len(content) > 0:
                    prompt = content[0].get('content', '') if isinstance(content[0], dict) else str(content[0])
                else:
                    prompt = str(content)
                
                teacher_response = row.get('teacher_response', '')
                
                if not prompt or not teacher_response:
                    continue
                
                all_prompts.append(prompt)
                all_teacher_responses.append(teacher_response)
            except Exception as e:
                print(f"âŒ è§£ææ ·æœ¬å¤±è´¥: {e}")
                continue
        
        print(f"ğŸ“ å‡†å¤‡å®Œæˆï¼Œå…± {len(all_prompts)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
        print(f"ğŸ”„ å¼€å§‹ç”ŸæˆStudent responses...")
        
        # ç”Ÿæˆæ‰€æœ‰Student responses
        all_student_responses = []
        for prompt in tqdm(all_prompts, desc="ç”Ÿæˆresponses"):
            student_responses_for_prompt = []
            for _ in range(n_resp_per_prompt):
                response = self.call_student_api(prompt, debug=False)
                if response:
                    student_responses_for_prompt.append(response)
                time.sleep(0.1)
            all_student_responses.append(student_responses_for_prompt)
        
        print(f"âœ… Student responsesç”Ÿæˆå®Œæˆ")
        print(f"ğŸ”„ å¼€å§‹Batchæ¨ç†ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„batchç»“æ„ï¼‰...")
        
        # ä½¿ç”¨è®­ç»ƒæ—¶çš„batchç»“æ„è¿›è¡Œæ¨ç†
        results = []
        total_correct = 0
        total_comparisons = 0
        
        all_teacher_scores = []
        all_student_scores = []
        
        for batch_start in tqdm(range(0, len(all_prompts), batch_size), desc="Batchæ¨ç†"):
            batch_end = min(batch_start + batch_size, len(all_prompts))
            
            batch_prompts = all_prompts[batch_start:batch_end]
            batch_teacher_responses = all_teacher_responses[batch_start:batch_end]
            batch_student_responses = all_student_responses[batch_start:batch_end]
            
            # ğŸ”§ å…³é”®ï¼šæ„å»ºè®­ç»ƒæ—¶çš„batchç»“æ„
            # æ ¼å¼ï¼š[teacher_1, ..., teacher_N, student_1a, student_1b, ..., student_Na, ...]
            
            mixed_prompts = []
            mixed_responses = []
            teacher_indices = []
            student_indices_map = {}
            
            current_idx = 0
            
            # å…ˆæ·»åŠ æ‰€æœ‰teachers
            for i, (prompt, teacher_resp) in enumerate(zip(batch_prompts, batch_teacher_responses)):
                mixed_prompts.append(prompt)
                mixed_responses.append(teacher_resp)
                teacher_indices.append(current_idx)
                current_idx += 1
            
            # å†æ·»åŠ æ‰€æœ‰students
            for i, (prompt, student_resps) in enumerate(zip(batch_prompts, batch_student_responses)):
                student_start_idx = current_idx
                for student_resp in student_resps:
                    mixed_prompts.append(prompt)
                    mixed_responses.append(student_resp)
                    current_idx += 1
                student_indices_map[i] = list(range(student_start_idx, current_idx))
            
            # Batchæ¨ç†ï¼ˆæ‰€æœ‰teacherså’Œstudentsåœ¨åŒä¸€ä¸ªbatchä¸­ï¼‰
            all_scores, all_lengths = self.get_critic_scores_batch(
                mixed_prompts, mixed_responses
            )
            
            # åˆ†ç¦»teacherå’Œstudentåˆ†æ•°
            for i in range(len(batch_prompts)):
                teacher_score = all_scores[teacher_indices[i]]
                student_score_indices = student_indices_map[i]
                student_scores = [all_scores[idx] for idx in student_score_indices]
                
                # ç»Ÿè®¡
                all_teacher_scores.append(teacher_score)
                all_student_scores.extend(student_scores)
                
                # è®¡ç®—å‡†ç¡®ç‡
                correct = sum(1 for s in student_scores if s <= teacher_score)
                total_correct += correct
                total_comparisons += len(student_scores)
                
                results.append({
                    'teacher_score': teacher_score,
                    'student_scores': student_scores,
                    'correct': correct,
                    'total': len(student_scores),
                })
        
        # æ‰“å°ç»Ÿè®¡
        print()
        print("=" * 100)
        print("ğŸ“ˆ æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„batchç»“æ„ï¼‰")
        print("=" * 100)
        
        print(f"\næ€»æ ·æœ¬æ•°: {len(results)}")
        print(f"æ€»Studentå“åº”æ•°: {total_comparisons}")
        print(f"æ€»ä½“å‡†ç¡®ç‡: {total_correct}/{total_comparisons} ({total_correct/total_comparisons*100:.2f}%)")
        print()
        
        print("Teacheræ¨¡å‹ç»Ÿè®¡:")
        teacher_mean = sum(all_teacher_scores) / len(all_teacher_scores)
        teacher_std = pd.Series(all_teacher_scores).std()
        print(f"  å¹³å‡åˆ†æ•°: {teacher_mean:.4f}")
        print(f"  åˆ†æ•°æ ‡å‡†å·®: {teacher_std:.4f}")
        print(f"  åˆ†æ•°èŒƒå›´: [{min(all_teacher_scores):.4f}, {max(all_teacher_scores):.4f}]")
        print()
        
        print("Studentæ¨¡å‹ç»Ÿè®¡:")
        student_mean = sum(all_student_scores) / len(all_student_scores)
        student_std = pd.Series(all_student_scores).std()
        print(f"  å¹³å‡åˆ†æ•°: {student_mean:.4f}")
        print(f"  åˆ†æ•°æ ‡å‡†å·®: {student_std:.4f}")
        print(f"  åˆ†æ•°èŒƒå›´: [{min(all_student_scores):.4f}, {max(all_student_scores):.4f}]")
        print()
        
        print("åˆ†æ•°å·®å¼‚:")
        score_diff = teacher_mean - student_mean
        print(f"  Teacher - Student: {score_diff:.4f}")
        print()
        
        print("âœ… ä½¿ç”¨trainæ¨¡å¼ + è®­ç»ƒæ—¶çš„batchç»“æ„")
        print("=" * 100)
        
        return results


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="æµ‹è¯•Criticæ¨¡å‹ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„batchç»“æ„ï¼‰")
    parser.add_argument("--data_path", type=str, 
                       default="/home/jovyan/JQ/gad_gspo_B300/data/trainning_dataset/subject_1-29/merged/merge-1-29.parquet")
    parser.add_argument("--num_samples", type=int, default=100)
    parser.add_argument("--n_resp_per_prompt", type=int, default=4,
                       help="æ¯ä¸ªpromptç”Ÿæˆçš„student responsesæ•°é‡ï¼ˆä¸è®­ç»ƒé…ç½®ä¸€è‡´ï¼‰")
    parser.add_argument("--batch_size", type=int, default=32,
                       help="æ¯ä¸ªbatchåŒ…å«çš„promptsæ•°é‡")
    
    args = parser.parse_args()
    
    print("=" * 100)
    print("ğŸ“‹ æµ‹è¯•é…ç½®ä¿¡æ¯ï¼ˆå®Œå…¨æ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„batchç»“æ„ï¼‰")
    print("=" * 100)
    print(f"æ•°æ®é›†è·¯å¾„: {args.data_path}")
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {args.num_samples}")
    print(f"æ¯ä¸ªpromptçš„Studentæ ·æœ¬æ•°: {args.n_resp_per_prompt}")
    print(f"Batchå¤§å°: {args.batch_size} prompts")
    print(f"Criticæ¨¡å‹: {API_CONFIGS['critic_model']['model_path']}")
    print(f"Studentæ¨¡å‹: {API_CONFIGS['student_model']['url']}")
    print()
    print("å…³é”®æ”¹è¿›:")
    print("  1. âœ… ä½¿ç”¨trainæ¨¡å¼ï¼ˆè€Œéevalæ¨¡å¼ï¼‰")
    print("  2. âœ… Teacherså’ŒStudentsåœ¨åŒä¸€ä¸ªbatchä¸­æ¨ç†")
    print("  3. âœ… æ¨¡æ‹Ÿè®­ç»ƒæ—¶çš„n_resp_per_prompt=4é…ç½®")
    print("=" * 100)
    print()
    
    tester = CriticTesterWithTrainingStructure(
        critic_config=API_CONFIGS["critic_model"],
        student_config=API_CONFIGS["student_model"],
        test_config=TEST_CONFIG
    )
    
    tester.test_with_training_batch_structure(
        data_path=args.data_path,
        num_samples=args.num_samples,
        n_resp_per_prompt=args.n_resp_per_prompt,
        batch_size=args.batch_size
    )
    
    print("âœ… æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()
