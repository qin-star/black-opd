"""
æµ‹è¯•è®­ç»ƒåçš„Criticå’ŒStudentæ¨¡å‹ - è®­ç»ƒæ¨¡å¼
å®Œå…¨å¤ç°è®­ç»ƒæ—¶çš„åˆ†æ•°è®¡ç®—æ–¹å¼ï¼ˆåŒ…æ‹¬æ··åˆå½’ä¸€åŒ–ã€batch normalizationã€temperatureç¼©æ”¾ï¼‰

ä¸ test_trained_models.py çš„åŒºåˆ«ï¼š
- test_trained_models.py: ä½¿ç”¨åŸå§‹çš„value headè¾“å‡ºï¼ˆæ¨¡å‹çœŸå®èƒ½åŠ›ï¼‰
- æœ¬è„šæœ¬: ä½¿ç”¨è®­ç»ƒæ—¶çš„å½’ä¸€åŒ–å’Œç¼©æ”¾ï¼ˆå¤ç°è®­ç»ƒæ—¥å¿—ä¸­çš„åˆ†æ•°ï¼‰
"""
import requests
import pandas as pd
import argparse
from typing import Dict, Optional, List
from datetime import datetime
from tqdm import tqdm
import time
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM


# ==================== é…ç½®åŒºåŸŸ ====================
# åœ¨è¿™é‡Œä¿®æ”¹ä½ çš„æ¨¡å‹é…ç½®å’Œæµ‹è¯•å‚æ•°

# æµ‹è¯•å‚æ•°é…ç½®
TEST_CONFIG = {
    "data_path": "/home/jovyan/JQ/gad_gspo_B300/data/trainning_dataset/subject_1-29/merged/merge-1-29.parquet",
    "num_samples": 10,  # æµ‹è¯•æ ·æœ¬æ•°é‡ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰
    "random_sample": True,  # æ˜¯å¦éšæœºæŠ½æ ·
    "random_seed": 42,  # éšæœºç§å­
    "output_dir": "/home/jovyan/JQ/gad_gspo_B300/outputs",
    "output_filename": None,  # Noneè¡¨ç¤ºè‡ªåŠ¨ç”Ÿæˆ
    
    # å¤šæ ·æœ¬ç”Ÿæˆé…ç½®
    "num_student_samples": 8,  # æ¯ä¸ªpromptç”Ÿæˆå¤šå°‘ä¸ªstudent response
    "student_temperature": 0.8,  # Studentç”Ÿæˆçš„temperatureï¼ˆå¢åŠ å¤šæ ·æ€§ï¼‰
    
    # è®­ç»ƒæ¨¡å¼é…ç½®ï¼ˆä¸ç®€åŒ–åçš„è®­ç»ƒä»£ç ä¿æŒä¸€è‡´ï¼‰
    # æ³¨æ„ï¼šè®­ç»ƒä»£ç å·²ç®€åŒ–ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹åˆ†æ•°ï¼ˆsumï¼‰ï¼Œä¸å†ä½¿ç”¨æ··åˆåˆ†æ•°å’Œbatch norm
    "use_raw_score_only": True,  # âœ… åªä½¿ç”¨åŸå§‹åˆ†æ•°ï¼ˆsumï¼‰ï¼Œä¸è®­ç»ƒä»£ç ä¸€è‡´
    "temperature": 5.0,  # Temperatureç¼©æ”¾ç³»æ•°ï¼ˆä»…ç”¨äºlossè®¡ç®—ï¼Œä¸å½±å“åˆ†æ•°æœ¬èº«ï¼‰
}

# æ¨¡å‹é…ç½®
API_CONFIGS = {
    "critic_model": {
        "name": "critic-model",
        "type": "local",
        "model_path": "/home/jovyan/JQ/gad_gspo_B300/models/opd-v9-1-29-fsdp2/global_step_500/critic_merged",
        "device": "cuda:4",
        "temperature": 0.0,
        "repetition_penalty": 1.0,
        "force_trl": True,
    },
    "student_model": {
        "name": "student-model",
        "type": "api",
        "url": "http://10.72.1.39:8008/v1/chat/completions",
        "api_key": "sk-xxxx",
        "model_name": "opd-v9-500",
        "temperature": 0.8,  # æé«˜temperatureä»¥å¢åŠ å¤šæ ·æ€§
        "repetition_penalty": 1.2
    }
}
# ================================================


class ModelTesterTrainingMode:
    """
    è®­ç»ƒæ¨¡å¼çš„æ¨¡å‹æµ‹è¯•å™¨
    å®Œå…¨å¤ç°è®­ç»ƒæ—¶çš„åˆ†æ•°è®¡ç®—é€»è¾‘
    """
    def __init__(self, critic_config: Dict, student_config: Dict, training_config: Dict):
        self.critic_config = critic_config
        self.student_config = student_config
        self.training_config = training_config
        
        # åŠ è½½æœ¬åœ°Criticæ¨¡å‹
        self.critic_model = None
        self.critic_tokenizer = None
        if critic_config.get("type") == "local":
            print(f"ğŸ”„ åŠ è½½æœ¬åœ°Criticæ¨¡å‹: {critic_config['model_path']}")
            self.load_local_critic(critic_config)
            print(f"âœ… Criticæ¨¡å‹åŠ è½½å®Œæˆ")
    
    def load_local_critic(self, config: Dict):
        """åŠ è½½æœ¬åœ°Criticæ¨¡å‹"""
        device = config.get("device", "cuda:0")
        model_path = config["model_path"]
        force_trl = config.get("force_trl", False)
        
        print(f"ğŸ”„ åŠ è½½tokenizer...")
        self.critic_tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True
        )
        print(f"âœ… TokenizeråŠ è½½å®Œæˆ")
        
        if force_trl:
            print(f"âš¡ é…ç½®å¼ºåˆ¶ä½¿ç”¨trlï¼Œè·³è¿‡TokenClassificationå°è¯•")
            self._load_with_trl(model_path, device)
            return
        
        from transformers import AutoModelForTokenClassification, AutoConfig
        
        print(f"ğŸ” æ£€æŸ¥æ¨¡å‹é…ç½®...")
        model_config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)
        print(f"  æ¶æ„: {model_config.architectures}")
        
        is_token_classification = any('TokenClassification' in arch for arch in model_config.architectures)
        
        if is_token_classification:
            print(f"ğŸ”„ ä½¿ç”¨ AutoModelForTokenClassification åŠ è½½...")
            try:
                self.critic_model = AutoModelForTokenClassification.from_pretrained(
                    model_path,
                    trust_remote_code=True,
                    torch_dtype=torch.float16,
                    device_map=device
                )
                print(f"âœ… æˆåŠŸä½¿ç”¨ AutoModelForTokenClassification åŠ è½½")
                self.critic_model.eval()
                return
            except Exception as e:
                print(f"âŒ AutoModelForTokenClassification åŠ è½½å¤±è´¥: {e}")
                raise
        
        print(f"ğŸ”„ æ¨¡å‹ä¸æ˜¯TokenClassificationç±»å‹ï¼Œä½¿ç”¨trlåŠ è½½...")
        self._load_with_trl(model_path, device)
    
    def _load_with_trl(self, model_path: str, device: str):
        """ä½¿ç”¨trlåŠ è½½æ¨¡å‹"""
        try:
            from trl import AutoModelForCausalLMWithValueHead
            from transformers import AutoModelForCausalLM
            
            print(f"  åŠ è½½åŸºç¡€CausalLMæ¨¡å‹...")
            base_model = AutoModelForCausalLM.from_pretrained(
                model_path,
                trust_remote_code=True,
                torch_dtype=torch.float16,
                device_map=device
            )
            
            print(f"  åŒ…è£…ä¸ºValueHeadæ¨¡å‹...")
            self.critic_model = AutoModelForCausalLMWithValueHead.from_pretrained(base_model)
            print(f"âœ… æˆåŠŸä½¿ç”¨ trl.AutoModelForCausalLMWithValueHead åŠ è½½")
            self.critic_model.eval()
        except Exception as e:
            print(f"âŒ trlåŠ è½½å¤±è´¥: {e}")
            raise RuntimeError(f"æ— æ³•åŠ è½½Criticæ¨¡å‹: {e}")

    
    def call_generation_model(self, config: Dict, prompt: str, max_tokens: int = 512) -> Dict:
        """è°ƒç”¨ç”Ÿæˆæ¨¡å‹API"""
        try:
            messages = [{"role": "user", "content": prompt}]
            
            payload = {
                "model": config["model_name"],
                "messages": messages,
                "max_tokens": max_tokens,
                "temperature": config.get("temperature", 0.7),
                "top_p": 0.9,
                "repetition_penalty": config.get("repetition_penalty", 1.0),
                "n": 1
            }
            
            headers = {
                "Authorization": f"Bearer {config['api_key']}",
                "Content-Type": "application/json"
            }
            
            response = requests.post(
                config["url"],
                json=payload,
                headers=headers,
                timeout=60
            )
            response.raise_for_status()
            
            result = response.json()
            generated_text = result['choices'][0]['message']['content']
            usage = result.get('usage', {})
            tokens = usage.get('completion_tokens', len(generated_text.split()))
            
            return {
                'text': generated_text.strip(),
                'length': tokens
            }
        except Exception as e:
            print(f"âŒ ç”Ÿæˆæ¨¡å‹è°ƒç”¨å¤±è´¥ [{config['name']}]: {e}")
            return {'text': '', 'length': 0}
    
    def get_raw_values(self, prompt: str, response: str) -> tuple:
        """
        è·å–åŸå§‹çš„value headè¾“å‡ºï¼ˆæœªç»å½’ä¸€åŒ–å’Œç¼©æ”¾ï¼‰
        
        Returns:
            values: åŸå§‹values tensor, shape (1, response_length)
            response_mask: response mask tensor, shape (1, response_length)
            response_length: responseé•¿åº¦
        """
        try:
            messages = [
                {"role": "user", "content": prompt},
                {"role": "assistant", "content": response}
            ]
            
            if hasattr(self.critic_tokenizer, 'apply_chat_template'):
                input_text = self.critic_tokenizer.apply_chat_template(
                    messages,
                    tokenize=False,
                    add_generation_prompt=False
                )
            else:
                input_text = f"User: {prompt}\nAssistant: {response}"
            
            inputs = self.critic_tokenizer(
                input_text,
                return_tensors="pt",
                truncation=True,
                max_length=2048
            )
            
            # è·å–è®¾å¤‡
            if hasattr(self.critic_model, 'device'):
                device = self.critic_model.device
            elif hasattr(self.critic_model, 'pretrained_model'):
                device = next(self.critic_model.pretrained_model.parameters()).device
            else:
                device = next(self.critic_model.parameters()).device
            
            inputs = inputs.to(device)
            input_ids = inputs['input_ids']
            attention_mask = inputs['attention_mask']
            
            # å‰å‘ä¼ æ’­
            with torch.no_grad():
                outputs = self.critic_model(**inputs, use_cache=False)
                
                # è·å–values
                if hasattr(outputs, 'logits'):
                    # TokenClassificationæ¨¡å‹
                    response_tokens = self.critic_tokenizer(
                        response,
                        add_special_tokens=False,
                        return_tensors="pt"
                    )
                    response_length = response_tokens['input_ids'].size(1)
                    values = outputs.logits[:, -response_length:]
                    if values.dim() == 3:
                        values = values.squeeze(-1)
                elif hasattr(self.critic_model, 'v_head') or isinstance(outputs, tuple):
                    # trlæ¨¡å‹
                    if isinstance(outputs, tuple) and len(outputs) > 2:
                        response_tokens = self.critic_tokenizer(
                            response,
                            add_special_tokens=False,
                            return_tensors="pt"
                        )
                        response_length = response_tokens['input_ids'].size(1)
                        values = outputs[2][:, -response_length:]
                        if values.dim() == 3:
                            values = values.squeeze(-1)
                    else:
                        raise ValueError("æ¨¡å‹è¿”å›tupleä½†æ ¼å¼ä¸æ­£ç¡®")
                else:
                    raise ValueError(f"æ— æ³•è¯†åˆ«çš„æ¨¡å‹è¾“å‡ºæ ¼å¼: {type(outputs)}")
            
            response_mask = attention_mask[:, -response_length:]
            response_ids = input_ids[:, -response_length:]
            
            # æ’é™¤EOS token
            eos_token_id = self.critic_tokenizer.eos_token_id
            is_eos = (response_ids == eos_token_id)
            response_mask_no_eos = response_mask & (~is_eos)
            
            return values, response_mask_no_eos, response_length
            
        except Exception as e:
            print(f"âŒ è·å–åŸå§‹valueså¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None, None, 0

    
    def compute_training_mode_scores_batch(self,
                                           teacher_values: torch.Tensor,
                                           student_values_list: List[torch.Tensor],
                                           teacher_mask: torch.Tensor,
                                           student_masks_list: List[torch.Tensor]) -> Dict:
        """
        ä½¿ç”¨è®­ç»ƒæ—¶çš„æ–¹å¼è®¡ç®—åˆ†æ•°ï¼ˆæ‰¹é‡ç‰ˆæœ¬ - ç®€åŒ–ç‰ˆï¼‰
        å®Œå…¨å¤ç°ç®€åŒ–åçš„ compute_discriminator_loss ä¸­çš„åˆ†æ•°è®¡ç®—é€»è¾‘
        
        è®­ç»ƒä»£ç å·²ç®€åŒ–ï¼š
        - ç›´æ¥ä½¿ç”¨åŸå§‹åˆ†æ•°ï¼ˆsumï¼‰ï¼Œä¸å†ä½¿ç”¨æ··åˆåˆ†æ•°
        - ä¸å†ä½¿ç”¨batch normalization
        - åˆ†æ•°å·®å¼‚ç›´æ¥è®¡ç®—ï¼Œtemperatureä»…ç”¨äºloss
        
        Args:
            teacher_values: shape (1, response_length)
            student_values_list: list of tensors, each shape (1, response_length)
            teacher_mask: shape (1, response_length)
            student_masks_list: list of tensors, each shape (1, response_length)
        
        Returns:
            åŒ…å«å„ç§åˆ†æ•°çš„å­—å…¸
        """
        # ==============================
        # 1. è®¡ç®—åŸå§‹åˆ†æ•°ï¼ˆsumï¼‰- ä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´
        # ==============================
        eps = 1e-8
        
        # Teacheråˆ†æ•°ï¼ˆåŸå§‹sumï¼‰
        teacher_score_raw = torch.sum(teacher_values * teacher_mask, dim=-1)
        teacher_mask_sum = teacher_mask.sum(dim=-1).clamp(min=eps)
        
        # å½’ä¸€åŒ–åˆ†æ•°ï¼ˆä»…ç”¨äºå‚è€ƒï¼Œä¸ç”¨äºåˆ¤æ–­ï¼‰
        teacher_score_norm = teacher_score_raw / teacher_mask_sum
        
        # Studentåˆ†æ•°ï¼ˆå¤šä¸ªï¼‰
        student_scores_raw = []
        student_scores_norm = []
        student_lengths = []
        
        for student_values, student_mask in zip(student_values_list, student_masks_list):
            score_raw = torch.sum(student_values * student_mask, dim=-1)
            mask_sum = student_mask.sum(dim=-1).clamp(min=eps)
            score_norm = score_raw / mask_sum
            
            student_scores_raw.append(score_raw)
            student_scores_norm.append(score_norm)
            student_lengths.append(mask_sum)
        
        # è½¬æ¢ä¸ºtensor
        student_scores_raw = torch.stack(student_scores_raw)  # (num_students,)
        student_scores_norm = torch.stack(student_scores_norm)  # (num_students,)
        student_lengths = torch.stack(student_lengths)  # (num_students,)
        
        # ==============================
        # 2. ä½¿ç”¨åŸå§‹åˆ†æ•°ï¼ˆä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰
        # ==============================
        # è®­ç»ƒä»£ç ç®€åŒ–ï¼šteacher_score = teacher_score_raw
        teacher_score = teacher_score_raw
        student_scores = student_scores_raw
        
        # ==============================
        # 3. è®¡ç®—åˆ†æ•°å·®å¼‚ï¼ˆç›´æ¥è®¡ç®—ï¼Œä¸ä½¿ç”¨batch normï¼‰
        # ==============================
        # è®­ç»ƒä»£ç ï¼šdiff = teacher_score - student_score
        diffs = student_scores - teacher_score  # æ³¨æ„ï¼šè¿™é‡Œæ˜¯student - teacher
        
        # Temperatureç¼©æ”¾ï¼ˆä»…ç”¨äºlossè®¡ç®—ï¼Œä¸å½±å“åˆ¤æ–­ï¼‰
        temperature = self.training_config.get("temperature", 5.0)
        diffs_scaled = diffs / temperature
        
        # ==============================
        # 4. è¿”å›æ‰€æœ‰åˆ†æ•°ä¿¡æ¯
        # ==============================
        return {
            # Teacheråˆ†æ•°
            "teacher_score_raw": teacher_score_raw.item(),
            "teacher_score_norm": teacher_score_norm.item(),
            "teacher_score": teacher_score.item(),  # æœ€ç»ˆä½¿ç”¨çš„åˆ†æ•°
            "teacher_length": teacher_mask_sum.item(),
            
            # Studentåˆ†æ•°ï¼ˆåˆ—è¡¨ï¼‰
            "student_scores_raw": student_scores_raw.cpu().tolist(),
            "student_scores_norm": student_scores_norm.cpu().tolist(),
            "student_scores": student_scores.cpu().tolist(),  # æœ€ç»ˆä½¿ç”¨çš„åˆ†æ•°
            "student_lengths": student_lengths.cpu().tolist(),
            
            # åˆ†æ•°å·®å¼‚ï¼ˆåˆ—è¡¨ï¼‰- æ³¨æ„ï¼šè´Ÿå€¼è¡¨ç¤ºstudent < teacherï¼ˆæ­£ç¡®ï¼‰
            "score_diffs": diffs.cpu().tolist(),
            "score_diffs_scaled": diffs_scaled.cpu().tolist(),
            
            # ç»Ÿè®¡ä¿¡æ¯
            "student_score_mean": student_scores.mean().item(),
            "student_score_norm_mean": student_scores_norm.mean().item(),
            "student_length_mean": student_lengths.mean().item(),
            
            "score_diff_mean": diffs.mean().item(),
            "score_diff_scaled_mean": diffs_scaled.mean().item(),
            
            # é…ç½®ä¿¡æ¯
            "temperature": temperature,
            "use_raw_score_only": True,
            "num_students": len(student_values_list),
        }

    
    def test_single_sample(self, prompt: str, teacher_response: str) -> Dict:
        """æµ‹è¯•å•ä¸ªæ ·æœ¬ï¼ˆç”Ÿæˆå¤šä¸ªstudent responsesï¼‰"""
        print("ğŸ“ Prompt:")
        print(f"  {prompt}")
        print()
        
        num_students = self.training_config.get("num_student_samples", 8)
        
        # ç”Ÿæˆå¤šä¸ªStudentå“åº”
        print(f"ğŸ”„ ç”Ÿæˆ {num_students} ä¸ªStudentå“åº”...")
        student_results = []
        for i in range(num_students):
            result = self.call_generation_model(self.student_config, prompt)
            student_results.append(result)
            print(f"  âœ“ Student #{i+1}: {len(result['text'])} chars")
        print()
        
        # è·å–teacherçš„åŸå§‹values
        teacher_values, teacher_mask, teacher_length = self.get_raw_values(prompt, teacher_response)
        
        if teacher_values is None:
            print("âŒ æ— æ³•è·å–teacher valuesï¼Œè·³è¿‡æ­¤æ ·æœ¬")
            return None
        
        # è·å–æ‰€æœ‰studentçš„åŸå§‹values
        student_values_list = []
        student_masks_list = []
        
        for i, result in enumerate(student_results):
            values, mask, length = self.get_raw_values(prompt, result['text'])
            if values is None:
                print(f"âŒ æ— æ³•è·å–Student #{i+1} valuesï¼Œè·³è¿‡")
                continue
            student_values_list.append(values)
            student_masks_list.append(mask)
        
        if len(student_values_list) == 0:
            print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„student valuesï¼Œè·³è¿‡æ­¤æ ·æœ¬")
            return None
        
        # è®¡ç®—è®­ç»ƒæ¨¡å¼åˆ†æ•°ï¼ˆæ‰¹é‡ç‰ˆæœ¬ï¼‰
        scores = self.compute_training_mode_scores_batch(
            teacher_values, student_values_list,
            teacher_mask, student_masks_list
        )
        
        # æ‰“å°ç»“æœ
        print("ï¿½â€ğŸ« Teacher Response:")
        print("-" * 100)
        print(f"  åŸå§‹åˆ†æ•°(sum):     {scores['teacher_score_raw']:7.4f}")
        print(f"  å½’ä¸€åŒ–åˆ†æ•°(mean):  {scores['teacher_score_norm']:7.4f}")
        print(f"  æ··åˆåˆ†æ•°(70/30):   {scores['teacher_score_mixed']:7.4f}")
        print(f"  æœ€ç»ˆåˆ†æ•°(+norm):   {scores['teacher_score_final']:7.4f}")
        print(f"  Length: {int(scores['teacher_length']):3d}")
        print(f"  Text (å®Œæ•´): {teacher_response}")
        print()
        
        print(f"ğŸ“ Student Responses ({len(student_results)} ä¸ª):")
        print("-" * 100)
        
        # æ˜¾ç¤ºæ¯ä¸ªstudent responseåŠå…¶åˆ†æ•°
        for i in range(len(student_values_list)):
            print(f"\n  Student #{i+1}:")
            print(f"    åŸå§‹åˆ†æ•°(sum):     {scores['student_scores_raw'][i]:7.4f}")
            print(f"    å½’ä¸€åŒ–åˆ†æ•°(mean):  {scores['student_scores_norm'][i]:7.4f}")
            print(f"    æ··åˆåˆ†æ•°(70/30):   {scores['student_scores_mixed'][i]:7.4f}")
            print(f"    æœ€ç»ˆåˆ†æ•°(+norm):   {scores['student_scores_final'][i]:7.4f}")
            print(f"    Length: {int(scores['student_lengths'][i]):3d}")
            print(f"    ä¸Teacheråˆ†å·®(æ··åˆ): {scores['score_diffs_mixed'][i]:7.4f}")
            print(f"    ä¸Teacheråˆ†å·®(æœ€ç»ˆ): {scores['score_diffs_final'][i]:7.4f}")
            print(f"    Text (å®Œæ•´): {student_results[i]['text']}")
        
        print()
        print("ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  Studentå¹³å‡åˆ†æ•°(æ··åˆ): {scores['student_score_mixed_mean']:7.4f}")
        print(f"  Studentå¹³å‡åˆ†æ•°(æœ€ç»ˆ): {scores['student_score_final_mean']:7.4f}")
        print(f"  å¹³å‡åˆ†å·®(æ··åˆ): {scores['score_diff_mixed_mean']:7.4f}")
        print(f"  å¹³å‡åˆ†å·®(æœ€ç»ˆ): {scores['score_diff_final_mean']:7.4f}")
        print(f"  å¹³å‡åˆ†å·®(ç¼©æ”¾): {scores['score_diff_scaled_mean']:7.4f} (temperature={scores['temperature']})")
        print()
        
        # åˆ¤æ–­å‡†ç¡®æ€§
        # ä½¿ç”¨æ··åˆåˆ†æ•°ï¼šæœ‰å¤šå°‘ä¸ªstudentçš„åˆ†æ•°ä½äºteacher
        correct_count_mixed = sum(1 for diff in scores['score_diffs_mixed'] if diff < 0)
        correct_count_final = sum(1 for diff in scores['score_diffs_final'] if diff < 0)
        
        print(f"  Teacher > Student çš„æ¯”ä¾‹:")
        print(f"    æ··åˆåˆ†æ•°: {correct_count_mixed}/{len(student_values_list)} ({correct_count_mixed/len(student_values_list)*100:.1f}%)")
        print(f"    æœ€ç»ˆåˆ†æ•°: {correct_count_final}/{len(student_values_list)} ({correct_count_final/len(student_values_list)*100:.1f}%)")
        
        if scores.get('use_batch_norm'):
            print(f"\n  Batch Normalization:")
            print(f"    Batch size: {scores.get('batch_size', 'N/A')}")
            print(f"    Batch mean: {scores.get('batch_mean', 0):.4f}")
            print(f"    Batch std:  {scores.get('batch_std', 0):.4f}")
        
        print()
        print("=" * 100)
        print()
        
        # ä¿å­˜ç»“æœ
        result = {
            'prompt': prompt,
            'teacher_response': teacher_response,
            'num_students': len(student_values_list),
            'correct_count_mixed': correct_count_mixed,
            'correct_count_final': correct_count_final,
            'accuracy_mixed': correct_count_mixed / len(student_values_list),
            'accuracy_final': correct_count_final / len(student_values_list),
            **scores
        }
        
        # æ·»åŠ æ¯ä¸ªstudentçš„responseæ–‡æœ¬
        for i, student_result in enumerate(student_results[:len(student_values_list)]):
            result[f'student_{i+1}_response'] = student_result['text']
        
        return result

    
    def test_dataset(self, data_path: str, num_samples: Optional[int] = None,
                    output_path: Optional[str] = None, random_sample: bool = False,
                    random_seed: int = 42) -> pd.DataFrame:
        """æµ‹è¯•æ•°æ®é›†"""
        # è¯»å–æ•°æ®
        if data_path.endswith('.parquet'):
            df = pd.read_parquet(data_path)
        elif data_path.endswith('.xlsx'):
            df = pd.read_excel(data_path)
        else:
            raise ValueError("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œä»…æ”¯æŒ.parquetå’Œ.xlsx")
        
        # é™åˆ¶æµ‹è¯•æ•°é‡
        if num_samples is not None:
            if random_sample:
                df = df.sample(n=min(num_samples, len(df)), random_state=random_seed)
                print(f"ğŸ“Š éšæœºæŠ½æ · {len(df)} ä¸ªæ ·æœ¬ï¼ˆç§å­={random_seed}ï¼‰...")
            else:
                df = df.head(num_samples)
                print(f"ğŸ“Š å–å‰ {len(df)} ä¸ªæ ·æœ¬...")
        else:
            print(f"ğŸ“Š æµ‹è¯•å…¨éƒ¨ {len(df)} ä¸ªæ ·æœ¬...")
        print()
        
        results = []
        for idx, row in tqdm(df.iterrows(), total=len(df), desc="æµ‹è¯•è¿›åº¦"):
            try:
                # è§£æcontentå­—æ®µ
                if 'content' in row:
                    content = row['content']
                    if isinstance(content, (list, tuple)) and len(content) > 0:
                        prompt = content[0].get('content', '') if isinstance(content[0], dict) else str(content[0])
                    else:
                        prompt = str(content)
                elif 'prompt' in row:
                    prompt = row['prompt']
                else:
                    print(f"âš ï¸  è·³è¿‡ç¬¬ {idx} è¡Œï¼šæ— æ³•è¯†åˆ«çš„æ•°æ®æ ¼å¼")
                    continue
                
                teacher_response = row.get('teacher_response', '')
                
                if not prompt or not teacher_response:
                    print(f"âš ï¸  è·³è¿‡ç¬¬ {idx} è¡Œï¼šç¼ºå°‘promptæˆ–teacher_response")
                    continue
                
                result = self.test_single_sample(prompt, teacher_response)
                if result is not None:
                    result['sample_id'] = idx
                    result['data_id'] = row.get('id', idx)
                    results.append(result)
                
                time.sleep(0.5)
            except Exception as e:
                print(f"âŒ æµ‹è¯•ç¬¬ {idx} ä¸ªæ ·æœ¬æ—¶å‡ºé”™: {e}")
                continue
        
        # è½¬æ¢ä¸ºDataFrame
        results_df = pd.DataFrame(results)
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        self.print_statistics(results_df)
        
        # ä¿å­˜ç»“æœ
        if output_path is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"test_results_training_mode_{timestamp}.xlsx"
        
        if not output_path.endswith('.xlsx'):
            output_path = output_path + '.xlsx'
        
        import os
        output_dir = os.path.dirname(output_path)
        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir, exist_ok=True)
        
        results_df.to_excel(output_path, index=False)
        print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
        
        return results_df

    
    def print_statistics(self, results_df: pd.DataFrame):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print()
        print("=" * 100)
        print("ğŸ“ˆ æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯ï¼ˆè®­ç»ƒæ¨¡å¼ - å¤šæ ·æœ¬ï¼‰")
        print("=" * 100)
        
        total = len(results_df)
        
        if total == 0:
            print("âš ï¸  æ²¡æœ‰æˆåŠŸæµ‹è¯•çš„æ ·æœ¬")
            print("=" * 100)
            return
        
        # ç»Ÿè®¡å‡†ç¡®ç‡
        avg_accuracy_mixed = results_df['accuracy_mixed'].mean() * 100
        avg_accuracy_final = results_df['accuracy_final'].mean() * 100
        
        total_students = results_df['num_students'].sum()
        total_correct_mixed = results_df['correct_count_mixed'].sum()
        total_correct_final = results_df['correct_count_final'].sum()
        
        print(f"æ€»æ ·æœ¬æ•°: {total}")
        print(f"æ€»Studentå“åº”æ•°: {total_students}")
        print()
        print(f"å‡†ç¡®ç‡ (æ··åˆåˆ†æ•°):")
        print(f"  å¹³å‡å‡†ç¡®ç‡: {avg_accuracy_mixed:.2f}%")
        print(f"  æ€»ä½“å‡†ç¡®ç‡: {total_correct_mixed}/{total_students} ({total_correct_mixed/total_students*100:.2f}%)")
        print()
        print(f"å‡†ç¡®ç‡ (æœ€ç»ˆåˆ†æ•°):")
        print(f"  å¹³å‡å‡†ç¡®ç‡: {avg_accuracy_final:.2f}%")
        print(f"  æ€»ä½“å‡†ç¡®ç‡: {total_correct_final}/{total_students} ({total_correct_final/total_students*100:.2f}%)")
        print()
        
        # åˆ†æ•°ç»Ÿè®¡
        print("Teacheræ¨¡å‹ç»Ÿè®¡:")
        print(f"  åŸå§‹åˆ†æ•°(sum):    å‡å€¼={results_df['teacher_score_raw'].mean():.4f}, æ ‡å‡†å·®={results_df['teacher_score_raw'].std():.4f}")
        print(f"  å½’ä¸€åŒ–åˆ†æ•°(mean): å‡å€¼={results_df['teacher_score_norm'].mean():.4f}, æ ‡å‡†å·®={results_df['teacher_score_norm'].std():.4f}")
        print(f"  æ··åˆåˆ†æ•°(70/30):  å‡å€¼={results_df['teacher_score_mixed'].mean():.4f}, æ ‡å‡†å·®={results_df['teacher_score_mixed'].std():.4f}")
        print(f"  æœ€ç»ˆåˆ†æ•°(+norm):  å‡å€¼={results_df['teacher_score_final'].mean():.4f}, æ ‡å‡†å·®={results_df['teacher_score_final'].std():.4f}")
        print(f"  å¹³å‡é•¿åº¦: {results_df['teacher_length'].mean():.2f}")
        print()
        
        print("Studentæ¨¡å‹ç»Ÿè®¡ (å¹³å‡):")
        print(f"  åŸå§‹åˆ†æ•°(sum):    å‡å€¼={results_df['student_score_raw_mean'].mean():.4f}, æ ‡å‡†å·®={results_df['student_score_raw_mean'].std():.4f}")
        print(f"  å½’ä¸€åŒ–åˆ†æ•°(mean): å‡å€¼={results_df['student_score_norm_mean'].mean():.4f}, æ ‡å‡†å·®={results_df['student_score_norm_mean'].std():.4f}")
        print(f"  æ··åˆåˆ†æ•°(70/30):  å‡å€¼={results_df['student_score_mixed_mean'].mean():.4f}, æ ‡å‡†å·®={results_df['student_score_mixed_mean'].std():.4f}")
        print(f"  æœ€ç»ˆåˆ†æ•°(+norm):  å‡å€¼={results_df['student_score_final_mean'].mean():.4f}, æ ‡å‡†å·®={results_df['student_score_final_mean'].std():.4f}")
        print(f"  å¹³å‡é•¿åº¦: {results_df['student_length_mean'].mean():.2f}")
        print()
        
        print("åˆ†æ•°å·®å¼‚ç»Ÿè®¡ (å¹³å‡):")
        print(f"  æ··åˆåˆ†å·®:   å‡å€¼={results_df['score_diff_mixed_mean'].mean():.4f}, æ ‡å‡†å·®={results_df['score_diff_mixed_mean'].std():.4f}")
        print(f"  æœ€ç»ˆåˆ†å·®:   å‡å€¼={results_df['score_diff_final_mean'].mean():.4f}, æ ‡å‡†å·®={results_df['score_diff_final_mean'].std():.4f}")
        print(f"  ç¼©æ”¾åˆ†å·®:   å‡å€¼={results_df['score_diff_scaled_mean'].mean():.4f}, æ ‡å‡†å·®={results_df['score_diff_scaled_mean'].std():.4f}")
        print()
        
        # è¯Šæ–­ä¿¡æ¯
        print("ğŸ” è¯Šæ–­ä¿¡æ¯:")
        
        # æ£€æŸ¥åˆ†æ•°å…³ç³»
        teacher_mixed_mean = results_df['teacher_score_mixed'].mean()
        student_mixed_mean = results_df['student_score_mixed_mean'].mean()
        
        if student_mixed_mean > teacher_mixed_mean:
            print(f"  âš ï¸  Studentå¹³å‡æ··åˆåˆ†æ•° ({student_mixed_mean:.2f}) > Teacherå¹³å‡æ··åˆåˆ†æ•° ({teacher_mixed_mean:.2f})")
            print(f"     è¿™è¯´æ˜Studentåœ¨æŸäº›æ ·æœ¬ä¸Šè¡¨ç°ä¼˜äºTeacher")
        else:
            print(f"  âœ… Teacherå¹³å‡æ··åˆåˆ†æ•° ({teacher_mixed_mean:.2f}) > Studentå¹³å‡æ··åˆåˆ†æ•° ({student_mixed_mean:.2f})")
        
        # Batch normalizationæ•ˆæœ
        if results_df['use_batch_norm'].iloc[0]:
            avg_batch_size = results_df['batch_size'].mean()
            print(f"\n  âœ… Batch Normalizationå·²å¯ç”¨")
            print(f"     å¹³å‡batch size: {avg_batch_size:.1f} (1 teacher + {avg_batch_size-1:.0f} students)")
            print(f"     è¿™ä¸è®­ç»ƒæ—¶çš„batchå¤„ç†æ–¹å¼ä¸€è‡´")
        
        print()
        
        print("é…ç½®ä¿¡æ¯:")
        print(f"  æ¯ä¸ªpromptçš„Studentæ ·æœ¬æ•°: {self.training_config.get('num_student_samples', 8)}")
        print(f"  Raw weight: {self.training_config.get('raw_weight', 0.7)}")
        print(f"  Norm weight: {self.training_config.get('norm_weight', 0.3)}")
        print(f"  Use batch norm: {self.training_config.get('use_batch_norm', True)}")
        print(f"  Temperature: {self.training_config.get('temperature', 5.0)}")
        print(f"  Adaptive temperature: {self.training_config.get('adaptive_temperature', False)}")
        print()
        print("=" * 100)


def main():
    parser = argparse.ArgumentParser(description="æµ‹è¯•è®­ç»ƒåçš„Criticå’ŒStudentæ¨¡å‹ï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰")
    parser.add_argument("--data_path", type=str, default=None,
                       help="æµ‹è¯•æ•°æ®é›†è·¯å¾„")
    parser.add_argument("--num_samples", type=int, default=None,
                       help="æµ‹è¯•æ ·æœ¬æ•°é‡")
    parser.add_argument("--output_path", type=str, default=None,
                       help="è¾“å‡ºExcelè·¯å¾„")
    
    args = parser.parse_args()
    
    # ä½¿ç”¨é…ç½®
    data_path = args.data_path or TEST_CONFIG["data_path"]
    num_samples = args.num_samples if args.num_samples is not None else TEST_CONFIG["num_samples"]
    
    if args.output_path:
        output_path = args.output_path
    else:
        output_dir = TEST_CONFIG["output_dir"]
        output_filename = TEST_CONFIG.get("output_filename")
        
        if output_filename:
            output_path = f"{output_dir}/{output_filename}"
        else:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"{output_dir}/test_results_training_mode_{timestamp}.xlsx"
    
    critic_config = API_CONFIGS["critic_model"]
    student_config = API_CONFIGS["student_model"]
    
    print("=" * 100)
    print("ğŸ“‹ æµ‹è¯•é…ç½®ä¿¡æ¯ï¼ˆè®­ç»ƒæ¨¡å¼ï¼‰")
    print("=" * 100)
    print(f"æ•°æ®é›†è·¯å¾„: {data_path}")
    print(f"æµ‹è¯•æ ·æœ¬æ•°: {num_samples if num_samples else 'å…¨éƒ¨'}")
    print(f"è¾“å‡ºè·¯å¾„: {output_path}")
    print()
    print(f"Criticæ¨¡å‹: {critic_config['name']} (æœ¬åœ°) @ {critic_config['model_path']}")
    print(f"Studentæ¨¡å‹: {student_config['name']} @ {student_config['url']}")
    print()
    print("è®­ç»ƒæ¨¡å¼é…ç½®:")
    print(f"  Raw weight: {TEST_CONFIG['raw_weight']}")
    print(f"  Norm weight: {TEST_CONFIG['norm_weight']}")
    print(f"  Use batch norm: {TEST_CONFIG['use_batch_norm']}")
    print(f"  Temperature: {TEST_CONFIG['temperature']}")
    print(f"  Adaptive temperature: {TEST_CONFIG['adaptive_temperature']}")
    print("=" * 100)
    print()
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = ModelTesterTrainingMode(
        critic_config=critic_config,
        student_config=student_config,
        training_config=TEST_CONFIG
    )
    
    # è¿è¡Œæµ‹è¯•
    results_df = tester.test_dataset(
        data_path=data_path,
        num_samples=num_samples,
        output_path=output_path,
        random_sample=TEST_CONFIG.get("random_sample", False),
        random_seed=TEST_CONFIG.get("random_seed", 42)
    )
    
    print("âœ… æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    main()
