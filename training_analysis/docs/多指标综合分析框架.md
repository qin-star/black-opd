# 多指标综合分析框架

## 问题：避免单一指标误导

你提出了一个非常重要的观点：**不能仅凭 d_acc 高就盲目修改代码**。我们需要综合多个指标来判断是否真的存在问题。

## 综合分析框架

### 场景1：d_acc 高 + 其他指标正常 = 可能是正常现象

#### 指标组合
```
critic/d_acc = 96-99%                    ✓ 高
critic/score_diff = 逐渐减小             ✓ 正常趋势
actor/format_reward_mean = 逐渐提升      ✓ Student 在进步
critic/ranking_loss = 逐渐增大           ✓ 区分难度增加
actor/policy_loss = 逐渐下降             ✓ Actor 在学习
```

#### 诊断结论
**这是正常的训练过程！**

**原因**：
- Teacher 质量确实比 Student 好很多（训练初期很正常）
- Critic 正确地学会了区分质量差异
- Student 正在逐步提升（从 format_reward 和 score_diff 趋势可以看出）
- d_acc 高是因为**质量差距确实大**，而不是 critic 学错了

**建议**：
- ✅ **不需要修改代码**
- ✅ 继续训练，观察 score_diff 是否持续缩小
- ✅ 观察 student 质量是否真正提升

---

### 场景2：d_acc 高 + score_diff 不收敛 = 可能有问题

#### 指标组合
```
critic/d_acc = 96-99%                    ⚠️ 高
critic/score_diff = 维持高位不变         ⚠️ 不收敛
actor/format_reward_mean = 不提升        ⚠️ Student 没进步
critic/ranking_loss = 维持低位           ⚠️ 区分太容易
actor/policy_loss = 不下降               ⚠️ Actor 没学习
```

#### 诊断结论
**可能存在问题！**

**可能原因**：
1. **Actor 训练有问题**：Student 质量没有提升
2. **Critic 过度自信**：Loss 设计导致 critic 放大差距
3. **训练陷入局部最优**：需要调整超参数

**建议**：
- 🔍 **先检查 actor 训练**：policy_loss, kl_divergence, entropy
- 🔍 **再检查 critic loss 组件**：ranking_loss, score_reg, diff_penalty
- 🔍 **最后考虑调整 temperature**

---

### 场景3：d_acc 高 + 分数漂移 = Loss 设计问题

#### 指标组合
```
critic/d_acc = 96-99%                         ⚠️ 高
critic/teacher_value_mean = 持续增大 (>5)     ⚠️ 漂移
critic/student_value_mean = 持续减小 (<-5)    ⚠️ 漂移
critic/score_reg = 很小 (<0.01)               ⚠️ 正则化不足
critic/diff_penalty = 很大 (>0.5)             ⚠️ 频繁触发
```

#### 诊断结论
**Loss 设计有问题！**

**原因**：
- Score regularization 权重太小，无法约束分数
- Critic 学会了放大差距来降低 loss
- 分数漂移到极端值

**建议**：
- ✅ 增大 score_reg 权重
- ✅ 降低 diff_penalty 阈值
- ✅ 可能需要调整 temperature

---

### 场景4：d_acc 高 + 数据问题 = 需要数据增强

#### 指标组合
```
critic/d_acc = 96-99%                    ⚠️ 高
critic/score_diff = 很大 (>3.0)          ⚠️ 差距巨大
actor/format_reward_mean = 很低 (<-1.0)  ⚠️ Student 质量极差
critic/ranking_loss = 很低 (<0.2)        ⚠️ 区分太容易
```

#### 诊断结论
**数据质量差距过大！**

**原因**：
- Teacher response 质量太高
- Student response 质量太差（可能是 format reward 过于严格）
- 数据分布单一（所有样本都是 teacher >> student）

**建议**：
- ✅ 降低 format reward 惩罚力度
- ✅ 检查数据集，确保有多样性
- ✅ 考虑使用更弱的 teacher 或更强的 student 初始化

---

## 关键诊断指标矩阵

### 必须观察的指标组合

| 指标类别 | 核心指标 | 辅助指标 | 诊断目的 |
|---------|---------|---------|---------|
| **Critic 性能** | d_acc | score_diff, raw_score_diff | 判断 critic 是否学对了 |
| **Critic 训练** | ranking_loss | score_reg, diff_penalty | 判断 loss 设计是否合理 |
| **分数分布** | teacher_value_mean, student_value_mean | score_diff_abs, score_overlap | 判断是否发生数值漂移 |
| **Actor 训练** | format_reward_mean | policy_loss, kl_divergence | 判断 student 是否在进步 |
| **训练动态** | score_diff 趋势 | d_acc 趋势 | 判断训练是否收敛 |

### 指标之间的逻辑关系

```
如果 d_acc 高：
  ├─ 检查 score_diff 趋势
  │   ├─ 如果逐渐减小 → 正常（student 在进步）
  │   └─ 如果不变或增大 → 有问题
  │
  ├─ 检查 format_reward 趋势
  │   ├─ 如果逐渐提升 → 正常（student 在进步）
  │   └─ 如果不变 → 有问题（actor 没学习）
  │
  ├─ 检查 ranking_loss
  │   ├─ 如果逐渐增大 → 正常（区分难度增加）
  │   └─ 如果维持低位 → 有问题（区分太容易）
  │
  └─ 检查绝对分数
      ├─ 如果在合理范围 (-2 到 2) → 正常
      └─ 如果过大 (>5 或 <-5) → 有问题（数值漂移）
```

---

## 基于你的训练曲线的初步分析

从你提供的截图来看：

### 观察到的现象
```
critic/d_acc = 96-99% (step 0-300)
- 从 step 0 开始就很高
- 在整个训练过程中维持高位
- 有轻微波动但没有明显下降趋势
```

### 需要进一步确认的指标

为了做出准确诊断，我需要你提供以下指标的曲线或数值：

#### 1. 训练趋势指标（最重要）
```
critic/score_diff          # 是否逐渐减小？
actor/format_reward_mean   # 是否逐渐提升？
critic/ranking_loss        # 是否逐渐增大？
```

**如果这三个指标都显示正常趋势**：
- score_diff 从 2.0 → 1.5 → 1.0（逐渐减小）
- format_reward 从 -0.8 → -0.5 → -0.2（逐渐提升）
- ranking_loss 从 0.3 → 0.4 → 0.5（逐渐增大）

**结论**：d_acc 高是正常的，不需要修改代码！

#### 2. 分数分布指标
```
critic/teacher_value_mean  # 是否在合理范围？
critic/student_value_mean  # 是否在合理范围？
critic/score_reg           # 是否足够大？
critic/diff_penalty        # 是否频繁触发？
```

**如果分数在合理范围**：
- teacher_value_mean 在 0-2 之间
- student_value_mean 在 -1-1 之间
- score_reg 在 0.01-0.1 之间
- diff_penalty 在 0-0.3 之间

**结论**：Loss 设计没问题，不需要修改！

#### 3. Actor 训练指标
```
actor/policy_loss          # 是否下降？
actor/kl_divergence        # 是否在合理范围？
actor/entropy              # 是否在合理范围？
```

**如果 actor 训练正常**：
- policy_loss 逐渐下降
- kl_divergence 在 0.01-0.1 之间
- entropy 不会过快衰减

**结论**：Actor 训练正常，student 在学习！

---

## 决策树：是否需要修改代码？

```
开始
  │
  ├─ d_acc 高 (>95%)
  │   │
  │   ├─ score_diff 逐渐减小？
  │   │   ├─ 是 → format_reward 提升？
  │   │   │   ├─ 是 → 【不需要修改】正常训练
  │   │   │   └─ 否 → 【检查 actor】可能 actor 有问题
  │   │   │
  │   │   └─ 否 → ranking_loss 增大？
  │   │       ├─ 是 → 【观察更多步】可能需要更长时间
  │   │       └─ 否 → 【需要修改】Loss 设计有问题
  │   │
  │   └─ 绝对分数正常？
  │       ├─ 是 → 【不需要修改】
  │       └─ 否 → 【需要修改】数值漂移问题
  │
  └─ d_acc 正常 (<85%)
      └─ 【不需要修改】继续训练
```

---

## 我的建议

### 第一步：收集更多指标数据

在做任何代码修改之前，请先提供以下信息：

1. **critic/score_diff** 的曲线（step 0-300）
2. **actor/format_reward_mean** 的曲线（step 0-300）
3. **critic/ranking_loss** 的曲线（step 0-300）
4. **critic/teacher_value_mean** 和 **critic/student_value_mean** 的数值
5. **actor/policy_loss** 的曲线

### 第二步：综合判断

基于这些指标，我们可以判断：

#### 情况A：正常训练（不需要修改）
```
✓ score_diff 逐渐减小
✓ format_reward 逐渐提升
✓ ranking_loss 逐渐增大
✓ 绝对分数在合理范围
→ d_acc 高是因为质量差距确实大，这是正常的
```

#### 情况B：需要修改（但要谨慎）
```
✗ score_diff 不收敛
✗ format_reward 不提升
✗ ranking_loss 维持低位
✗ 绝对分数过大
→ 可能需要调整 temperature 或其他参数
```

### 第三步：如果确实需要修改

**保守的修改策略**：

1. **先尝试小幅调整**：
   ```python
   # 不要一次性从 2.0 跳到 5.0
   temperature = 2.0 → 3.0  # 先增大 50%
   ```

2. **观察效果**：
   - 训练 100-200 steps
   - 观察 d_acc 是否下降
   - 观察其他指标是否恶化

3. **根据效果继续调整**：
   - 如果 d_acc 下降但其他指标正常 → 继续增大
   - 如果 d_acc 下降且其他指标恶化 → 回退
   - 如果 d_acc 不变 → 继续增大

---

## 总结

### 核心原则

1. **不要只看 d_acc**：必须结合多个指标综合判断
2. **关注趋势而非绝对值**：score_diff 是否缩小比 d_acc 是否高更重要
3. **验证 student 是否进步**：format_reward 是最直接的证据
4. **保守修改**：小步快跑，观察效果再调整

### 下一步行动

请提供以下指标的数据，我会给出更准确的诊断：

1. ✅ critic/score_diff（最重要）
2. ✅ actor/format_reward_mean（最重要）
3. ✅ critic/ranking_loss（重要）
4. ✅ critic/teacher_value_mean 和 student_value_mean（重要）
5. ✅ actor/policy_loss（辅助）

有了这些数据，我们可以做出**基于证据的决策**，而不是盲目修改代码。
