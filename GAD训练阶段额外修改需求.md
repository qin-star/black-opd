# GAD è®­ç»ƒé˜¶æ®µï¼ˆç¬¬ä¸‰é˜¶æ®µï¼‰é¢å¤–ä¿®æ”¹éœ€æ±‚åˆ†æ

## ä¸€ã€å·²å®Œæˆçš„ä¿®æ”¹å›é¡¾

åœ¨ Warmup é˜¶æ®µçš„é€‚é…ä¸­ï¼Œæˆ‘ä»¬å·²ç»å®Œæˆäº†ä»¥ä¸‹æ ¸å¿ƒä¿®æ”¹ï¼š

### âœ… å·²å®Œæˆçš„ä¿®æ”¹

1. **`core_algos.py`**
   - âœ… æ·»åŠ  `compute_discriminator_loss` å‡½æ•°
   - âœ… æ”¯æŒ GSPO ç­–ç•¥æŸå¤±ï¼ˆ`compute_policy_loss_gspo`ï¼‰

2. **`dp_critic.py`**
   - âœ… `_forward_micro_batch` æ”¯æŒ `compute_teacher` å‚æ•°
   - âœ… å®ç°åºåˆ—çº§å¥–åŠ±æ¨¡å‹ï¼ˆlast token maskï¼‰
   - âœ… æ·»åŠ  `_forward_batch_teacher_forcing_grpo` æ–¹æ³•
   - âœ… `compute_values` æ”¯æŒæ•™å¸ˆæ•°æ®å’Œ teacher forcing
   - âœ… `update_critic` è‡ªåŠ¨æ£€æµ‹ GAD æ¨¡å¼ï¼Œä½¿ç”¨åˆ¤åˆ«å™¨æŸå¤±

3. **`rl_dataset.py`**
   - âœ… æ”¯æŒ `teacher_response` å­—æ®µçš„åŠ è½½
   - âœ… è‡ªåŠ¨æ„å»º `teacher_input_ids`ã€`teacher_attention_mask`ã€`teacher_position_ids`

---

## äºŒã€GAD è®­ç»ƒé˜¶æ®µçš„ç‰¹æ®Šéœ€æ±‚

æ ¹æ®æ–‡æ¡£åˆ†æï¼ŒGAD è®­ç»ƒé˜¶æ®µä¸ Warmup é˜¶æ®µçš„ä¸»è¦åŒºåˆ«ï¼š

### 2.1 è®­ç»ƒæµç¨‹å·®å¼‚

| ç»´åº¦ | Warmup é˜¶æ®µ | GAD è®­ç»ƒé˜¶æ®µ |
|------|------------|-------------|
| **Critic ä½œç”¨** | é¢„æµ‹ value | ä½œä¸ºåˆ¤åˆ«å™¨æ‰“åˆ† |
| **è®­ç»ƒç›®æ ‡** | æ¨¡ä»¿æ•™å¸ˆå“åº” | å¯¹æŠ—è®­ç»ƒ |
| **æŸå¤±å‡½æ•°** | åˆ¤åˆ«å™¨æŸå¤± | åˆ¤åˆ«å™¨æŸå¤± + PPO æŸå¤± |
| **ä¼˜åŠ¿ä¼°è®¡** | GRPO | GRPO |
| **Actor æ›´æ–°** | æœ‰ warmup é™åˆ¶ | æ­£å¸¸æ›´æ–°ï¼ˆwarmup åï¼‰ |
| **æ¢ç´¢æ€§** | è¾ƒä½ | è¾ƒé«˜ï¼ˆtemperature=0.8ï¼‰ |
| **æ•°æ®æ¥æº** | æ•°æ®é›†ä¸­çš„ teacher_response | æ•°æ®é›†ä¸­çš„ teacher_response |

### 2.2 å…³é”®è§‚å¯Ÿ

**å¥½æ¶ˆæ¯**ï¼šGAD è®­ç»ƒé˜¶æ®µçš„æ ¸å¿ƒæœºåˆ¶ä¸ Warmup é˜¶æ®µ**å®Œå…¨ç›¸åŒ**ï¼

- âœ… éƒ½ä½¿ç”¨åˆ¤åˆ«å™¨æŸå¤±è®­ç»ƒ Critic
- âœ… éƒ½ä½¿ç”¨ GRPO è®¡ç®—ä¼˜åŠ¿
- âœ… éƒ½éœ€è¦ `teacher_response` æ•°æ®
- âœ… éƒ½ä½¿ç”¨ç›¸åŒçš„æ•°æ®æµè®¾è®¡

**å”¯ä¸€çš„åŒºåˆ«**ï¼š
- Warmup é˜¶æ®µï¼š`trainer.critic_warmup=10`ï¼ˆå‰ 10 æ­¥åªè®­ç»ƒ Criticï¼‰
- GAD é˜¶æ®µï¼š`trainer.critic_warmup=0`ï¼ˆä»ç¬¬ 0 æ­¥å¼€å§‹åŒæ—¶è®­ç»ƒ Critic å’Œ Actorï¼‰

---

## ä¸‰ã€éœ€è¦éªŒè¯çš„åŠŸèƒ½ç‚¹

è™½ç„¶æ ¸å¿ƒä»£ç å·²ç»å®Œæˆï¼Œä½†æˆ‘ä»¬éœ€è¦éªŒè¯ä»¥ä¸‹åŠŸèƒ½æ˜¯å¦æ­£ç¡®å·¥ä½œï¼š

### 3.1 Rollout é˜¶æ®µçš„æ•°æ®æ‰©å…… âš ï¸

**æ–‡æ¡£ä¸­çš„æè¿°**ï¼ˆç¬¬ 322-364 è¡Œï¼‰ï¼š

```python
# å­¦ç”Ÿå“åº”
seq = torch.cat([idx, response], dim=-1)
response_position_ids = position_ids[..., -1:] + delta_position_id
position_ids = torch.cat([position_ids, response_position_ids], dim=-1)

# æ•™å¸ˆå“åº”
teacher_seq = torch.cat([idx, teacher_response], dim=-1)
teacher_response_position_ids = position_ids[..., -1:] + teacher_delta_position_id
teacher_position_ids = torch.cat([position_ids, teacher_response_position_ids], dim=-1)
```

**éœ€è¦æ£€æŸ¥çš„æ–‡ä»¶**ï¼š
- `verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py`
- æˆ–å…¶ä»– rollout ç›¸å…³æ–‡ä»¶

**æ£€æŸ¥å†…å®¹**ï¼š
1. â“ Rollout é˜¶æ®µæ˜¯å¦æ­£ç¡®å¤„ç† `teacher_response`
2. â“ æ˜¯å¦æ­£ç¡®æ„å»º `teacher_input_ids`ã€`teacher_attention_mask`ã€`teacher_position_ids`
3. â“ ç”Ÿæˆçš„ batch æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…è¦çš„æ•™å¸ˆæ•°æ®å­—æ®µ

### 3.2 KL æƒ©ç½šæœºåˆ¶ âš ï¸

**æ–‡æ¡£ä¸­çš„æè¿°**ï¼ˆç¬¬ 196-213 è¡Œï¼‰ï¼š

```python
if self.config.algorithm.use_kl_in_reward:
    batch, kl_metrics = apply_kl_penalty(batch, kl_ctrl=self.kl_ctrl_in_reward)
    metrics.update(kl_metrics)
```

**éœ€è¦æ£€æŸ¥çš„æ–‡ä»¶**ï¼š
- `verl/trainer/ppo/ray_trainer.py`

**æ£€æŸ¥å†…å®¹**ï¼š
1. â“ `apply_kl_penalty` å‡½æ•°æ˜¯å¦å­˜åœ¨
2. â“ æ˜¯å¦æ­£ç¡®åº”ç”¨ KL æƒ©ç½šåˆ° `token_level_rewards`
3. â“ é…ç½®å‚æ•° `algorithm.use_kl_in_reward` æ˜¯å¦ç”Ÿæ•ˆ

### 3.3 å‚è€ƒç­–ç•¥ï¼ˆReference Policyï¼‰ âš ï¸

**æ–‡æ¡£ä¸­çš„æè¿°**ï¼ˆç¬¬ 170-181 è¡Œï¼‰ï¼š

```python
if self.use_reference_policy:
    ref_log_prob = self.actor_rollout_wg.compute_ref_log_prob(batch)
    batch = batch.union(ref_log_prob)
```

**éœ€è¦æ£€æŸ¥çš„æ–‡ä»¶**ï¼š
- `verl/workers/actor/dp_actor.py` æˆ–ç›¸å…³æ–‡ä»¶

**æ£€æŸ¥å†…å®¹**ï¼š
1. â“ `compute_ref_log_prob` æ–¹æ³•æ˜¯å¦å­˜åœ¨
2. â“ Reference model æ˜¯å¦æ­£ç¡®åŠ è½½
3. â“ KL æ•£åº¦è®¡ç®—æ˜¯å¦æ­£ç¡®

### 3.4 Actor çš„ KL æŸå¤± âš ï¸

**æ–‡æ¡£ä¸­çš„æè¿°**ï¼ˆç¬¬ 295-299 è¡Œï¼‰ï¼š

```python
if self.config.use_kl_loss:
    kld = kl_penalty(logprob=log_prob, ref_logprob=ref_log_prob)
    kl_loss = agg_loss(loss_mat=kld, loss_mask=response_mask)
    policy_loss = policy_loss + kl_loss * self.config.kl_loss_coef  # 0.001
```

**éœ€è¦æ£€æŸ¥çš„æ–‡ä»¶**ï¼š
- `verl/workers/actor/dp_actor.py`

**æ£€æŸ¥å†…å®¹**ï¼š
1. â“ Actor çš„ `update_policy` æ–¹æ³•æ˜¯å¦æ”¯æŒ `use_kl_loss`
2. â“ `kl_penalty` å‡½æ•°æ˜¯å¦å­˜åœ¨ï¼ˆåº”è¯¥åœ¨ `core_algos.py` ä¸­ï¼‰
3. â“ KL æŸå¤±æ˜¯å¦æ­£ç¡®æ·»åŠ åˆ°ç­–ç•¥æŸå¤±ä¸­

---

## å››ã€éœ€è¦é¢å¤–ä¿®æ”¹çš„éƒ¨åˆ†

åŸºäºæ–‡æ¡£åˆ†æï¼Œä»¥ä¸‹æ˜¯å¯èƒ½éœ€è¦é¢å¤–ä¿®æ”¹çš„éƒ¨åˆ†ï¼š

### 4.1 Rollout é˜¶æ®µçš„æ•™å¸ˆæ•°æ®å¤„ç† ğŸ”§

**ä½ç½®**ï¼š`verl/workers/rollout/vllm_rollout/vllm_rollout_spmd.py` æˆ–ç±»ä¼¼æ–‡ä»¶

**å½“å‰çŠ¶æ€**ï¼šâ“ æœªçŸ¥ï¼Œéœ€è¦æ£€æŸ¥

**éœ€è¦çš„åŠŸèƒ½**ï¼š
```python
def generate_sequences(self, prompts):
    # ç”Ÿæˆå­¦ç”Ÿå“åº”
    student_responses = self.vllm_engine.generate(prompts, n=8)
    
    # ä»æ•°æ®é›†ä¸­è·å–æ•™å¸ˆå“åº”
    teacher_responses = prompts.batch.get("teacher_response")
    
    if teacher_responses is not None:
        # æ„å»ºæ•™å¸ˆçš„å®Œæ•´åºåˆ—
        teacher_input_ids = torch.cat([prompts.batch["input_ids"], teacher_responses], dim=-1)
        teacher_attention_mask = (teacher_input_ids != pad_token_id).long()
        teacher_position_ids = compute_position_ids(teacher_attention_mask)
        
        # æ·»åŠ åˆ°è¿”å›çš„ batch ä¸­
        output_batch.update({
            "teacher_input_ids": teacher_input_ids,
            "teacher_attention_mask": teacher_attention_mask,
            "teacher_position_ids": teacher_position_ids,
            "teacher_response": teacher_responses,
        })
    
    return output_batch
```

**ä¿®æ”¹å»ºè®®**ï¼š
- å¦‚æœ Rollout é˜¶æ®µæ²¡æœ‰å¤„ç†æ•™å¸ˆæ•°æ®ï¼Œéœ€è¦æ·»åŠ 
- å¦‚æœå·²ç»åœ¨ `rl_dataset.py` ä¸­å¤„ç†ï¼Œå¯èƒ½ä¸éœ€è¦é¢å¤–ä¿®æ”¹

### 4.2 æ£€æŸ¥ `ray_trainer.py` çš„è®­ç»ƒæµç¨‹ ğŸ”

**ä½ç½®**ï¼š`verl/trainer/ppo/ray_trainer.py`

**éœ€è¦éªŒè¯çš„æµç¨‹**ï¼š
```python
def fit(self):
    for epoch in range(total_epochs):
        for batch in dataloader:
            # 1. ç”Ÿæˆå“åº”
            gen_batch = self.actor_rollout_wg.generate_sequences(batch)
            
            # 2. è®¡ç®— old_log_prob
            old_log_prob = self.actor_rollout_wg.compute_log_prob(batch)
            
            # 3. è®¡ç®— ref_log_probï¼ˆå¦‚æœä½¿ç”¨ï¼‰
            if self.use_reference_policy:
                ref_log_prob = self.actor_rollout_wg.compute_ref_log_prob(batch)
            
            # 4. Critic æ‰“åˆ†
            values = self.critic_wg.compute_values(batch)
            
            # 5. åº”ç”¨ KL æƒ©ç½šï¼ˆå¦‚æœä½¿ç”¨ï¼‰
            if self.config.algorithm.use_kl_in_reward:
                batch = apply_kl_penalty(batch, kl_ctrl=self.kl_ctrl_in_reward)
            
            # 6. è®¡ç®—ä¼˜åŠ¿
            batch = compute_advantage(batch, adv_estimator="grpo")
            
            # 7. æ›´æ–° Critic
            if self.use_critic:
                self.critic_wg.update_critic(batch)
            
            # 8. æ›´æ–° Actorï¼ˆå¦‚æœè¿‡äº† warmupï¼‰
            if self.global_steps > self.config.trainer.critic_warmup:
                self.actor_rollout_wg.update_actor(batch)
```

**æ£€æŸ¥è¦ç‚¹**ï¼š
- âœ… è®­ç»ƒæµç¨‹æ˜¯å¦ä¸æ–‡æ¡£æè¿°ä¸€è‡´
- â“ `apply_kl_penalty` æ˜¯å¦å­˜åœ¨
- â“ `compute_ref_log_prob` æ˜¯å¦å­˜åœ¨
- â“ æ•°æ®æµæ˜¯å¦æ­£ç¡®ä¼ é€’æ•™å¸ˆæ•°æ®

### 4.3 éªŒè¯ Actor çš„ KL æŸå¤±æ”¯æŒ ğŸ”

**ä½ç½®**ï¼š`verl/workers/actor/dp_actor.py`

**éœ€è¦æ£€æŸ¥**ï¼š
```python
def update_policy(self, data):
    # ... å‰å‘ä¼ æ’­ ...
    
    # è®¡ç®—ç­–ç•¥æŸå¤±
    pg_loss = compute_policy_loss(...)
    
    # ç†µæ­£åˆ™åŒ–
    entropy_loss = compute_entropy_loss(...)
    policy_loss = pg_loss - entropy_coeff * entropy_loss
    
    # KL æŸå¤±ï¼ˆéœ€è¦éªŒè¯è¿™éƒ¨åˆ†æ˜¯å¦å­˜åœ¨ï¼‰
    if self.config.use_kl_loss:
        ref_log_prob = data.batch.get("ref_log_prob")
        if ref_log_prob is not None:
            kld = kl_penalty(logprob=log_prob, ref_logprob=ref_log_prob)
            kl_loss = agg_loss(loss_mat=kld, loss_mask=response_mask)
            policy_loss = policy_loss + kl_loss * self.config.kl_loss_coef
    
    # åå‘ä¼ æ’­
    policy_loss.backward()
```

---

## äº”ã€ä¿®æ”¹ä¼˜å…ˆçº§

### ğŸ”´ é«˜ä¼˜å…ˆçº§ï¼ˆå¿…é¡»ä¿®æ”¹ï¼‰

**æ— **ï¼æ ¸å¿ƒåŠŸèƒ½å·²ç»åœ¨ Warmup é˜¶æ®µå®Œæˆã€‚

### ğŸŸ¡ ä¸­ä¼˜å…ˆçº§ï¼ˆéœ€è¦éªŒè¯ï¼‰

1. **éªŒè¯ Rollout é˜¶æ®µçš„æ•™å¸ˆæ•°æ®å¤„ç†**
   - æ£€æŸ¥ `vllm_rollout_spmd.py` æˆ–ç±»ä¼¼æ–‡ä»¶
   - ç¡®è®¤æ•™å¸ˆæ•°æ®æ˜¯å¦æ­£ç¡®ä¼ é€’åˆ° Critic

2. **éªŒè¯ `ray_trainer.py` çš„è®­ç»ƒæµç¨‹**
   - ç¡®è®¤æ˜¯å¦æ”¯æŒ `apply_kl_penalty`
   - ç¡®è®¤æ˜¯å¦æ”¯æŒ `compute_ref_log_prob`

3. **éªŒè¯ Actor çš„ KL æŸå¤±**
   - æ£€æŸ¥ `dp_actor.py` çš„ `update_policy` æ–¹æ³•
   - ç¡®è®¤æ˜¯å¦æ”¯æŒ `use_kl_loss` é…ç½®

### ğŸŸ¢ ä½ä¼˜å…ˆçº§ï¼ˆå¯é€‰ä¼˜åŒ–ï¼‰

1. **åŒé‡è£å‰ªæœºåˆ¶**ï¼ˆæ–‡æ¡£ç¬¬ 1256-1271 è¡Œï¼‰
   - GAD å®ç°äº†æ›´å¤æ‚çš„åŒé‡è£å‰ª
   - å¦‚æœæ–°æ¡†æ¶æ²¡æœ‰ï¼Œå¯ä»¥è€ƒè™‘æ·»åŠ 
   - ä½†è¿™ä¸æ˜¯å¿…éœ€çš„ï¼Œæ ‡å‡† PPO è£å‰ªå·²ç»è¶³å¤Ÿ

---

## å…­ã€éªŒè¯æ¸…å•

### 6.1 ä»£ç éªŒè¯

- [ ] æ£€æŸ¥ `verl/workers/rollout/` ç›®å½•ä¸‹çš„æ–‡ä»¶ï¼Œç¡®è®¤æ•™å¸ˆæ•°æ®å¤„ç†
- [ ] æ£€æŸ¥ `ray_trainer.py` çš„ `fit` æ–¹æ³•ï¼Œç¡®è®¤è®­ç»ƒæµç¨‹
- [ ] æ£€æŸ¥ `dp_actor.py` çš„ `update_policy` æ–¹æ³•ï¼Œç¡®è®¤ KL æŸå¤±æ”¯æŒ
- [ ] æœç´¢ `apply_kl_penalty` å‡½æ•°æ˜¯å¦å­˜åœ¨
- [ ] æœç´¢ `compute_ref_log_prob` æ–¹æ³•æ˜¯å¦å­˜åœ¨
- [ ] æœç´¢ `kl_penalty` å‡½æ•°æ˜¯å¦å­˜åœ¨ï¼ˆåº”è¯¥åœ¨ `core_algos.py` ä¸­ï¼‰

### 6.2 é…ç½®éªŒè¯

- [ ] ç¡®è®¤é…ç½®æ–‡ä»¶æ”¯æŒä»¥ä¸‹å‚æ•°ï¼š
  - `algorithm.use_kl_in_reward`
  - `actor_rollout_ref.actor.use_kl_loss`
  - `actor_rollout_ref.actor.kl_loss_coef`
  - `actor_rollout_ref.actor.kl_loss_type`
  - `trainer.critic_warmup`

### 6.3 åŠŸèƒ½éªŒè¯

- [ ] è¿è¡Œ Warmup è®­ç»ƒï¼ŒéªŒè¯åˆ¤åˆ«å™¨è®­ç»ƒæ­£å¸¸
- [ ] è¿è¡Œ GAD è®­ç»ƒï¼ŒéªŒè¯ Actor å’Œ Critic åŒæ—¶æ›´æ–°
- [ ] æ£€æŸ¥è®­ç»ƒæ—¥å¿—ï¼Œç¡®è®¤ä»¥ä¸‹æŒ‡æ ‡ï¼š
  - `critic/d_loss`
  - `critic/d_acc`
  - `actor/pg_loss`
  - `actor/ppo_kl`
  - `actor/kl_loss`ï¼ˆå¦‚æœä½¿ç”¨ï¼‰

---

## ä¸ƒã€æ€»ç»“

### 7.1 æ ¸å¿ƒç»“è®º

**å¥½æ¶ˆæ¯**ï¼šGAD è®­ç»ƒé˜¶æ®µçš„æ ¸å¿ƒä»£ç å·²ç»åœ¨ Warmup é˜¶æ®µå®Œæˆï¼

æˆ‘ä»¬å·²ç»å®ç°çš„ä¿®æ”¹å·²ç»è¦†ç›–äº† GAD è®­ç»ƒçš„æ ¸å¿ƒéœ€æ±‚ï¼š
- âœ… åˆ¤åˆ«å™¨æŸå¤±è®¡ç®—
- âœ… åºåˆ—çº§å¥–åŠ±æ¨¡å‹
- âœ… GRPO ä¼˜åŠ¿ä¼°è®¡ï¼ˆæ¡†æ¶å†…ç½®ï¼‰
- âœ… GSPO ç­–ç•¥æŸå¤±ï¼ˆæ¡†æ¶å†…ç½®ï¼‰
- âœ… æ•™å¸ˆæ•°æ®åŠ è½½å’Œå¤„ç†

### 7.2 éœ€è¦éªŒè¯çš„éƒ¨åˆ†

ä¸»è¦éœ€è¦éªŒè¯ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š
1. **Rollout é˜¶æ®µ**ï¼šæ•™å¸ˆæ•°æ®æ˜¯å¦æ­£ç¡®ä¼ é€’
2. **KL æƒ©ç½š**ï¼š`apply_kl_penalty` æ˜¯å¦å­˜åœ¨å’Œæ­£ç¡®å·¥ä½œ
3. **å‚è€ƒç­–ç•¥**ï¼š`compute_ref_log_prob` æ˜¯å¦å­˜åœ¨å’Œæ­£ç¡®å·¥ä½œ
4. **Actor KL æŸå¤±**ï¼š`use_kl_loss` é…ç½®æ˜¯å¦ç”Ÿæ•ˆ

### 7.3 æ¨èçš„éªŒè¯æµç¨‹

1. **å…ˆè¿è¡Œ Warmup è®­ç»ƒ**
   - ä½¿ç”¨ `trainer.critic_warmup=10`
   - éªŒè¯åˆ¤åˆ«å™¨è®­ç»ƒæ­£å¸¸
   - éªŒè¯ Actor åœ¨ç¬¬ 10 æ­¥åå¼€å§‹æ›´æ–°

2. **å†è¿è¡Œ GAD è®­ç»ƒ**
   - ä½¿ç”¨ `trainer.critic_warmup=0`
   - ä» Warmup æ£€æŸ¥ç‚¹ç»§ç»­è®­ç»ƒ
   - éªŒè¯ Actor å’Œ Critic åŒæ—¶æ›´æ–°

3. **ç›‘æ§å…³é”®æŒ‡æ ‡**
   - `critic/d_acc` åº”è¯¥åœ¨ 0.5-0.8 ä¹‹é—´
   - `actor/pg_clipfrac` åº”è¯¥åœ¨ 0.1-0.3 ä¹‹é—´
   - å¦‚æœä½¿ç”¨ KL æŸå¤±ï¼Œ`actor/kl_loss` åº”è¯¥è¾ƒå°

### 7.4 å¦‚æœé‡åˆ°é—®é¢˜

å¦‚æœéªŒè¯è¿‡ç¨‹ä¸­å‘ç°ç¼ºå¤±çš„åŠŸèƒ½ï¼Œå¯ä»¥å‚è€ƒä»¥ä¸‹ä¼˜å…ˆçº§ï¼š

1. **å¿…é¡»ä¿®å¤**ï¼š
   - æ•™å¸ˆæ•°æ®æœªæ­£ç¡®ä¼ é€’åˆ° Critic
   - åˆ¤åˆ«å™¨æŸå¤±è®¡ç®—é”™è¯¯
   - GRPO ä¼˜åŠ¿è®¡ç®—é”™è¯¯

2. **å»ºè®®ä¿®å¤**ï¼š
   - KL æƒ©ç½šåŠŸèƒ½ç¼ºå¤±
   - å‚è€ƒç­–ç•¥åŠŸèƒ½ç¼ºå¤±
   - Actor KL æŸå¤±åŠŸèƒ½ç¼ºå¤±

3. **å¯é€‰ä¼˜åŒ–**ï¼š
   - åŒé‡è£å‰ªæœºåˆ¶
   - æ›´å¤æ‚çš„ç›‘æ§æŒ‡æ ‡
   - æ€§èƒ½ä¼˜åŒ–

---

## å…«ã€ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### 8.1 ç«‹å³è¡ŒåŠ¨

1. **æ£€æŸ¥ Rollout æ–‡ä»¶**
   ```bash
   # æŸ¥æ‰¾ rollout ç›¸å…³æ–‡ä»¶
   find verl/workers/rollout -name "*.py"
   
   # æœç´¢ teacher_response çš„å¤„ç†
   grep -r "teacher_response" verl/workers/rollout/
   ```

2. **æ£€æŸ¥ ray_trainer.py**
   ```bash
   # æœç´¢ apply_kl_penalty
   grep -n "apply_kl_penalty" verl/trainer/ppo/ray_trainer.py
   
   # æœç´¢ compute_ref_log_prob
   grep -n "compute_ref_log_prob" verl/trainer/ppo/ray_trainer.py
   ```

3. **æ£€æŸ¥ dp_actor.py**
   ```bash
   # æœç´¢ use_kl_loss
   grep -n "use_kl_loss" verl/workers/actor/dp_actor.py
   
   # æœç´¢ kl_penalty å‡½æ•°
   grep -n "def kl_penalty" verl/trainer/ppo/core_algos.py
   ```

### 8.2 å¦‚æœå‘ç°ç¼ºå¤±åŠŸèƒ½

æ ¹æ®æ£€æŸ¥ç»“æœï¼Œæˆ‘å¯ä»¥å¸®ä½ ï¼š
1. æ·»åŠ ç¼ºå¤±çš„å‡½æ•°
2. ä¿®æ”¹ç°æœ‰ä»£ç ä»¥æ”¯æŒæ–°åŠŸèƒ½
3. æä¾›å®Œæ•´çš„å®ç°æ–¹æ¡ˆ

### 8.3 æµ‹è¯•å»ºè®®

1. **å•å…ƒæµ‹è¯•**ï¼š
   - æµ‹è¯• `compute_discriminator_loss` å‡½æ•°
   - æµ‹è¯• `_forward_micro_batch` çš„åŒè·¯æ¨ç†
   - æµ‹è¯•æ•™å¸ˆæ•°æ®åŠ è½½

2. **é›†æˆæµ‹è¯•**ï¼š
   - è¿è¡Œå°è§„æ¨¡ Warmup è®­ç»ƒï¼ˆ1-2 ä¸ª epochï¼‰
   - æ£€æŸ¥æ‰€æœ‰æŒ‡æ ‡æ˜¯å¦æ­£å¸¸è¾“å‡º
   - éªŒè¯æ¨¡å‹æ£€æŸ¥ç‚¹å¯ä»¥æ­£å¸¸ä¿å­˜å’ŒåŠ è½½

3. **å®Œæ•´è®­ç»ƒ**ï¼š
   - è¿è¡Œå®Œæ•´çš„ Warmup è®­ç»ƒ
   - ä» Warmup æ£€æŸ¥ç‚¹ç»§ç»­ GAD è®­ç»ƒ
   - ç›‘æ§è®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›æ€§

---

**ç»“è®º**ï¼šæˆ‘ä»¬å·²ç»å®Œæˆäº† GAD è®­ç»ƒçš„æ ¸å¿ƒä»£ç ä¿®æ”¹ã€‚ç°åœ¨ä¸»è¦éœ€è¦éªŒè¯ä¸€äº›è¾…åŠ©åŠŸèƒ½ï¼ˆKL æƒ©ç½šã€å‚è€ƒç­–ç•¥ç­‰ï¼‰æ˜¯å¦å­˜åœ¨å’Œæ­£ç¡®å·¥ä½œã€‚å¦‚æœè¿™äº›åŠŸèƒ½ç¼ºå¤±ï¼Œæˆ‘å¯ä»¥å¸®ä½ æ·»åŠ ã€‚
