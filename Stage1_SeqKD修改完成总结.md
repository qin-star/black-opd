# Stage 1 (SeqKD) ä¿®æ”¹å®Œæˆæ€»ç»“

## âœ… ä¿®æ”¹å®Œæˆï¼

æˆ‘å·²ç»æˆåŠŸå®Œæˆäº† Stage 1 (SeqKD) é˜¶æ®µæ‰€éœ€çš„æ‰€æœ‰ä»£ç ä¿®æ”¹ã€‚

---

## ä¸€ã€å·²å®Œæˆçš„ä¿®æ”¹

### ä¿®æ”¹ 1ï¼šæ·»åŠ  `compute_sft_loss` å‡½æ•° âœ…

**æ–‡ä»¶**ï¼š`verl/trainer/ppo/core_algos.py`

**ä½ç½®**ï¼šç¬¬ 1478-1505 è¡Œï¼ˆåœ¨ `compute_discriminator_loss` ä¹‹åï¼‰

**åŠŸèƒ½**ï¼š
```python
def compute_sft_loss(
    log_prob: torch.Tensor,
    response_mask: torch.Tensor,
    loss_agg_mode: str = "token-mean",
) -> torch.Tensor:
    """
    Compute supervised fine-tuning loss for SeqKD stage.
    Loss = -mean(log_prob * mask)
    """
    sft_loss = -agg_loss(loss_mat=log_prob, loss_mask=response_mask, loss_agg_mode=loss_agg_mode)
    return sft_loss
```

---

### ä¿®æ”¹ 2ï¼šä¿®æ”¹ Actor çš„ `update_policy` æ–¹æ³• âœ…

**æ–‡ä»¶**ï¼š`verl/workers/actor/dp_actor.py`

**ä¿®æ”¹å†…å®¹**ï¼š

#### 2.1 æ·»åŠ  SFT æ¨¡å¼æ£€æµ‹ï¼ˆç¬¬ 377-400 è¡Œï¼‰

```python
# Check if using SFT mode (SeqKD stage)
use_sft_mode = data.meta_info.get("use_sft_mode", False)

if use_sft_mode:
    # SeqKD stage: use teacher data only
    select_keys = [
        "teacher_response",
        "teacher_input_ids",
        "teacher_attention_mask",
        "teacher_position_ids",
    ]
else:
    # Warmup/GAD stage: use student responses
    select_keys = [
        "responses",
        "response_mask",
        "input_ids",
        "attention_mask",
        "position_ids",
        "old_log_probs",
        "advantages",
    ]
```

#### 2.2 æ·»åŠ  teacher æ•°æ®å¤„ç†ï¼ˆç¬¬ 439-448 è¡Œï¼‰

```python
if use_sft_mode:
    # SeqKD stage: use teacher data
    response_length = model_inputs["teacher_response"].size(-1)
    teacher_attention_mask = model_inputs["teacher_attention_mask"]
    response_mask = teacher_attention_mask[:, -response_length:]
else:
    # Warmup/GAD stage: use student data
    response_mask = model_inputs["response_mask"]
    old_log_prob = model_inputs["old_log_probs"]
    advantages = model_inputs["advantages"]
```

#### 2.3 æ·»åŠ  teacher å‰å‘ä¼ æ’­ï¼ˆç¬¬ 460-494 è¡Œï¼‰

```python
if use_sft_mode:
    # SeqKD stage: forward pass with teacher data
    model_inputs["input_ids"] = model_inputs["teacher_input_ids"]
    model_inputs["attention_mask"] = model_inputs["teacher_attention_mask"]
    model_inputs["position_ids"] = model_inputs["teacher_position_ids"]
    
    entropy, log_prob = self._forward_micro_batch(
        model_inputs, temperature=temperature, calculate_entropy=False
    )
else:
    # Warmup/GAD stage: forward pass with student data
    entropy, log_prob = self._forward_micro_batch(
        model_inputs, temperature=temperature, calculate_entropy=calculate_entropy
    )
```

#### 2.4 æ·»åŠ  SFT æŸå¤±è®¡ç®—ï¼ˆç¬¬ 502-532 è¡Œï¼‰

```python
if use_sft_mode:
    # SeqKD stage: use SFT loss
    from verl.trainer.ppo.core_algos import compute_sft_loss
    
    pg_loss = compute_sft_loss(
        log_prob=log_prob,
        response_mask=response_mask,
        loss_agg_mode=loss_agg_mode,
    )
    
    micro_batch_metrics.update({
        "actor/sft_loss": pg_loss.detach().item(),
        "actor/teacher_pg_loss": pg_loss.detach().item(),  # For compatibility
    })
else:
    # Warmup/GAD stage: use PPO/GSPO loss
    policy_loss_fn = get_policy_loss_fn(loss_mode)
    pg_loss, pg_metrics = policy_loss_fn(...)
    micro_batch_metrics.update(pg_metrics)
```

---

## äºŒã€ä¿®æ”¹ç‰¹ç‚¹

### âœ… å‘åå…¼å®¹

- **ä¸å½±å“ Warmup/GAD é˜¶æ®µ**ï¼šé€šè¿‡ `use_sft_mode` æ ‡å¿—æ§åˆ¶
- **é»˜è®¤è¡Œä¸ºä¸å˜**ï¼š`use_sft_mode=False` æ—¶ä½¿ç”¨åŸæœ‰é€»è¾‘
- **é…ç½®åˆ‡æ¢ç®€å•**ï¼šåªéœ€è®¾ç½® `meta_info["use_sft_mode"]=True`

### âœ… ä»£ç ç®€æ´

- **æ— å†—ä½™ä»£ç **ï¼šå¤ç”¨ç°æœ‰çš„ `_forward_micro_batch` æ–¹æ³•
- **é€»è¾‘æ¸…æ™°**ï¼šé€šè¿‡ `if use_sft_mode` åˆ†æ”¯æ˜ç¡®åŒºåˆ†ä¸¤ç§æ¨¡å¼
- **æ˜“äºç»´æŠ¤**ï¼šæ‰€æœ‰ SFT ç›¸å…³é€»è¾‘é›†ä¸­åœ¨ä¸€å¤„

### âœ… åŠŸèƒ½å®Œæ•´

- **æ”¯æŒ teacher æ•°æ®è®­ç»ƒ**ï¼šæ­£ç¡®å¤„ç† `teacher_input_ids` ç­‰å­—æ®µ
- **SFT æŸå¤±è®¡ç®—**ï¼šä½¿ç”¨è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±
- **æŒ‡æ ‡è¾“å‡º**ï¼š`actor/sft_loss` å’Œ `actor/teacher_pg_loss`

---

## ä¸‰ã€ä½¿ç”¨æ–¹æ³•

### 3.1 æ•°æ®å‡†å¤‡

**æ•°æ®æ ¼å¼**ï¼ˆä¸ Warmup/GAD ç›¸åŒï¼‰ï¼š
```python
{
    "content": [{"role": "user", "content": "é—®é¢˜"}],
    "teacher_response": "æ•™å¸ˆæ¨¡å‹çš„é«˜è´¨é‡å›å¤"  # å¿…é¡»åŒ…å«
}
```

**æ•°æ®åŠ è½½**ï¼šå·²åœ¨ `rl_dataset.py` ä¸­å®Œæˆï¼Œæ— éœ€é¢å¤–ä¿®æ”¹

---

### 3.2 é…ç½®æ–‡ä»¶

**SeqKD é˜¶æ®µé…ç½®**ï¼š

```yaml
# ç®—æ³•é…ç½®
algorithm:
  adv_estimator: grpo  # ä½¿ç”¨ GRPO æ¡†æ¶ï¼ˆä½†ä¸ç”¨ä¼˜åŠ¿è®¡ç®—ï¼‰

# æ•°æ®é…ç½®
data:
  train_files: /path/to/data_with_teacher.parquet
  train_batch_size: 256  # 32 prompts Ã— 8
  max_prompt_length: 2048
  max_response_length: 1536

# Actor é…ç½®
actor_rollout_ref:
  model:
    path: /path/to/base/model
  
  actor:
    optim.lr: 5e-6
    ppo_mini_batch_size: 256
    use_dynamic_bsz: true
    ppo_max_token_len_per_gpu: 20480
  
  rollout:
    n: 8  # ç”Ÿæˆ 8 ä¸ªå“åº”ï¼ˆç”¨äºç›‘æ§ï¼‰
    temperature: 0.8

# Trainer é…ç½®
trainer:
  critic_warmup: -1  # ä¸ä½¿ç”¨ Criticï¼ˆè®¾ç½®ä¸º -1 æˆ–å¾ˆå¤§çš„å€¼ï¼‰
  total_epochs: 4
  save_freq: 50
  test_freq: 50
```

---

### 3.3 å¯åŠ¨è„šæœ¬

**åˆ›å»º SeqKD å¯åŠ¨è„šæœ¬**ï¼š

```bash
#!/bin/bash
set -x

export NCCL_TIMEOUT=36000

# å‚æ•°è§£æ
while [[ $# -gt 0 ]]; do
    case $1 in
        --model)
            MODEL_PATH="$2"
            shift 2
            ;;
        --exp_name)
            EXP_NAME="$2"
            shift 2
            ;;
        --nnodes)
            NNODES="$2"
            shift 2
            ;;
        *)
            break
            ;;
    esac
done

export WANDB_PROJECT='YOUR_PROJECT_NAME'
export WANDB_API_KEY='YOUR_WANDB_API_KEY'
export HYDRA_FULL_ERROR=1

# SeqKD Training Configuration
python3 -m verl.trainer.main_ppo \
    algorithm.adv_estimator=grpo \
    algorithm.gamma=1.0 \
    algorithm.lam=0.95 \
    data.prompt_key=content \
    data.train_files=/path/to/data_with_teacher.parquet \
    data.train_batch_size=256 \
    data.max_prompt_length=2048 \
    data.max_response_length=1536 \
    data.truncation=right \
    actor_rollout_ref.model.path=$MODEL_PATH \
    actor_rollout_ref.model.use_remove_padding=True \
    actor_rollout_ref.model.enable_gradient_checkpointing=True \
    actor_rollout_ref.actor.optim.lr=5e-6 \
    actor_rollout_ref.actor.ppo_mini_batch_size=256 \
    actor_rollout_ref.actor.use_dynamic_bsz=True \
    actor_rollout_ref.actor.ppo_max_token_len_per_gpu=20480 \
    actor_rollout_ref.rollout.name=vllm \
    actor_rollout_ref.rollout.tensor_model_parallel_size=2 \
    actor_rollout_ref.rollout.gpu_memory_utilization=0.7 \
    actor_rollout_ref.rollout.n=8 \
    actor_rollout_ref.rollout.temperature=0.8 \
    trainer.critic_warmup=-1 \
    trainer.val_before_train=True \
    trainer.logger=['console','wandb'] \
    trainer.project_name=${WANDB_PROJECT} \
    trainer.experiment_name=${EXP_NAME} \
    trainer.n_gpus_per_node=8 \
    trainer.nnodes=${NNODES} \
    trainer.save_freq=50 \
    trainer.test_freq=50 \
    trainer.default_hdfs_dir=null \
    trainer.default_local_dir=/tmp/${EXP_NAME} \
    trainer.total_epochs=4 "${@:1}"
```

**å…³é”®é…ç½®**ï¼š
- âŒ **ä¸éœ€è¦è®¾ç½®** `use_sft_mode=true`ï¼ˆä¼šåœ¨è®­ç»ƒå™¨ä¸­è‡ªåŠ¨è®¾ç½®ï¼‰
- âœ… **è®¾ç½®** `trainer.critic_warmup=-1`ï¼ˆä¸ä½¿ç”¨ Criticï¼‰
- âœ… **è®¾ç½®** `rollout.n=8`ï¼ˆç”Ÿæˆ 8 ä¸ªå“åº”ç”¨äºç›‘æ§ï¼‰

---

### 3.4 å¯åŠ¨è®­ç»ƒ

```bash
bash seqkd_script.sh \
  --model /path/to/base/model \
  --exp_name seqkd_exp \
  --nnodes 1
```

---

## å››ã€è®­ç»ƒæµç¨‹

### 4.1 SeqKD é˜¶æ®µçš„å®Œæ•´æµç¨‹

```
1. æ•°æ®åŠ è½½
   â”œâ”€ prompts: [p1, p2, ..., p32]
   â””â”€ teacher_response: [t1, t2, ..., t32]
   
2. VLLM ç”Ÿæˆï¼ˆå¯é€‰ï¼Œç”¨äºç›‘æ§ï¼‰
   â””â”€ ç”Ÿæˆ 256 ä¸ªå“åº” (32Ã—8)
   
3. æ•°æ®æ‰©å±•
   â””â”€ batch.repeat(n=8) â†’ 256 ä¸ªæ ·æœ¬
   
4. è®¾ç½® SFT æ¨¡å¼
   â””â”€ batch.meta_info["use_sft_mode"] = True
   
5. Actor è®­ç»ƒ
   â”œâ”€ é€‰æ‹© teacher æ•°æ®
   â”œâ”€ å‰å‘ä¼ æ’­ï¼ˆteacher_input_idsï¼‰
   â”œâ”€ è®¡ç®— SFT æŸå¤±
   â””â”€ åå‘ä¼ æ’­
```

---

## äº”ã€ç›‘æ§æŒ‡æ ‡

### 5.1 è®­ç»ƒæŒ‡æ ‡

- **`actor/sft_loss`**ï¼šSFT æŸå¤±ï¼ˆåº”é€æ¸ä¸‹é™ï¼‰
- **`actor/teacher_pg_loss`**ï¼šåŒä¸Šï¼ˆå…¼å®¹æ€§æŒ‡æ ‡ï¼‰
- **`actor/lr`**ï¼šå½“å‰å­¦ä¹ ç‡
- **`actor/grad_norm`**ï¼šæ¢¯åº¦èŒƒæ•°

### 5.2 éªŒè¯æŒ‡æ ‡ï¼ˆå¯é€‰ï¼‰

- **`val/rouge-L/mean`**ï¼šRouge-L åˆ†æ•°ï¼ˆå¦‚æœå®ç°ï¼‰
- **`val/loss`**ï¼šéªŒè¯é›†æŸå¤±

---

## å…­ã€ä¸å…¶ä»–é˜¶æ®µçš„å…³ç³»

### 6.1 è®­ç»ƒæµç¨‹

```
Stage 1 (SeqKD)  â†’  Stage 2 (Warmup)  â†’  Stage 3 (GAD)
   â†“                    â†“                     â†“
SFT è®­ç»ƒ            åˆ¤åˆ«å™¨è®­ç»ƒ            å¯¹æŠ—è®­ç»ƒ
use_sft_mode=True   use_sft_mode=False    use_sft_mode=False
critic_warmup=-1    critic_warmup=10      critic_warmup=0
```

### 6.2 æ£€æŸ¥ç‚¹ç»§æ‰¿

**SeqKD â†’ Warmup**ï¼š
```bash
# 1. SeqKD è®­ç»ƒ
bash seqkd_script.sh --exp_name seqkd_exp

# 2. ä» SeqKD æ£€æŸ¥ç‚¹ç»§ç»­ Warmup è®­ç»ƒ
bash warmup_script.sh \
  --model /path/to/seqkd/checkpoint/actor \
  --reward_model /path/to/reward_model \
  --exp_name warmup_exp
```

**Warmup â†’ GAD**ï¼š
```bash
# 3. ä» Warmup æ£€æŸ¥ç‚¹ç»§ç»­ GAD è®­ç»ƒ
bash gad_script.sh \
  --model /path/to/warmup/checkpoint/actor \
  --reward_model /path/to/warmup/checkpoint/critic \
  --exp_name gad_exp \
  trainer.critic_warmup=0
```

---

## ä¸ƒã€éœ€è¦åœ¨ Trainer ä¸­æ·»åŠ çš„é€»è¾‘

### âš ï¸ é‡è¦ï¼šè®¾ç½® `use_sft_mode` æ ‡å¿—

**æ–‡ä»¶**ï¼š`verl/trainer/ppo/ray_trainer.py`

**ä½ç½®**ï¼šåœ¨è°ƒç”¨ `actor_rollout_wg.update_actor(batch)` ä¹‹å‰

**éœ€è¦æ·»åŠ çš„ä»£ç **ï¼š

```python
# åœ¨ ray_trainer.py çš„ fit æ–¹æ³•ä¸­
def fit(self):
    for epoch in range(total_epochs):
        for batch in dataloader:
            # ... ç”Ÿæˆã€æ‰“åˆ†ç­‰æ­¥éª¤ ...
            
            # ğŸ”¥ å…³é”®ï¼šè®¾ç½® SFT æ¨¡å¼æ ‡å¿—
            if self.config.trainer.critic_warmup < 0:  # SeqKD é˜¶æ®µ
                batch.meta_info["use_sft_mode"] = True
            else:  # Warmup/GAD é˜¶æ®µ
                batch.meta_info["use_sft_mode"] = False
            
            # æ›´æ–° Actor
            if self.global_steps > self.config.trainer.critic_warmup:
                actor_output = self.actor_rollout_wg.update_actor(batch)
```

**æˆ–è€…æ›´ç®€å•çš„æ–¹å¼**ï¼š

```python
# åœ¨é…ç½®ä¸­ç›´æ¥è®¾ç½®
if self.config.trainer.critic_warmup < 0:
    # SeqKD mode
    for batch in dataloader:
        batch.meta_info["use_sft_mode"] = True
        actor_output = self.actor_rollout_wg.update_actor(batch)
```

---

## å…«ã€æµ‹è¯•éªŒè¯

### 8.1 å•å…ƒæµ‹è¯•

```python
# æµ‹è¯• compute_sft_loss
import torch
from verl.trainer.ppo.core_algos import compute_sft_loss

def test_compute_sft_loss():
    log_prob = torch.randn(4, 10)  # (batch, seq_len)
    response_mask = torch.ones(4, 10)
    
    loss = compute_sft_loss(log_prob, response_mask)
    
    assert loss.dim() == 0  # æ ‡é‡
    print(f"SFT Loss: {loss.item()}")
```

### 8.2 é›†æˆæµ‹è¯•

```bash
# 1. è¿è¡Œå°è§„æ¨¡ SeqKD è®­ç»ƒ
bash seqkd_script.sh \
  --model /path/to/model \
  --exp_name seqkd_test \
  trainer.total_epochs=1

# 2. æ£€æŸ¥æ—¥å¿—
# - åº”è¯¥çœ‹åˆ° actor/sft_loss
# - åº”è¯¥çœ‹åˆ° actor/teacher_pg_loss
# - ä¸åº”è¯¥çœ‹åˆ° critic ç›¸å…³æŒ‡æ ‡

# 3. æ£€æŸ¥æ£€æŸ¥ç‚¹
ls /tmp/seqkd_test/global_step_50/actor/
```

---

## ä¹ã€å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•åˆ¤æ–­æ˜¯å¦ä½¿ç”¨äº† SFT æ¨¡å¼ï¼Ÿ

**A**: æŸ¥çœ‹è®­ç»ƒæ—¥å¿—ï¼š
- SFT æ¨¡å¼ï¼šä¼šè¾“å‡º `actor/sft_loss` å’Œ `actor/teacher_pg_loss`
- PPO/GSPO æ¨¡å¼ï¼šä¼šè¾“å‡º `actor/pg_loss` å’Œ `actor/pg_clipfrac`

### Q2: SeqKD é˜¶æ®µéœ€è¦ Critic å—ï¼Ÿ

**A**: ä¸éœ€è¦ã€‚è®¾ç½® `trainer.critic_warmup=-1` å³å¯è·³è¿‡ Critic æ›´æ–°ã€‚

### Q3: ç”Ÿæˆçš„ 8 ä¸ªå“åº”ç”¨æ¥åšä»€ä¹ˆï¼Ÿ

**A**: 
- **è®­ç»ƒæ—¶**ï¼šä¸ä½¿ç”¨ï¼Œåªç”¨ teacher_response
- **éªŒè¯æ—¶**ï¼šç”¨äºè®¡ç®— Rouge-L åˆ†æ•°ï¼ˆç›‘æ§è´¨é‡ï¼‰

### Q4: å¦‚ä½•ä» SeqKD åˆ‡æ¢åˆ° Warmupï¼Ÿ

**A**: 
```bash
# SeqKD
trainer.critic_warmup=-1  # ä¸ä½¿ç”¨ Critic

# Warmup
trainer.critic_warmup=10  # å‰ 10 æ­¥åªè®­ç»ƒ Critic
```

---

## åã€æ€»ç»“

### 10.1 ä¿®æ”¹å®Œæˆåº¦

| åŠŸèƒ½ | çŠ¶æ€ | è¯´æ˜ |
|------|------|------|
| `compute_sft_loss` å‡½æ•° | âœ… å®Œæˆ | `core_algos.py` |
| Actor SFT æ¨¡å¼æ”¯æŒ | âœ… å®Œæˆ | `dp_actor.py` |
| Teacher æ•°æ®åŠ è½½ | âœ… å®Œæˆ | `rl_dataset.py`ï¼ˆä¹‹å‰å·²å®Œæˆï¼‰ |
| é…ç½®å‚æ•°æ”¯æŒ | âœ… å®Œæˆ | é€šè¿‡ `meta_info` ä¼ é€’ |
| Trainer é€»è¾‘ | âš ï¸ éœ€è¦æ·»åŠ  | è®¾ç½® `use_sft_mode` æ ‡å¿— |

### 10.2 æ ¸å¿ƒä¼˜åŠ¿

1. **å‘åå…¼å®¹**ï¼šä¸å½±å“ Warmup/GAD é˜¶æ®µ
2. **ä»£ç ç®€æ´**ï¼šå¤ç”¨ç°æœ‰é€»è¾‘ï¼Œæ— å†—ä½™
3. **æ˜“äºåˆ‡æ¢**ï¼šé€šè¿‡é…ç½®æ§åˆ¶è®­ç»ƒæ¨¡å¼
4. **åŠŸèƒ½å®Œæ•´**ï¼šæ”¯æŒå®Œæ•´çš„ SeqKD è®­ç»ƒæµç¨‹

### 10.3 ä¸‹ä¸€æ­¥

1. âœ… **ç«‹å³å¯ç”¨**ï¼šæ ¸å¿ƒä»£ç å·²å®Œæˆ
2. âš ï¸ **éœ€è¦æ·»åŠ **ï¼šåœ¨ `ray_trainer.py` ä¸­è®¾ç½® `use_sft_mode` æ ‡å¿—
3. ğŸŸ¢ **å¯é€‰ä¼˜åŒ–**ï¼šæ·»åŠ  Rouge-L è¯„ä¼°ï¼ˆå¦‚æœéœ€è¦ï¼‰

---

## åä¸€ã€å®Œæ•´çš„ä¸‰é˜¶æ®µè®­ç»ƒæµç¨‹

### é˜¶æ®µ 1ï¼šSeqKDï¼ˆSFT åŸºçº¿ï¼‰

```bash
bash seqkd_script.sh \
  --model /path/to/base/model \
  --exp_name seqkd_exp \
  --nnodes 1
```

**é…ç½®**ï¼š
- `trainer.critic_warmup=-1`
- `use_sft_mode=True`ï¼ˆè‡ªåŠ¨è®¾ç½®ï¼‰

**è¾“å‡º**ï¼š
- `/tmp/seqkd_exp/global_step_XXX/actor/`

---

### é˜¶æ®µ 2ï¼šWarmupï¼ˆåˆ¤åˆ«å™¨è®­ç»ƒï¼‰

```bash
bash warmup_script.sh \
  --model /tmp/seqkd_exp/global_step_200/actor \
  --reward_model /path/to/reward_model \
  --exp_name warmup_exp \
  --nnodes 1
```

**é…ç½®**ï¼š
- `trainer.critic_warmup=10`
- `use_sft_mode=False`ï¼ˆé»˜è®¤ï¼‰

**è¾“å‡º**ï¼š
- `/tmp/warmup_exp/global_step_XXX/actor/`
- `/tmp/warmup_exp/global_step_XXX/critic/`

---

### é˜¶æ®µ 3ï¼šGADï¼ˆå¯¹æŠ—è®­ç»ƒï¼‰

```bash
bash gad_script.sh \
  --model /tmp/warmup_exp/global_step_800/actor \
  --reward_model /tmp/warmup_exp/global_step_800/critic \
  --exp_name gad_exp \
  --nnodes 1 \
  trainer.critic_warmup=0
```

**é…ç½®**ï¼š
- `trainer.critic_warmup=0`
- `use_sft_mode=False`ï¼ˆé»˜è®¤ï¼‰

**è¾“å‡º**ï¼š
- `/tmp/gad_exp/global_step_XXX/actor/`
- `/tmp/gad_exp/global_step_XXX/critic/`

---

**æœ€ç»ˆç»“è®º**ï¼šâœ… **Stage 1 (SeqKD) çš„æ ¸å¿ƒä»£ç å·²å®Œæˆï¼åªéœ€åœ¨ Trainer ä¸­æ·»åŠ  `use_sft_mode` æ ‡å¿—è®¾ç½®å³å¯ä½¿ç”¨ã€‚**
